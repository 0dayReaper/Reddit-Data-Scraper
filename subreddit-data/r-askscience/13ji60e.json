{
    "id": "13ji60e",
    "score": 16,
    "title": "We often can't conduct true experiments (e.g., randomly assign people to smoke or not smoke) for practical or ethical reasons. But can statistics be used to determine causes in these studies? If so, how?",
    "author": "A-manual-cant",
    "date": 1684273351.0,
    "url": "https://www.reddit.com/r/askscience/comments/13ji60e",
    "media_urls": [],
    "other_urls": [],
    "postText": "\rI don't know much about stats so excuse the question. But every day I come across studies that make claims, like coffee is good for you, abused children develop mental illness in adulthood, socializing prevents Alzheimer's disease, etc.\r  \n\r  \nBut rarely are any of these findings from true experiments. That is to say, the researchers either did not do a random selection, or did not randomly assign people to either do the behavior in question or not, and keeping everything else constant.\r  \n\r  \nThis can happen for practical reasons, ethical reasons, whatever. But this means the findings are correlational. I think much of epidemiological research and natural experiments are in this group.\r  \n\r  \nMy question is that with some of these studies, which cost millions of dollars and follow some group of people for years, can we draw any conclusions stronger than X is associated/correlated with Y? How? How confident can we be that there is a causal relationship?\r  \n\r  \nObviously this is important to do, otherwise we would still tell people we don't know if smoking \"causes\" a lot of diseases associated with smoking. Because we never conducted true experiments.",
    "comments": [
        {
            "level": 0,
            "comment": "It's a complex area. \n\nThose studies are known as \"observational\", and as such are subject to what is known as \"confounding\" - what you think is an effect you are measuring is actually due to a confounder. \n\nFor example, an effect that you think is due to diet might actually be due to socioeconomic class.\n\nThere are some advanced statistical techniques that can be used to tease away some of the effect of confounding, but there is often residual confounding that you can't get rid of because you either don't know about it or have no way to measure it. \n\nThe case of smoking is a good one - we knew smoking caused cancer because the risk ratios - how big the effect was between smoking and not smoking - was so huge, on the order of 9 to 13 times more likely to get cancer if you smoked. \n\nThat was big enough that the confounding essentially didn't matter. \n\nThe problem with most of the observational studies published these days is that their risk ratios are small - a risk ratio of 1.5 would be large and I've seen many studies published with risk ratios of 1.2 or smaller. That's tiny, and frankly so small that the result is more likely to come from confounding than a real effect. \n\nIf you look at those studies, they will say that something like \"drinking lots of soda is associated with obesity\" because observational studies are rarely strong enough to show causality. \n\nAnd then somebody writes an article that assumes that it's causal and sometimes the researches give press conferences that assume the same. It's sloppy, but it happens a lot. \n\nThis is incidentally why results tend to jump around a lot. Eggs are bad, eggs are good, eggs are bad, eggs are good. \n\nThe observational studies just aren't the right tool to answer questions like this.",
            "score": 10,
            "author": "Triabolical_",
            "replies": [
                {
                    "level": 1,
                    "comment": "One thing to look into if you want to know more about this is \"causal inference\". This is the concept that it's impossible to establish causality from observational data alone, but that there are other techniques (like randomized control trials) that can be used.\n\nIn \"The Book of Why\", a popular science book written by one of the founders of the causal inference field, the author specifically addresses the question of whether smoking causes cancer. Smoking was extremely common and an extremely personal habit, and many people attributed the higher rates of cancer in smokers to genetic factors. For example, you could argue that a genetic factor both causes cancer and predisposes people to smoking, but that there's no causal link between smoking and cancer.\n\nHow do you prove that there is a causal link? Moreover, how do you prove it to such an extent that your colleagues will give up a lifelong personal habit, based on your math?\n\nIn the end, they showed mathematically that in order for a genetic factor to cause both smoking and cancer, it would have to be an order of magnitude stronger than any genetic factor ever discovered to date. This analysis had a massive influence on public policy, and today everyone knows that smoking causes lung cancer!\n\nHighly recommend reading the book if you're interested! I found it quite accessible and understandable, and it really feels like something more people should know about!",
                    "score": 2,
                    "author": "Ceofy"
                }
            ]
        },
        {
            "level": 0,
            "comment": "Lots of good answers with pointers to relevant fields of statistics. But let me summarize some points in plain language. \n\nSimply comparing smokers to non-smokers is indeed a bad idea, due to correlation with confounders. I.e. your average smoker is likely to have lower income, less education, worse diet, and more of other unhealthy habits than a non-smoker. All of which leads to worse health even without smoking. \n\nOne way to deal with this is to make a model predicting who is likely to be smoking (or start smoking in a multi-year study). You can use variables like education, income, other conditions (especially mental). Then you can take people with same predicted chance of smoking, and compare actual smokers vs. non-smokers within that group. Ideally, you want to compare to people who have same level of all of these predictor variables, but you rarely have enough data for that. \n\nAn easier (but less precise) way is to add all the confounding variables to a regression or other model that tries to explain health of a person using smoking as well as education, income, diet, habits, other conditions, etc.  This lets you isolate \"incremental\" effect\" of smoking, i.e. effect of adding smoking to all these other factors.  \n\nYou can use \"natural experiments\", e.g. take two neighboring cities that passed indoor smoking ban at different times. Then watch how population health changed in both cities. Ideally, earlier ban should lead to earlier improvement in health. \n\nFinally, you can do true experiments with quitting smoking, or prevention of starting to smoke. E.g. you \nassign subjects randomly to a program that provides them with nicotine patches, or a seminar on dangers of smoking. Then you check on these people later, and see if the group that went through a program is doing better.",
            "score": 4,
            "author": "BaldBear_13"
        },
        {
            "level": 0,
            "comment": "*Clinical* studies are the gold standard.  Almost always a double blind study.  Not always practical, very expensive, etc.\n\nTwin studies are amazing but also niche.  Take one twin and do something, take the other and do something else.\n\n*Observational* studies can be good but take more design work.  [Longitudinal study](https://en.wikipedia.org/wiki/Longitudinal_study) \n or a *cohort study* is also pretty good.  That is close enough to what you are describing.  You actively take a group of people, ideally it's babies, and every few years you check in with them and see what is happening.\n\nA really neat thing you can do is *retroactive* analysis.  You can take an existing data set and sort it.\n\nThe other observational example is taking a group of factory workers and monitoring them for exposure to some hazard.  You could say find that people who do welding are more likely to inhale welding fume which results in blah blah and blah.  You can compare your group of welders to a control group of the normal population.\n\nA downside to that method is welders may have their lung health monitored over time and early interventions.  As a result they may experience *fewer* lung related issues than the normal population, despite higher risk.\n\nThese are almost always *observational* and not *interventions*. They tend to be as good as the questions that get asked.\n\nA key point is these are not done out of the blue.  The researchers are starting their study by targeting known or potential *risk factors*.\n\nThe researchers find a robust data set and they start with correlations, but they can narrow it down to causes with enough supporting evidence.\n\nThe preferred statistical model used by the FDA is called the *[mixed effect model](https://en.wikipedia.org/wiki/Mixed_model)*.  It is... complicated.  Uses words like \"Bayesian statistics\".  Another is [ANOVA](https://en.wikipedia.org/wiki/Analysis_of_variance).  Key difference in one sentence between those two is the first can manage irregularly timed data and incomplete data.\n\nMy favourite quote (from a problematic person) about this type of analysis is a half-assed attempt to explain the technique called *the Johari window*:\n\n&gt; Reports that say that something hasn't happened are always interesting to me, because as we know, there are known knowns; there are things we know we know. We also know there are known unknowns; that is to say we know there are some things we do not know. But there are also unknown unknowns\u2014the ones we don't know we don't know. And if one looks throughout the history of our country and other free countries, it is the latter category that tends to be the difficult ones.\n\nThe statistical models are incredibly advanced.  They can distinguish between truly random effects and unexpected-but-within-reason variability.\n\nOutcome of a longitudinal study is targeted follow up with clinical studies.  It also gives information about *risk factors*.  We don't necessarily need to know 100% when all signs point to something.  It allows for a policy called *[informed consent](https://en.wikipedia.org/wiki/Informed_consent)*.  It is valuable to be able to say to a person that thing is correlated.",
            "score": 8,
            "author": "Indemnity4"
        },
        {
            "level": 0,
            "comment": "This is epidemiology! We use statistics to estimate likelihood or attributable risk. I can't speak to specifics about grants awarded, but i can talk about the statistics and the back-and-forth in the literature. \n\nOne of the most difficult parts of epi, or any observational study, is the ability to gather a cohort representative of the population you are studying. So, if you want to study cancer incidence specifically in black men who smoke, the sample you collect data from should not include white men (or white or black women). If you want to study the impact of sleep apnea on heart disease in American men, you want to have a large population that is representative of america (that means, collecting data and weighting data such that minority populations are accurately represented in your final value). \n\nThe most basic method we use to analyze association is a 2 x 2 table. Lets use the incidence of lung cancer amongst people who smoke cigarettes. (All of the numbers in this example are made up, you can look up actual odds ratios yourself if you want).\nCigarette smoking is the exposure (what the group is independently exposed to), and lung cancer development is the outcome (what we are measuring odds of). Of 100 cigarette smokers, 75 smokers developed lung cancer and 25 didn't. Of 100 controls (non smokers), 15 developed lung cancer and 85 didn't. \n\nWe can calculate the odds ratio (or, the probability that cigarette smokers will develop lung cancer) by some quick math:\n\n(75 * 85)/ (15 * 25) = 17.\n\nSo, in this example, cigarette smokers have 17x greater the odds as non-cigarette smokers of developing lung cancer. Great! Get some confidence intervals or a p-value (these are measures of probability that your answer is not due to chance; they are what allow us to say something has an association or not), slap em on that huge Odds Ratio, write a paper, and be on your merry way.\n\nExcept...not really. That back-and-forth you see about \"coffee is good for you, actually\" is really common! It happened with eggs back in the day, too. What it comes down to is, genuinely, miscommunication. Lots of news articles that aren't behind a paywall (and many that are behind a paywall) are made to be readable to a broad audience, and with that territory comes the need for a good balance of technical and narrative skill. \nWhat tends to happen is, a paper will say something like \"Coffee is bad for you again\" when in reality, the study itself said \"white men who drank coffee in this study population were 2x more likely to suffer a heart attack than white men who didnt drink coffee\". \n\nNow for the really fun part. A META-ANALYSIS!\nthis is getting long, so i'll be quick. A meta-analysis is a thorough review of all the literature published in the topic of study. So, if we had 30 papers about coffee drinking and adverse outcomes, 20 papers about coffee drinking and improved outcomes, we smash all the data from those papers together and see what direction the research trends towards. A meta-analysis is one of the strongest (when done properly) ways to confidently \"make\" an association or correlation or risk or prevalence. \n\n\nI could go on, but if you're interested, ask me about imputation and how study design contributes to the type of data you collect. Or look into it yourself! Epidemiology is a great field!!!!",
            "score": 6,
            "author": "claycolorfighter"
        },
        {
            "level": 0,
            "comment": "Welcome to the world of epidemiology. True experiments is weird word though. I think you mean clinical trials, but there are plenty of ways to find counterfactuals that are rigorous as well. Longitudinal studies are where you should start but that is only the beginning. Natural experiments. Analogies like rat studies, and cross sectional studies all have their uses. It gets really crazy when you start getting into minor causes though.",
            "score": 2,
            "author": "crolin"
        },
        {
            "level": 0,
            "comment": "I read a study about an HIV vaccine which had promising results in computer simulations and lab tests but had obvious ethical implications with human testing. You can't give people a placebo then infect them with HIV just to test your vaccine. You also can't give people an unproven HIV vaccine then infect them with HIV.\n\nSo they gave the vaccine to people already at high risk of HIV, I think it was people in the sex industry in a country with widespread HIV infections, or people who frequented the sex industry, something like that. The idea was to vaccinate people at high risk then check back in a year's time and start calculating statistics. Not as clear-cut as actually infecting them in the lab but much more palatable ethically. \n\nUnfortunately the vaccine actually *increased* your chances of contracting HIV. They double-checked the statistical analysis to see if it was a fluke in the dataset but they got the same result from multiple simultaneous studies in different villages. The vaccine definitely increased your risk of contracting HIV. I don't recall the name of the vaccine or the mechanism it was aiming to work by, there probably is a logical explanation for why the vaccine made it worse but I don't know it off the top of my head.",
            "score": 2,
            "author": "Simon_Drake"
        },
        {
            "level": 0,
            "comment": "Statistical tools tell you how likely you would be to get the observed association by chance, but they don\u2019t prove it\u2019s causal. Clinical reasoning suggests a possible effect, statistics shows if it exists and whether it can be explained by other factors.\n\nEg you want to see if coffee drinking causes cancer, you observe a very strong association unlikely to be due to chance: then you correct for smoking (hard to do entirely, maybe you just restrict the analysis to never smokers) and the effect goes away.",
            "score": 1,
            "author": "Additional-Fee1780"
        },
        {
            "level": 0,
            "comment": "&gt;..., can we draw any conclusions stronger than X is associated/correlated with Y? How? How confident can we be that there is a causal relationship?\n\nThe answer is \"it depends\". \n\n* Do we know of a causal mechanism? \n* How large is the effect? \n* How random was it? \n* Was the study done well?\n\nIf you have a strong prior knowledge of a mechanism, large effects, reasonable randomization, didn't cherry pick or torture the data, you can make very strong causal claims. That is what happened with smoking and cancer.\n\nIf you have no causal explanation, small effects, little randomization, and had to \"control\" for a large variety of variables to find the effect, you can make only weak correlational claims. That is what happens with coffee benefit studies.",
            "score": 1,
            "author": "yuzirnayme"
        }
    ]
}