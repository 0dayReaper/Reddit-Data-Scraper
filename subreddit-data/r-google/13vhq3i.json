{
    "id": "13vhq3i",
    "score": 44,
    "title": "Rumor: Google Gemini project 1-5 Trillion parameters, 20 Trillion tokens",
    "author": "hasanahmad",
    "date": 1685425213.0,
    "url": "https://www.reddit.com/r/google/comments/13vhq3i",
    "media_urls": [
        "https://i.redd.it/5rb2i5dh6y2b1.jpg"
    ],
    "other_urls": [],
    "postText": "",
    "comments": [
        {
            "level": 0,
            "comment": "Is the \"Alan\" there supposed to be Dr. Alan Thompson? I trust him because he's been right about AI insider info and release dates in the past. DeepMind is making their model Chinchilla optimal as expected. Also, I hope this memo is a recent one, not an old one.\n\nWhat does convergence date mean? I really want Gemini to be released as soon as possible, at least for research so we can get an idea of its capabilities now that training has already been completed. OpenAI has consistently been downgrading their GPT-4 ever since its release.",
            "score": 5,
            "author": "Bombtast",
            "replies": [
                {
                    "level": 1,
                    "comment": "There isn't a single Alan in Gemini or AI in general. Theres a reason that deadline was missed. All the Alans do is steal.",
                    "score": 1,
                    "author": "17311422237"
                }
            ]
        },
        {
            "level": 0,
            "comment": "Are you paid by google to post this 'leaks'??",
            "score": 4,
            "author": "ExHax",
            "replies": [
                {
                    "level": 1,
                    "comment": "You're on the Google subreddit what did you expect?",
                    "score": 5,
                    "author": "pinghing"
                }
            ]
        },
        {
            "level": 0,
            "comment": "OpenAI has repeatedly said its not about how big the numbers are, its more about the RLHF. From the interviews I have watched, they said the emergent properties didn't really start to show until after RLHF.",
            "score": 1,
            "author": "solinar",
            "replies": [
                {
                    "level": 1,
                    "comment": "Google / DeepMind is the one that publicly / published the benefits of more training on smaller models \n\nhttps://www.deepmind.com/publications/an-empirical-analysis-of-compute-optimal-large-language-model-training\n\nGoogle is well aware of this.",
                    "score": 6,
                    "author": "cosmic_backlash"
                }
            ]
        },
        {
            "level": 0,
            "comment": "Such big numbers.\n\nMuch wow.\n\nNo meaning.",
            "score": -12,
            "author": "Bunnymancer",
            "replies": [
                {
                    "level": 1,
                    "comment": "Not really. While I don't think posting the numbers alone is useful, higher numbers usually mean smarter models",
                    "score": -1,
                    "author": "Honza368"
                }
            ]
        },
        {
            "level": 0,
            "comment": "That would be insanely big.   Which also mean more expensive in inference.\n\nGoogle has the most popular web site there has ever been and has over 2 billion daily active users for search.\n\nNow Google was really smart to have done the TPUs.  Now working on the fifth generation with the fourth deployed.\n\nhttps://blog.bitvore.com/googles-tpu-pods-are-breaking-benchmark-records\n\nBut even with this advantage it is still hard to imagine Google can offer a model this size broadly and without some new revenue stream.\n\nIt must be 10x the cost in computation compared to search today.",
            "score": 1,
            "author": "bartturner"
        }
    ]
}