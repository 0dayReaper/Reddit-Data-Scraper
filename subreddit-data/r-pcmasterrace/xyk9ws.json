{
    "id": "xyk9ws",
    "score": 74812,
    "title": "Skeletor knows the truth.",
    "author": "B3_CHAD",
    "date": 1665206355.0,
    "url": "https://www.reddit.com/r/pcmasterrace/comments/xyk9ws",
    "media_urls": [
        "https://i.redd.it/fgdhionwnis91.jpg"
    ],
    "other_urls": [],
    "postText": "",
    "comments": [
        {
            "level": 0,
            "comment": "Welcome everyone from r/all! Please remember:\r\n\r\n1 - You too can be part of the PCMR! You don't necessarily need a PC. You just have to love PCs! It's not about the hardware in your rig, but the software in your heart! Your age, nationality, race, gender, sexuality, religion (or lack of), political affiliation, economic status and PC specs are irrelevant. If you love PCs or want to learn about them, you can be part of our community! Everyone is welcome!\r\n\r\n2 - If you're not a PC gamer because you think doing so is expensive, know that it is possible to build a competent gaming PC for a lower price than you think. Check http://www.pcmasterrace.org for our builds and don't be afraid to create new posts here asking for tips and help!\r\n\r\n3 - Consider joining our efforts to get as many PCs worldwide help the folding@home effort, in fighting against Cancer, Covid, Alzheimer's, Parkinson's and more. Learn more here: https://pcmasterrace.org/folding\r\n\r\n-----------\r\n\r\nFeel free to use this community to post about any kind of doubt you might have about becoming a PC gamer or anything you'd like to know about PCs. That kind of content is not only allowed but welcome here! We also have a [Daily Simple Questions Megathread](https://www.reddit.com/r/pcmasterrace/search?q=Simple+Questions+Thread+subreddit%3Apcmasterrace+author%3AAutoModerator&amp;restrict_sr=on&amp;sort=new&amp;t=all) for your simplest questions. No question is too dumb!\r\n\r\n\r\nWelcome to the PCMR.",
            "score": 1,
            "author": "PCMRBot"
        },
        {
            "level": 0,
            "comment": "With this development, in 10 years we gonna return to early computer size",
            "score": 631,
            "author": "MP_Cook",
            "replies": [
                {
                    "level": 1,
                    "comment": "You joke but that\u2019s probably what will happen. Computers are about to get very large again and then start shrinking exponentially again",
                    "score": 286,
                    "author": "mobius_chicken",
                    "replies": [
                        {
                            "level": 2,
                            "comment": "I don't know how much further we can realistically shrink chips. We're reaching the physical limits of the materials at this point. Unless we start using diamond or graphene or something instead of silicon I don't think we're going to cram a supercomputer into a chip ever again.\n\nGranted, people have been saying this for 20 years now and it hasn't borne out yet, but still. Progress has certainly slowed in the desktop CPU space compared to 20 years ago.",
                            "score": 170,
                            "author": "holly_hoots",
                            "replies": [
                                {
                                    "level": 3,
                                    "comment": "Biological chips, go full sci-fi on their asses",
                                    "score": 84,
                                    "author": "mobius_chicken",
                                    "replies": [
                                        {
                                            "level": 4,
                                            "comment": "Guys my computer has a cold I\u2019m lagging so hard rn",
                                            "score": 92,
                                            "author": "youreblockingmyshot",
                                            "replies": [
                                                {
                                                    "level": 5,
                                                    "comment": "This was a plot point in Voyager so many times.",
                                                    "score": 23,
                                                    "author": "acquaintedwithheight"
                                                }
                                            ]
                                        },
                                        {
                                            "level": 4,
                                            "comment": "Big 'ole slab'o meat, sat there pulsating and squelching on top of an IKEA Micke.",
                                            "score": 21,
                                            "author": "fullrackferg"
                                        },
                                        {
                                            "level": 4,
                                            "comment": "oh sweet manmade horrors beyond my comprehension",
                                            "score": 9,
                                            "author": "dinsfire24"
                                        },
                                        {
                                            "level": 4,
                                            "comment": "No, quantum chips. Actual tech being developed to compute faster instead of sci fi fiction.",
                                            "score": 7,
                                            "author": "BlueMANAHat",
                                            "replies": []
                                        }
                                    ]
                                },
                                {
                                    "level": 3,
                                    "comment": "We'll probably start adding analog, non-binary and/or quantum components into computers in the next 15 years. In the shorter term, we might start layering",
                                    "score": 25,
                                    "author": "EdgyAsFuk",
                                    "replies": []
                                }
                            ]
                        }
                    ]
                }
            ]
        },
        {
            "level": 0,
            "comment": "I\u2019m not gonna delve too deep into the 40 series. I\u2019m gonna stick with the 30 series because they\u2019re good for gaming \u2014 some overkill.",
            "score": 1162,
            "author": "[deleted]",
            "replies": [
                {
                    "level": 1,
                    "comment": "I just got a 3080 for a 1440p set up and I will be using this for the next decade I presume.",
                    "score": 469,
                    "author": "erupting_lolcano",
                    "replies": [
                        {
                            "level": 2,
                            "comment": "Exactly what I did. I used a 750ti from '14 to '21 lol, I'm pretty sure I'll be fine with a 3080 for at least 7-10 years. Assuming DLSS remains usable on it.",
                            "score": 237,
                            "author": "fistymcbuttpuncher",
                            "replies": [
                                {
                                    "level": 3,
                                    "comment": "Yo same I went from 970 since 2014 to 2022 to a 3060",
                                    "score": 78,
                                    "author": "FallSkull",
                                    "replies": [
                                        {
                                            "level": 4,
                                            "comment": "Same for me, except 2060 Super in 2020. 970 was seriously like the best gpu of all time. I'm still using it in a secondary PC for my friends to use when they come over",
                                            "score": 40,
                                            "author": "d4rk_matt3r",
                                            "replies": [
                                                {
                                                    "level": 5,
                                                    "comment": "It was great. Mine is still chugging away cause my wife wanted it for her computer.",
                                                    "score": 12,
                                                    "author": "FallSkull",
                                                    "replies": []
                                                }
                                            ]
                                        }
                                    ]
                                },
                                {
                                    "level": 3,
                                    "comment": "And that's how they'll get us. They can force obsolescence by reducing/ending support of DLSS 2. NVIDIA has the luxury option of making our hardware upgrades occur over artificial software decisions. They've made that much clear already with the 40 series.\n\nThey don't want you to keep your video card for 7 to 10 years. That's bad business for them",
                                    "score": 36,
                                    "author": "redvyper",
                                    "replies": [
                                        {
                                            "level": 4,
                                            "comment": "AMD doesn't play like that. Their FSR is getting closer and closer to DLSS and one day it may even surpass it. \n\nI bought a 3070 in 2020 and it might as well be my last Nvidia card.",
                                            "score": 27,
                                            "author": "stefan714",
                                            "replies": [
                                                {
                                                    "level": 5,
                                                    "comment": "As soon as I had enough money that I could start buying my own shit I got a 1070ti and then last year I got a 3070. Always been green. Will be going AMD next upgrade because Nvidia is too greedy.",
                                                    "score": 20,
                                                    "author": "[deleted]",
                                                    "replies": []
                                                }
                                            ]
                                        }
                                    ]
                                },
                                {
                                    "level": 3,
                                    "comment": "Even if DLSS tanks, FSR and XeSS should remain usable. The former is in current consoles, so I imagine it'll stay useful for a while.",
                                    "score": 9,
                                    "author": "Drenlin",
                                    "replies": []
                                }
                            ]
                        }
                    ]
                },
                {
                    "level": 1,
                    "comment": "I'm sitting here with my 1660 TI waiting for to buy a 30 card in a couple years.",
                    "score": 101,
                    "author": "TheRealOgMark",
                    "replies": [
                        {
                            "level": 2,
                            "comment": "Man I\u2019m sitting over here with a 1050ti.  Served me well all these years",
                            "score": 34,
                            "author": "basedauthright",
                            "replies": [
                                {
                                    "level": 3,
                                    "comment": "970 still doing good for me. I need an upgrade but I can still play what I want to.",
                                    "score": 29,
                                    "author": "i_regret_joining",
                                    "replies": [
                                        {
                                            "level": 4,
                                            "comment": "Still rocking a 980 here. Sure, some brand new high end titles can't be maxed anymore, but it's still 95% capable of everything I do at 1080p. \n\nI'm going to grab a 3070 when I'm finished paying off my two sons' 3070s. \ud83d\ude06",
                                            "score": 12,
                                            "author": "relic1882",
                                            "replies": []
                                        }
                                    ]
                                },
                                {
                                    "level": 3,
                                    "comment": "I still have a 1070 and it still runs brand new games like a champ on max settings. I don't understand the need some people have to upgrade to the newest card every year.",
                                    "score": 16,
                                    "author": "mrsamus101",
                                    "replies": [
                                        {
                                            "level": 4,
                                            "comment": "How? I feel like mine struggles hard... I'm assuming I need a legit upgrade to my cpu?",
                                            "score": 11,
                                            "author": "[deleted]",
                                            "replies": [
                                                {
                                                    "level": 5,
                                                    "comment": "I have a 1070, and figured out a bit over a year ago that my old CPU was bottlenecking some newer games like borderlands 3 and causing mad stuttering and crazy texture load times, so I upgraded to an Intel i5 12th gen, now it's chugging along happily. I don't remember exactly, but I'm pretty sure the original was a 3rd gen. Now I can get 55-60 fps 1080p on high to very high graphics in most games like Cyberpunk and Elden Ring.\n\nIf you're one of those people who plays AAA titles and thinks anything less than 60 at max settings is unacceptable, then the 1070 is quickly approaching the end of its operational lifespan, but a cpu upgrade might help you get more out of it.",
                                                    "score": 13,
                                                    "author": "ChefKraken",
                                                    "replies": []
                                                }
                                            ]
                                        }
                                    ]
                                }
                            ]
                        }
                    ]
                },
                {
                    "level": 1,
                    "comment": "Literally all of the games I enjoy right now could run on a potato, so there's no real reason to upgrade.",
                    "score": 10,
                    "author": "MarkAldrichIsMe",
                    "replies": []
                },
                {
                    "level": 1,
                    "comment": "Question is what game needs the 40series? Nothing coming yet that screams upgrade needed. \n\nAll the rage to was to get the 30 series for CP2077.",
                    "score": 7,
                    "author": "Hey_Hoot",
                    "replies": []
                }
            ]
        },
        {
            "level": 0,
            "comment": "...not wrong.",
            "score": 4752,
            "author": "SkavensWhiteRaven",
            "replies": [
                {
                    "level": 1,
                    "comment": "It\u2019s possible they planned this design, while miners were looking like their best customers. \n\nWould explain the chonk.\n\nI think they know they\u2019re in for a bumpy ride now, between inflation/recession trends, being all but certain through q4-q3 of 2023.\n\nGood time to be diversified, with low cost options. Not sure \u201cbeast mode brick\u201d is going to get me to open my wallet rn. Yes I can afford it, just don\u2019t feel like bleeding edge has ever been satisfying for very long.\n\n**Edit** *For classic \u201cthey\u2019re\u201d vs \u201ctheir\u201d typo. At least it wasn\u2019t \u201cthere\u201d\u2026*",
                    "score": 2619,
                    "author": "Robo_Patton",
                    "replies": [
                        {
                            "level": 2,
                            "comment": "Bleeding edge is a cool flex, but it's just such a diminishing return especially with tech not making as big leaps as it used to.",
                            "score": 735,
                            "author": "battlechicken12",
                            "replies": [
                                {
                                    "level": 3,
                                    "comment": "I already know people who regretted the 3090 and even some the 3080 as overkill for what we have available. The 4series in general is surplus currently until we get truly next gen titles imo\n\nFlex buy for sure.",
                                    "score": 381,
                                    "author": "ponytoaster",
                                    "replies": [
                                        {
                                            "level": 4,
                                            "comment": "It feels like the only substantial difference between the 3080 and 3090 is the VRAM. 3090 is great for artists cuz that shit takes a lot of VRAM. However even the most demanding games nowadays don't take more than 12gb VRAM.",
                                            "score": 124,
                                            "author": "ZmentAdverti",
                                            "replies": [
                                                {
                                                    "level": 5,
                                                    "comment": "Honestly its a very specific type of artist that needs a 3090. Of all my colleagues the only few that ACTUALLY need that power are the vfx team and some of the environment designers. Even as freelancing using 10\u2019s of millions of polygons on blender and rendering i can get through with a 1660 just fine. And to be honest, if the rendering needs so much vram, the computer is pretty unusableat this point. Better save the money and get a second machine of hire a render farm. No matter your need or your rig, a render farm will outperform and deliver what you need at a faster rate.",
                                                    "score": 88,
                                                    "author": "5spikecelio",
                                                    "replies": [
                                                        {
                                                            "level": 6,
                                                            "comment": "AI stuff can benefit from more RAM too",
                                                            "score": 21,
                                                            "author": "InEnduringGrowStrong",
                                                            "replies": []
                                                        }
                                                    ]
                                                },
                                                {
                                                    "level": 5,
                                                    "comment": "[deleted]",
                                                    "score": 39,
                                                    "author": "[deleted]",
                                                    "replies": [
                                                        {
                                                            "level": 6,
                                                            "comment": "We need more charts that showcase this. $70-80/1 fps.",
                                                            "score": 15,
                                                            "author": "relic1882"
                                                        },
                                                        {
                                                            "level": 6,
                                                            "comment": "Random here; I have a 2080ti and people are telling my GPU is \u201centry level\u201d",
                                                            "score": 9,
                                                            "author": "Adeyotol",
                                                            "replies": []
                                                        }
                                                    ]
                                                },
                                                {
                                                    "level": 5,
                                                    "comment": "Oh definitely, CAD and general rendering will be faster for sure as will ML but it makes me chuckle when you see kids flexing a 3090 to play CoD and Fortnite just because they are top tier hardware.\n\nLike, cool. But it's like getting a HyperCar and only using it to drive 20mph around the city.",
                                                    "score": 86,
                                                    "author": "ponytoaster",
                                                    "replies": [
                                                        {
                                                            "level": 6,
                                                            "comment": "Or like having a personal chef for your dog.\n\nDogs literally enjoy eating poo, what do they need a chef for?",
                                                            "score": 41,
                                                            "author": "twisted7ogic",
                                                            "replies": []
                                                        }
                                                    ]
                                                },
                                                {
                                                    "level": 5,
                                                    "comment": "Resident Evil 8 would like to have a word.\n\nCouldn't believe that my 3080ti hadn't enough vram for allmax settings.",
                                                    "score": 37,
                                                    "author": "desilusionator",
                                                    "replies": [
                                                        {
                                                            "level": 6,
                                                            "comment": "Wow and I thought it was just the 10G that had issues, glad to know even 12 wouldn't have solved that problem.",
                                                            "score": 17,
                                                            "author": "GlammBeck",
                                                            "replies": [
                                                                {
                                                                    "level": 7,
                                                                    "comment": "I think you need around 16G for RE8. Ridiculous",
                                                                    "score": 11,
                                                                    "author": "desilusionator",
                                                                    "replies": [
                                                                        {
                                                                            "level": 8,
                                                                            "comment": "If AMD puts out a GPU with similar RT performance to a 3080, 16GB of VRAM, and better raster, I'm gonna jump.",
                                                                            "score": 9,
                                                                            "author": "GlammBeck"
                                                                        }
                                                                    ]
                                                                }
                                                            ]
                                                        }
                                                    ]
                                                }
                                            ]
                                        },
                                        {
                                            "level": 4,
                                            "comment": "I got a 2080 super on sale when 3000 was announced but just before the bulk of the chip shortage. Got it cheaper than a 2080 super is retailing for even now, and it came with death stranding.\nFor 1080 gaming, and oculus quest VR. I can\u2019t see myself upgrading any time soon.",
                                            "score": 28,
                                            "author": "zaphodbeeblemox",
                                            "replies": []
                                        },
                                        {
                                            "level": 4,
                                            "comment": "There won't be any next gen title that anything that is 3070 or above shouldn't be able to run on console settings. Until the next gen of consoles.\n\nThe greatest challenges will be in CPU and I/O.",
                                            "score": 127,
                                            "author": "hpstg",
                                            "replies": [
                                                {
                                                    "level": 5,
                                                    "comment": "Considering that a large portion of games are still being released on the last gen, I don\u2019t think we\u2019ll see too many titles pushing the limits of high-end PC parts for a little while.",
                                                    "score": 76,
                                                    "author": "Ospov",
                                                    "replies": [
                                                        {
                                                            "level": 6,
                                                            "comment": "I've been out of the console loop for over two generations now, but is there really games still being exclusively released on PS4/whatever the Xbox was called? I know that PS said something like, anything released on PS4 after a certain date will have to be PS5 compatible, right?",
                                                            "score": 10,
                                                            "author": "Djeheuty",
                                                            "replies": [
                                                                {
                                                                    "level": 7,
                                                                    "comment": "Pretty sure they meant games that are being backported to the PS4/XBOne, rather than games being developed exclusively for PS4/XBOne. Sorta like Titanfall being an XBOne title that was also an XB360 title or LBP3 releasing on PS3 and PS4.",
                                                                    "score": 9,
                                                                    "author": "2KDrop",
                                                                    "replies": []
                                                                }
                                                            ]
                                                        }
                                                    ]
                                                },
                                                {
                                                    "level": 5,
                                                    "comment": "IDK, depends on what you're doing really. For 4K+ gaming and especially VR, having the best of the best would still make a difference, but other than that it does seem rather pointless.",
                                                    "score": 51,
                                                    "author": "Sikletrynet",
                                                    "replies": [
                                                        {
                                                            "level": 6,
                                                            "comment": "Skyrim VR with mods maxes out my VRAM on my 3070 even though the card can handle the rasterization easily, so mad that they skimped on VRAM.",
                                                            "score": 25,
                                                            "author": "deviance1337",
                                                            "replies": [
                                                                {
                                                                    "level": 7,
                                                                    "comment": "I think your frame rate issue might be your cpu\u2026",
                                                                    "score": 10,
                                                                    "author": "Separate-Eye5179",
                                                                    "replies": [
                                                                        {
                                                                            "level": 8,
                                                                            "comment": "If you're going off my flair I just noticed that it's outdated, I'm on a 5600x now. Tested specifically for VRAM and it's definitely the issue, lowering texture mods or resolution helped a lot.",
                                                                            "score": 12,
                                                                            "author": "deviance1337",
                                                                            "replies": []
                                                                        }
                                                                    ]
                                                                }
                                                            ]
                                                        }
                                                    ]
                                                },
                                                {
                                                    "level": 5,
                                                    "comment": "Don't forget we're about to have a 2 year recession. So Budgets will be smaller. During the last recession the market moved to cheap netbooks. - I'm pretty sure something similar will happen this time around. \n\nWe'll go low cost to weather the storm.",
                                                    "score": 9,
                                                    "author": "Centralredditfan",
                                                    "replies": [
                                                        {
                                                            "level": 6,
                                                            "comment": "Not to mention Moore's law is predicted to fail for the first time in '25. I think as a whole manufacturers are going to have to go smaller, more efficient, less power demanding. At least until we figure out other breakthroughs in general.",
                                                            "score": 5,
                                                            "author": "tinydragongamer"
                                                        }
                                                    ]
                                                },
                                                {
                                                    "level": 5,
                                                    "comment": "The greatest challenge will be the PSU,  \nand by that I mean it won't get any P because I can't afford the S.",
                                                    "score": 5,
                                                    "author": "Genneth_Kriffin",
                                                    "replies": []
                                                }
                                            ]
                                        },
                                        {
                                            "level": 4,
                                            "comment": "I feel like the 40-series might be surplus until we get truly nextgen equipment. At this point it half feels like they've tried to go Intel's route. If AMD swings in with lower power, high output cards, nvidia is going to be laughed out of the building.\n\nI don't *really* believe this will happen. Sure feels similar though.",
                                            "score": 10,
                                            "author": "Phylar",
                                            "replies": [
                                                {
                                                    "level": 5,
                                                    "comment": "Imo the 4000 series was targeted mainly at the mining crowd, then the crypto crash came, and now they are trying to damage control by selling them to gamers.",
                                                    "score": 12,
                                                    "author": "the_ebastler",
                                                    "replies": []
                                                }
                                            ]
                                        },
                                        {
                                            "level": 4,
                                            "comment": "3080 handles 4k at high frame rates so yeah, maybe if it was more efficient room gets hot with all the increasing TDP these parts do.",
                                            "score": 11,
                                            "author": "arnoldzgreat"
                                        },
                                        {
                                            "level": 4,
                                            "comment": "I kinda felt like the 80's and 90's in the 30 series were too much power draw for me. the 40's? no way. I went with a 3070 and will prob run it until it dies or price/wattage/performance gets wayyyyy better.",
                                            "score": 11,
                                            "author": "thundar00"
                                        },
                                        {
                                            "level": 4,
                                            "comment": "I even feel the regret getting the 3070 considering I only play Apex and Valorant at 1080p. But oh well, might as well hold on to it for the next three years",
                                            "score": 38,
                                            "author": "Aimpossible",
                                            "replies": [
                                                {
                                                    "level": 5,
                                                    "comment": "Look into 1440p monitor perhaps? You get a bit more resolution and more utilisation of your hardware. The resolution makes a big difference in games imo.",
                                                    "score": 47,
                                                    "author": "Ziggy_the_third",
                                                    "replies": [
                                                        {
                                                            "level": 6,
                                                            "comment": "That's the plan. I'm just waiting for a good deal for a 27\" 1440p or a 34\" Ultrawide monitor",
                                                            "score": 7,
                                                            "author": "Aimpossible",
                                                            "replies": [
                                                                {
                                                                    "level": 7,
                                                                    "comment": "You might look into it but I think you would have to play Valorsnt with black bars on an ultra wide.",
                                                                    "score": 7,
                                                                    "author": "TanaerSG"
                                                                },
                                                                {
                                                                    "level": 7,
                                                                    "comment": "You can also look into a monitor on eBay.  I got a 3080 when Newegg finally had them in stock early this year and went to eBay and got like a 30 inch 4k monitor.  I think I paid 220 bucks for it.\n\nThe 3080 can play plenty of stuff maxed out at 4k60fps, your 3070 would basically be able to do the same.  I haven\u2019t stressed it on something like Cyberpunk yet but mk11, street fighter 6, Elden ring, doom eternal, all run at max 4k60fps\n\nJust make sure the eBay person has a positive review and allows refunds in case it gets to you with a dead pixel or something",
                                                                    "score": 3,
                                                                    "author": "RockBandDood"
                                                                }
                                                            ]
                                                        }
                                                    ]
                                                },
                                                {
                                                    "level": 5,
                                                    "comment": "&gt;\tfor the next three years\n\n*coughs in 970*",
                                                    "score": 11,
                                                    "author": "[deleted]",
                                                    "replies": []
                                                }
                                            ]
                                        },
                                        {
                                            "level": 4,
                                            "comment": "Yup. Most game need to catch up to the potential.  Half the games are not even optimized to run 120fps 2k yet, and nvidia already pushing 8k like its nothing.",
                                            "score": 7,
                                            "author": "BrokeTunder"
                                        },
                                        {
                                            "level": 4,
                                            "comment": "I mean, what monster of a game that needs a 4090? I can already play all the newest games at max setting with my 3080. The new games that are supposed to come in 2023 dont look too demanding either. Unless they start making a game out of that new engine demo for The Matrix, we certainly can skip 4000s generation.",
                                            "score": 6,
                                            "author": "soluuloi",
                                            "replies": []
                                        },
                                        {
                                            "level": 4,
                                            "comment": "[removed]",
                                            "score": 17,
                                            "author": "[deleted]",
                                            "replies": [
                                                {
                                                    "level": 5,
                                                    "comment": "[deleted]",
                                                    "score": 10,
                                                    "author": "[deleted]"
                                                }
                                            ]
                                        },
                                        {
                                            "level": 4,
                                            "comment": "Eh?\nHow can you regret the 3080, for the MSRP it was a steal and there are definitely games that can use it...",
                                            "score": 7,
                                            "author": "Cushions",
                                            "replies": [
                                                {
                                                    "level": 5,
                                                    "comment": "My friends that do have them could have done with the 3070.\nI think if they sold at MSRP it may be ok but a majority didn't sadly.",
                                                    "score": 4,
                                                    "author": "ponytoaster",
                                                    "replies": []
                                                }
                                            ]
                                        },
                                        {
                                            "level": 4,
                                            "comment": "Every unoptimized game and 4k would like a word. Unfortunately not even my 3090 Ti is made for 4k 144FPS on games like RDR2, Kingdom Come Deliverance, etc.   \n\n\nI decided to not play on 4k and instead went from 1080p -&gt; 1440p because I hate the idea of buying a new PC, just to lower game settings.",
                                            "score": 9,
                                            "author": "C0deEve",
                                            "replies": [
                                                {
                                                    "level": 5,
                                                    "comment": "Oh god indeed... Games are getting worse for that.\n\nThat's my concern too is that game developers will get even more lazy as newer cards will \"just cope\" which prices out a whole market from enjoying the games :(",
                                                    "score": 5,
                                                    "author": "ponytoaster",
                                                    "replies": []
                                                }
                                            ]
                                        }
                                    ]
                                },
                                {
                                    "level": 3,
                                    "comment": "It's always been diminishing returns. The top of the line cards have always just been for people with too much money or too little sense.",
                                    "score": 6,
                                    "author": "DeliciousWaifood"
                                },
                                {
                                    "level": 3,
                                    "comment": "Ampere was a huge jump from Turing especially price/performance wise, and Lovelace is a huge jump as well now that Nvidia ditched Samsung and went with TSMC.  \n\nThe problem with bleeding edge is that right now it\u2019s stupid expensive",
                                    "score": 5,
                                    "author": "nerfzacian"
                                },
                                {
                                    "level": 3,
                                    "comment": "There's also the fact most programs and games are optimized for average hardware which further worsens the diminishing returns. Having better than average is definitely still a good thing, but you can still get 1440p 120fps pretty easily on most modern games even with cards a generation or two old.",
                                    "score": 8,
                                    "author": "jzillacon",
                                    "replies": [
                                        {
                                            "level": 4,
                                            "comment": "Recently got a 3080. I know there's several reasons I shouldn't have been surprised, but I cranked Doom 2016's graphics up all the way and was still getting 200 fps on my 1440p screen.\n\n(Mostly got it to get into VR, though.)",
                                            "score": 6,
                                            "author": "atimholt"
                                        }
                                    ]
                                }
                            ]
                        },
                        {
                            "level": 2,
                            "comment": "&gt;It\u2019s possible they planned this design\n\nSome said that nVidia was planning 600w cards because they thought the process node wasn't going to be as good/or efficient, told that (600w) to their partners, then back tracked to 450w cards, when they discovered the process node was good enough.\n\nAIBs probably didn't have time to come up with smaller cooling solutions.",
                            "score": 39,
                            "author": "DrKrFfXx",
                            "replies": []
                        },
                        {
                            "level": 2,
                            "comment": "Maybe they think they can out last the consumer too?\nLots of people haven't upgraded since 1080s. \nWith someone having been on an Nvidia platform for so long what are the chances they'd jump to amd... realistically brand loyalty is a thing.\n\nOnly time will tell but 6600xt is still my recommendation for 90% of gamers.",
                            "score": 35,
                            "author": "SkavensWhiteRaven",
                            "replies": [
                                {
                                    "level": 3,
                                    "comment": "I got the 6650 XT 2 weeks ago. 1080p Ultra on my ultra wide, 55fps Cyberpunk. Drop those settings to high and FPS is consistently in the mid 90's. Now, Ray tracing on the other hand... Do not attempt. Just because it can, doesn't mean you should. 24FPS on low ray tracing, I forget what mid is, but high ray tracing brought me down to 9 fps. Not fun.\n\n7/10 would recommend again.",
                                    "score": 16,
                                    "author": "TakeThisNameToo"
                                },
                                {
                                    "level": 3,
                                    "comment": "&gt; Lots of people haven't upgraded since 1080s\n\nIsn't the 1060 the most common card?",
                                    "score": 6,
                                    "author": "DeliciousWaifood",
                                    "replies": [
                                        {
                                            "level": 4,
                                            "comment": "Yeah but it was also released twice (gddr4 in 2016 and gddr5 in 2018) so idk if that really counts... but either way you're right that's probably the audience they're targeting (if* they're not just fucking up because the crypto crash lmao)",
                                            "score": 4,
                                            "author": "SkavensWhiteRaven",
                                            "replies": []
                                        }
                                    ]
                                }
                            ]
                        },
                        {
                            "level": 2,
                            "comment": "it would also explain the curve of prices for each  hardware increment.\n\nMiners usually get, on average, more income if they have more processing power, specifically, the number of hashes they can produce and check per unit of second (hash power) (assuming Proof of Work).\n\nbefore, nvidia cards had this \"concave property\" that as the more price you go (3060 then 3070 then 3080 and 3090) the lesser amount of hardware specs you get for each price increment (thinking in simple, gpu cores and vram). if you were a miner buying using that price curve, you should have concentrated on buying a lot of 3060s instead of a few 3090s since buying in this form would give more mining processing power per second for the same price, taking the utility (= income - cost) higher since the income is better and the cost is the same.\n\nNow look at the curve for the 40xx, the 4080 makes the curve steep up, so additional increment of money spent  gets hardware specs for cheaper, and that happens again for the jump between 4080 and 4090 (even both ram capacities), so the optimal strategy for a supposed miner is to concentrate on buying the 4090 (although they may not be so prevalent because mining may even not be profitable in this crypto ice age, also considering PoS and that bitcoin is better mined with ASICS ). If crypto gets back, expect the 4090 being the first to stockout and then slowly creep on the lower end models\n\ntldr: the hardware v/s price curve nvidia presented for this generationof gpu incentivizes buying the 4090 if you gain monetary profits from gpu hardware specs",
                            "score": 38,
                            "author": "[deleted]",
                            "replies": []
                        },
                        {
                            "level": 2,
                            "comment": "&gt; I think they know their in for a bumpy ride now between inflation/recession trends, being all but certain through q4-q3 of 2023.\n\nFuck Nvidia.  They chose to ditch their core customer base for miners so they can eat a dick.",
                            "score": 9,
                            "author": "Idle_Redditing"
                        },
                        {
                            "level": 2,
                            "comment": "Nvidia needs new strategists. Their goals this cycle are wildly out of phase with reality. They thought the market would be the same as 2020/21.\n\nWell i. 2021 we were bored, and really needed an escape. Cyberpunk was driving part of the desire on the gamer side, and regular crypto growth was driving desire from miners. Inflation hadn't struck yet. Miners made up the VAST majority of people willing to spend big because they believed they'd make the money back and then some. As inflation and recession set in and crypto started diving, miners began selling rigs and parts. Gamers are also feeling the squeeze and console gaming is looking frugal again. Now it's funny to me that I could anticipate; especially once the ukraine war began, that the market would go to shit, yet nvidia thought they could capitalize on 2021's market in 2022/23. Dumb asses.",
                            "score": 18,
                            "author": "newbrevity",
                            "replies": []
                        },
                        {
                            "level": 2,
                            "comment": "Bleeding edge is a thrill and a trap. Best value is usually found with upper mid-range CPUs &amp; GPUs that are about one year old.\n\nSure, they'll last you only about 4 years instead of 6 years. But for 25%-50% of the price you would pay for the latest top model. Also, games aren't developed with 3090/4090 users in mind, there's just too few of them around.",
                            "score": 6,
                            "author": "Oli-Baba"
                        },
                        {
                            "level": 2,
                            "comment": "Or maybe, quite possibly, they\u2019re cooling the very hot chip powering the very powerful GPU.",
                            "score": 40,
                            "author": "275MPHFordGT40",
                            "replies": []
                        },
                        {
                            "level": 2,
                            "comment": "Well, they should be going for good temperatures anyways, and with the higher power draw it needs more cooling.\n\nRemember that the blower style 2 slot cards would be more or less always running at 82c during gaming, and generally not reaching its full potential.",
                            "score": 5,
                            "author": "TrymWS"
                        },
                        {
                            "level": 2,
                            "comment": "Even if i had the money, i wouldnt get the 4090. Sure it's got alot of new shit, but a 4070 or 4080 would probably be good enough to run the top games at high settings for like most of its lifespan",
                            "score": 4,
                            "author": "MrMakerHasLigma"
                        }
                    ]
                },
                {
                    "level": 1,
                    "comment": "It is wrong. What matters for mining isn't how powerful the GPU is, it's how efficient it is. It's much better to have two weak GPUs that are very power efficient than one high end that isn't as power efficient for the same price, because it will lead to more profits.\n\nThe 4090 will definitely not be the most efficient of the 40 lineup, so even assuming miners upgrade their rigs they won't go for that one.",
                    "score": 89,
                    "author": "ZeAthenA714",
                    "replies": [
                        {
                            "level": 2,
                            "comment": "Yes and no. It's just like a server, density matters a lot. Efficiency definitely plays a part but the 4090 isn't inefficient just fat.",
                            "score": 42,
                            "author": "SkavensWhiteRaven",
                            "replies": [
                                {
                                    "level": 3,
                                    "comment": "True, efficiency isn't the only variable. But like you say, density matters, and the 4090 is a very bad card in terms of power/size.\n\nIf Nvidia wanted to make a card to target miners, the 4090 would be a very piss poor attempt at it.",
                                    "score": 18,
                                    "author": "ZeAthenA714",
                                    "replies": []
                                }
                            ]
                        }
                    ]
                }
            ]
        },
        {
            "level": 0,
            "comment": "95% of every GPU is the heatsink. Put it on water and it's small again.\n\nNow all you need are 4x720mm radiators!",
            "score": 1563,
            "author": "CalvinHobbesN7",
            "replies": [
                {
                    "level": 1,
                    "comment": "[deleted]",
                    "score": 558,
                    "author": "[deleted]",
                    "replies": [
                        {
                            "level": 2,
                            "comment": "I live by a lake is that a big enough reservoir?",
                            "score": 192,
                            "author": "harpalss",
                            "replies": [
                                {
                                    "level": 3,
                                    "comment": "Only until the 5090 releases",
                                    "score": 195,
                                    "author": "newontheblock99",
                                    "replies": [
                                        {
                                            "level": 4,
                                            "comment": "I live by an ocean is that a big enough reservoir?",
                                            "score": 36,
                                            "author": "crybllrd",
                                            "replies": [
                                                {
                                                    "level": 5,
                                                    "comment": "You would need 3 planets for a 7090 at this rate",
                                                    "score": 42,
                                                    "author": "Bloxxy213",
                                                    "replies": [
                                                        {
                                                            "level": 6,
                                                            "comment": "Guess it's time to terraform Mars and Venus.",
                                                            "score": 13,
                                                            "author": "razrblck",
                                                            "replies": [
                                                                {
                                                                    "level": 7,
                                                                    "comment": "I can terraform Uranus",
                                                                    "score": 11,
                                                                    "author": "stoic_ferret",
                                                                    "replies": []
                                                                }
                                                            ]
                                                        },
                                                        {
                                                            "level": 6,
                                                            "comment": "3 planets for cooling, or 3 planets for powering?",
                                                            "score": 13,
                                                            "author": "MC_chrome",
                                                            "replies": [
                                                                {
                                                                    "level": 7,
                                                                    "comment": "You confused it with the 5090. The 5090 needs 3 planets for power, the 7090 needs 12, duh!",
                                                                    "score": 11,
                                                                    "author": "Bloxxy213",
                                                                    "replies": [
                                                                        {
                                                                            "level": 8,
                                                                            "comment": "Nah, 7090 needs a sun for power. Planets don\u2019t provide enough.",
                                                                            "score": 8,
                                                                            "author": "notrufus",
                                                                            "replies": [
                                                                                {
                                                                                    "level": 9,
                                                                                    "comment": "Nvidia expects casual users in 2300 to own a few galaxies",
                                                                                    "score": 7,
                                                                                    "author": "Bloxxy213",
                                                                                    "replies": []
                                                                                }
                                                                            ]
                                                                        }
                                                                    ]
                                                                }
                                                            ]
                                                        }
                                                    ]
                                                }
                                            ]
                                        }
                                    ]
                                },
                                {
                                    "level": 3,
                                    "comment": "time to build the worlds first lake-cooled pc",
                                    "score": 30,
                                    "author": "siraweed",
                                    "replies": [
                                        {
                                            "level": 4,
                                            "comment": "We have one of those in Utah!",
                                            "score": 7,
                                            "author": "tachyonfield",
                                            "replies": [
                                                {
                                                    "level": 5,
                                                    "comment": "Will hook every system in the city to the lake.\n\nBOOM A geothermal lake",
                                                    "score": 5,
                                                    "author": "TechkyJerry"
                                                }
                                            ]
                                        },
                                        {
                                            "level": 4,
                                            "comment": "World's First lake-sized hot springs.",
                                            "score": 5,
                                            "author": "Toadsted"
                                        }
                                    ]
                                }
                            ]
                        }
                    ]
                },
                {
                    "level": 1,
                    "comment": "Damn, my water cooled CPU already took the rest of the available place, and the front mounted radiator is limiting the length of the GPU. So I might have to return to an air cooled CPU to install the new 40X0 cards. \nBut they are so expensive... I will stay with my 3080.",
                    "score": 49,
                    "author": "Exodard",
                    "replies": [
                        {
                            "level": 2,
                            "comment": "Just a tip/reminder, but radiators dont have to actually fit *inside* the pc. When you're doing water cooling, whether AIO or custom loop, you don't need the case walls either.",
                            "score": 40,
                            "author": "Toadsted",
                            "replies": [
                                {
                                    "level": 3,
                                    "comment": "You don't need the case at all. Double sided tape your PC to the wall, and the room becomes the case.",
                                    "score": 43,
                                    "author": "knbang",
                                    "replies": [
                                        {
                                            "level": 4,
                                            "comment": "seems insecure to me\n\nI\u2019d drive some nails through the mobo just to be safe",
                                            "score": 10,
                                            "author": "BoonesFarmJackfruit"
                                        },
                                        {
                                            "level": 4,
                                            "comment": "Using your wall as the breadboard takes me back, like having a wall to store your hand tools in the shop.",
                                            "score": 9,
                                            "author": "Toadsted"
                                        },
                                        {
                                            "level": 4,
                                            "comment": "Double sided tape to the outside of your house and THE WHOLE WORLD becomes the case\n\n&amp;#x200B;\n\n^(might destroy your PC parts)",
                                            "score": 7,
                                            "author": "Omnilatent"
                                        }
                                    ]
                                },
                                {
                                    "level": 3,
                                    "comment": "[You're not wrong](https://www.titanrig.com/blog/post/watercools-mo-ra3-radiator-system)",
                                    "score": 5,
                                    "author": "AnonyDexx"
                                }
                            ]
                        }
                    ]
                }
            ]
        },
        {
            "level": 0,
            "comment": "I think EVGA knew what they were doing when they decided to cut ties.",
            "score": 101,
            "author": "hatnscarf",
            "replies": [
                {
                    "level": 1,
                    "comment": "So glad my 3080ti is EVGA. Will be sitting on this for a while.",
                    "score": 27,
                    "author": "mybachhurts",
                    "replies": []
                }
            ]
        },
        {
            "level": 0,
            "comment": "Are they IPX rated too so they can get hosed down when they get dusty \ud83e\udd23",
            "score": 253,
            "author": "ICO_Agro",
            "replies": [
                {
                    "level": 1,
                    "comment": "Anything is IPX if you wait enough before turning them on again lol",
                    "score": 137,
                    "author": "Super_Cheburek",
                    "replies": [
                        {
                            "level": 2,
                            "comment": "Not if you live somewhere with hard water",
                            "score": 57,
                            "author": "generalthunder",
                            "replies": [
                                {
                                    "level": 3,
                                    "comment": "I use liquid water for cleaning, not ice cubes \ud83d\ude44 /j",
                                    "score": 108,
                                    "author": "Super_Cheburek",
                                    "replies": [
                                        {
                                            "level": 4,
                                            "comment": "Solid choice",
                                            "score": 44,
                                            "author": "bot_licker",
                                            "replies": []
                                        }
                                    ]
                                }
                            ]
                        }
                    ]
                }
            ]
        },
        {
            "level": 0,
            "comment": "Nah, it's that big because it's extremely power inefficient. Nvidia is very insecure about Radeon coming back in a big way, so they threw power efficiency out the window to say they have the fastest GPU.",
            "score": 1586,
            "author": "DarkKratoz",
            "replies": [
                {
                    "level": 1,
                    "comment": "Now imagine of you shunt mod a 4090!  Get that card drawing near 1kW",
                    "score": 414,
                    "author": "erikwarm",
                    "replies": [
                        {
                            "level": 2,
                            "comment": "We are gonna need a whole 'nother power supply just for the GPU!",
                            "score": 208,
                            "author": "the-sin-farmer",
                            "replies": [
                                {
                                    "level": 3,
                                    "comment": "Might as well integrate it in the card. Get a dedicated power connector next to all the display connections",
                                    "score": 124,
                                    "author": "erikwarm",
                                    "replies": [
                                        {
                                            "level": 4,
                                            "comment": "How far are we from the GPU card being the center of the case design?",
                                            "score": 87,
                                            "author": "tomerjm",
                                            "replies": [
                                                {
                                                    "level": 5,
                                                    "comment": "Who\u2019s the motherboard now Bitch!",
                                                    "score": 105,
                                                    "author": "erikwarm",
                                                    "replies": [
                                                        {
                                                            "level": 6,
                                                            "comment": "Mothercube*",
                                                            "score": 24,
                                                            "author": "Toadsted"
                                                        },
                                                        {
                                                            "level": 6,
                                                            "comment": "It\u2019s fatherboard now",
                                                            "score": 7,
                                                            "author": "PRODSKY22"
                                                        }
                                                    ]
                                                },
                                                {
                                                    "level": 5,
                                                    "comment": "With the trend of this going, pretty soon we will plug motherboard into GPU instead.",
                                                    "score": 29,
                                                    "author": "PretendRegister7516"
                                                }
                                            ]
                                        },
                                        {
                                            "level": 4,
                                            "comment": "Plug that baby straight into the main line, no surge protector, no UPS, 100% rawdog for highest fps, I promise it works 100% my dad works for Apple",
                                            "score": 29,
                                            "author": "ner0417"
                                        },
                                        {
                                            "level": 4,
                                            "comment": "If i remember correctly there was a card like that back in the 90s",
                                            "score": 8,
                                            "author": "Evantaur",
                                            "replies": [
                                                {
                                                    "level": 5,
                                                    "comment": "Voodoo 5 i think.",
                                                    "score": 4,
                                                    "author": "counter-strike",
                                                    "replies": []
                                                }
                                            ]
                                        }
                                    ]
                                },
                                {
                                    "level": 3,
                                    "comment": "You gonna need a whole new circuit breaker just for the GPU.",
                                    "score": 9,
                                    "author": "bargu"
                                },
                                {
                                    "level": 3,
                                    "comment": "Now we know why der8auer wanted two PSU mounts in the O11D",
                                    "score": 5,
                                    "author": "rpungello"
                                }
                            ]
                        },
                        {
                            "level": 2,
                            "comment": "That is literally A.C. territory",
                            "score": 6,
                            "author": "anshulkhatri13",
                            "replies": []
                        },
                        {
                            "level": 2,
                            "comment": "1 kW isn't that hard to draw. I mean it is, but it ain't. Now, let's start talking about kWh's. Let's see what it does under a sustained load.",
                            "score": 11,
                            "author": "TakeThisNameToo",
                            "replies": []
                        }
                    ]
                },
                {
                    "level": 1,
                    "comment": "This isn't even the fully enabled chip. There's going to be a 4090 ti that can use even more power",
                    "score": 89,
                    "author": "ZubZubZubZubZubZub",
                    "replies": [
                        {
                            "level": 2,
                            "comment": "And we\u2019re all taking nvidias crazy pills by caring about this card for gaming rather than sequestering it as a professional grade Titan card. They knew all they had to do was change the name and it would get gamers chomping at the bit.",
                            "score": 36,
                            "author": "03Titanium",
                            "replies": []
                        }
                    ]
                },
                {
                    "level": 1,
                    "comment": "I'm sure it's that simple",
                    "score": 58,
                    "author": "Careful-Combination7",
                    "replies": [
                        {
                            "level": 2,
                            "comment": "[deleted]",
                            "score": 27,
                            "author": "[deleted]",
                            "replies": [
                                {
                                    "level": 3,
                                    "comment": "It is toasty because it uses more power than an actual toaster. Why is just as simple as looking at how close Radeon is getting to them in gaming and other performance tasks.",
                                    "score": 12,
                                    "author": "sparklyboi2015"
                                }
                            ]
                        },
                        {
                            "level": 2,
                            "comment": "What is it ockams razor? \"The simple solution is often the right one\"",
                            "score": 98,
                            "author": "funnybreadman",
                            "replies": []
                        }
                    ]
                }
            ]
        },
        {
            "level": 0,
            "comment": "[deleted]",
            "score": 168,
            "author": "[deleted]",
            "replies": [
                {
                    "level": 1,
                    "comment": "That does explain why the FE is so much smaller than all the AIB cards, if Nvidia has a history of not communicating changes to AIB partners until the last moment, like the MSRP as EVGA claims.\n\nThe 4090 FE is still a 3 slot GPU that's marginally shorter than the 3090 FE, while AIB cards like that 4-5 slot Zotac 4090 dwarfs it by comparison",
                    "score": 90,
                    "author": "BaitSimulator2020",
                    "replies": [
                        {
                            "level": 2,
                            "comment": "yeah the AIBs are monstrous, but the FE is basically the same size but thicker.",
                            "score": 8,
                            "author": "gokarrt"
                        }
                    ]
                },
                {
                    "level": 1,
                    "comment": "This.\n\nhttps://www.igorslab.de/en/nvidia-geforce-rtx-4090-where-the-mistake-with-the-600-watt-really-comes-from-and-why-the-cards-are-so-giant/",
                    "score": 11,
                    "author": "staxx6"
                }
            ]
        },
        {
            "level": 0,
            "comment": "This actually makes a ton of sense because miners made up a ton of their profits last year and the miner market collapsed overnight last month so it makes sense that they would make another series of cards dedicated to that market.\n\nNvidia hasn't cared about gamers in the least bit since the 10 series. That's why those are some of the best gamer cards they've ever made. After that the cards are more catered toward other things.",
            "score": 26,
            "author": "EJohns1004"
        },
        {
            "level": 0,
            "comment": "That is the thing, it probably *was* supposed to go into mining rigs! [*fingerguns*] That and \"Look, more power use so more power out, yes we innovate!\"",
            "score": 448,
            "author": "UseThEreDdiTapP",
            "replies": [
                {
                    "level": 1,
                    "comment": "Doesn\u2019t it have the same TDP as a 3090 ti?",
                    "score": 49,
                    "author": "zen1706",
                    "replies": [
                        {
                            "level": 2,
                            "comment": "Nope it doesn't scale power per wat the same as a 3090. It does however have a significantly higher tdp.",
                            "score": 31,
                            "author": "Edwardteech",
                            "replies": [
                                {
                                    "level": 3,
                                    "comment": "Uh call me crazy but, it says on Nvidia website the 3090 ti\u2019s GCP is 450W. \n\nhttps://www.nvidia.com/en-us/geforce/graphics-cards/30-series/rtx-3090-3090ti/\n\nSame amount as the 4090.\n\nAnd yeah no shit the 4090 obviously has more \u201cperformance per watt\u201d compared to the 3090 TI.",
                                    "score": 16,
                                    "author": "zen1706",
                                    "replies": []
                                }
                            ]
                        }
                    ]
                },
                {
                    "level": 1,
                    "comment": "No, that's very unlikely. First of all, mining is slowly dying out for multiple reasons. But more importantly, the 4090, would be a horrible mining card, it's super inefficient. Indeed it does generate more coins than other cards, but it does so at a much higher wattage. And electrical energy ain't free. \n\nSo slower cards which draw less power are usually the ones that make the most profit. That's also exactly the reason why every miner that knows what he's doing, is undervolting their cards. I always laugh my ass off when I see somebody overclock their GPU to the max (usually in their personal rig), and just let it mine some coins at night or when not using the pc.\n\nYes, they might be getting 20% more coins, but their computer draws up to 40-50% more power. So in that scenario it wouldn't even be unlikely for them to lose money. Lots of people forget about computers upkeep cost. If people would see the charts I was seeing daily at my old job, they would change their mind.\n\nI was an IT consultant at a big corporation. One of the charts I was talking about was the upkeep of all our computers (we had a few thousand across hundreds of locations). Solely the electrical bill for the computers alone (excluding monitors, peripherals and other accessories), was up in the tens of thousands of euros.",
                    "score": 16,
                    "author": "TheCreepyPL",
                    "replies": []
                },
                {
                    "level": 1,
                    "comment": "The biggest crypto that used to be dependant on GPU for mining has been working for years on creating a system that is more environmental-friendly. They have announced a few years back that 2021-2022 is the target for them to introduce the new version. Would be weird if NVDIA created a whole generation of card when it was pretty clear that it will be obsolete on arrival.",
                    "score": 29,
                    "author": "seppukuAsPerKeikaku",
                    "replies": [
                        {
                            "level": 2,
                            "comment": "If you are referring to Ethereum, it has already happened.\n\n* [\u2018The Merge\u2019 unplugged Ethereum from crypto mining](https://www.morningbrew.com/incrypto/stories/2022/10/06/the-merge-unplugged-ethereum-from-crypto-mining)\n* [Post-Ethereum Merge, Is Cryptomining Losing Its Value Proposition? ](https://marketscale.com/industries/software-and-technology/post-ethereum-merge-is-cryptomining-losing-its-value-proposition/)",
                            "score": 37,
                            "author": "Icy_Application"
                        }
                    ]
                }
            ]
        },
        {
            "level": 0,
            "comment": "Maybe it should have been a mining GPU to begin with.",
            "score": 178,
            "author": "CryptographerBulky86",
            "replies": [
                {
                    "level": 1,
                    "comment": "Most gpu crypto farms were mining Ethereum and a recent change in their tech made gpu mining impossible. So selling graphics cards for mining is going to be a lot less profitable now.",
                    "score": 88,
                    "author": "ShadyGuy_",
                    "replies": [
                        {
                            "level": 2,
                            "comment": "Ethereum mining was replaced altogether with \"proof of stake\" - basically bank account interest.",
                            "score": 74,
                            "author": "Nurgus",
                            "replies": [
                                {
                                    "level": 3,
                                    "comment": "&gt; basically bank account interest.\n\nNot basically, it is exactly that. You earn because you support the system essentially, (same way banks operated except they made the money on stock market and not by simply printing money).",
                                    "score": 42,
                                    "author": "photenth",
                                    "replies": [
                                        {
                                            "level": 4,
                                            "comment": "No, not exactly that. You can get interest by simply having money in the bank. You don't get interest on ETH by simply holding it in a wallet - you have to \"lock\" it, and you can't sell it or use it until it's unlocked.",
                                            "score": 19,
                                            "author": "Chewbacker",
                                            "replies": [
                                                {
                                                    "level": 5,
                                                    "comment": "But that's still kinda the same though because today the only accounts (if any) that actually earn interest are accounts where you can't just withdraw money from. Back in the days where bank accounts had interests, the highest interest accounts were the type where you couldn't just withdraw the money.",
                                                    "score": 15,
                                                    "author": "photenth",
                                                    "replies": []
                                                }
                                            ]
                                        }
                                    ]
                                }
                            ]
                        }
                    ]
                }
            ]
        },
        {
            "level": 0,
            "comment": "I think its so big because nvidia couldnt handle the heat, which is a reason against the card, my 1500$ radiator works with water to heat my room lul",
            "score": 66,
            "author": "MarineAhoy",
            "replies": []
        },
        {
            "level": 0,
            "comment": "Ive got a 3070 in my desktop, my 7 year laptop still does the job with a 1070, i think im good with a 3070 for at least 7 years",
            "score": 13,
            "author": "Unethica-Genki"
        },
        {
            "level": 0,
            "comment": "Jokes aside this might actually be it",
            "score": 321,
            "author": "danwilan",
            "replies": [
                {
                    "level": 1,
                    "comment": "nvidiaman : omg they figured it out",
                    "score": 76,
                    "author": "sciencewonders"
                },
                {
                    "level": 1,
                    "comment": "It's big because the planned with inefficient Samsung 8mm chips for Ada, so the MIBs bought big ass heatsinks.\n\nThey are now using more efficient 4nm TSMC chips for Ada, so the Heatsinks are overkill. These cards will run very quietly because the cooling solution was designes for more power than is needed right now.",
                    "score": 41,
                    "author": "waxzR",
                    "replies": [
                        {
                            "level": 2,
                            "comment": "The cards will easily consume over 400W, and even over 600W [with Furmark](https://www.hardwaretimes.com/nvidia-rtx-4090-draws-over-600w-of-power-in-certain-benchmarks-can-be-overclocked-to-3ghz-or-more/)\n\n\nThey will absolutely need this big heatsink\n\n\nEdit: downvote me all you want just for not blindly following your opinion.  I see it as an achievement",
                            "score": 36,
                            "author": "Thomas9002",
                            "replies": []
                        }
                    ]
                }
            ]
        },
        {
            "level": 0,
            "comment": "I think it's more to do with the fact moore's law stopped being completely true about a decade ago. We're approaching the silicon wall, you can only make silicon logic gates so small before they can no longer function, and we're basically there already so the only real recourse is to make stuff larger",
            "score": 57,
            "author": "Macksimoose",
            "replies": [
                {
                    "level": 1,
                    "comment": "Exactly. The rate of improvements have plummeted over the last 10-15 years. \n\n5950X PBO has the same performance of a 7950X if we enable 65w lock *but* the jump in perf/w is much lower than before.\n\n3060 can be UVed|OCed to match 1080 ti performance with marginally better efficiency. Again this a very minor improvement since it's 2 generations apart",
                    "score": 27,
                    "author": "Viztiz006",
                    "replies": [
                        {
                            "level": 2,
                            "comment": "Supposedly we have ideas on how to improve shit for another 10 years or so, and the big assumption is that we'll develop new tech within that 10 years that allows us to either continue wringing improvements out of silicon or move beyond it.\n\nPersonally, I'm excited to see where tech ends up going.",
                            "score": 12,
                            "author": "Megneous",
                            "replies": [
                                {
                                    "level": 3,
                                    "comment": "Intel is planning to switch from nanometers to \u00e5ngstorm units (approximate units but the density is larger). So my guess is that we still have some hope till 2026 atleast",
                                    "score": 4,
                                    "author": "Viztiz006"
                                },
                                {
                                    "level": 3,
                                    "comment": "Probably more specialized cpus, like the gpu",
                                    "score": 5,
                                    "author": "33Yalkin33"
                                }
                            ]
                        }
                    ]
                }
            ]
        },
        {
            "level": 0,
            "comment": "Not sure if this was intended to be a backhanded remark...\n\nOn one hand, it sounds ***really dumb***, that a multi-billion--if not trillion, dollar company would appeal to a crowd of people grossly misusing their hardware, akin to Ford or Honda designing gasoline cars for people that use them as rudimentary generators.\n\nBut then I remembered the reason why nVidia is sitting on an absolute glut of parts, because they very well intended for miners to be picking them up before the market crashed, because hey, *they're still moving product, right?*\n\nI think it really comes down to how far out they had these planned, or maybe those fears of power draw really weren't exaggerated, and it really needs that beefy of a cooler, and this is all just a PR-spin.  I don't know, it's stuff like this where you look back on it and remember, *yeah those 40-series of cards were a* ***thing****.*\n\nThat it actually happened, the artifact being the 4090, *and it's insanely huge cooler design and power requirements.*",
            "score": 6,
            "author": "y0ungfreshpickles",
            "replies": []
        },
        {
            "level": 0,
            "comment": "And here i am with my 3gb 1060",
            "score": 5,
            "author": "Aland20a"
        },
        {
            "level": 0,
            "comment": "Wait, I thought we were all supposed to be boycotting the 40xx, anyway?\n\nI can't keep up.",
            "score": 5,
            "author": "Geek_Verve"
        },
        {
            "level": 0,
            "comment": "When in reality all NV needed to do was invest $0.02 in higher quality thermal pads.\n\nGiggitybyte, looking at you too.",
            "score": 5,
            "author": "dennispang"
        },
        {
            "level": 0,
            "comment": "It's because common sense went with EVGA when they left.",
            "score": 15,
            "author": "D9Dagger",
            "replies": []
        },
        {
            "level": 0,
            "comment": "[deleted]",
            "score": 15,
            "author": "[deleted]",
            "replies": [
                {
                    "level": 1,
                    "comment": "Laughs in GT635m",
                    "score": 5,
                    "author": "Sidotre",
                    "replies": [
                        {
                            "level": 2,
                            "comment": "*laughs in RX580 that failed just yesterday*",
                            "score": 8,
                            "author": "Xx_MW2360noscope_xX",
                            "replies": []
                        }
                    ]
                }
            ]
        },
        {
            "level": 0,
            "comment": "Based Skeletor",
            "score": 29,
            "author": "[deleted]"
        },
        {
            "level": 0,
            "comment": "Ok but how many real world applications does a 4090 have outside of mining. How many people are gaming in 8k. Or 4k 240hz. Id imagine a tiny fraction of even the people who can afford a 4090",
            "score": 36,
            "author": "Jamesaya",
            "replies": [
                {
                    "level": 1,
                    "comment": "The 4090 isn't going to be able to clear 4k 120hz native in any AAA game with RT. This is a halo product, the cost is never going to make sense. People will still go out and buy a Ferrari.",
                    "score": 31,
                    "author": "thrownawayzss"
                },
                {
                    "level": 1,
                    "comment": "4090 can't even game in 4K 240Hz, still using the ancient DP 1.4 connector.",
                    "score": 32,
                    "author": "[deleted]",
                    "replies": []
                },
                {
                    "level": 1,
                    "comment": "This is like saying gtx 295 is overkill in 2008 because who plays nfs most wanted in 4k?\nGames adjust to new hardware and become more and more demanding.",
                    "score": 14,
                    "author": "paschenflush",
                    "replies": [
                        {
                            "level": 2,
                            "comment": "the thing is that most games adjust to the hardware most people is using, yes,, there are games really heavi like rdr2 and cyberpunk but they still run in a gtx970 (I was using one until just a couple months ago when I upgraded to a 3060) for that reason most AAA games still work in aa 1060",
                            "score": 5,
                            "author": "x88dragon"
                        }
                    ]
                }
            ]
        },
        {
            "level": 0,
            "comment": "It consumes more power on its own than my rig with a 1070",
            "score": 5,
            "author": "ThagaardJunior",
            "replies": []
        },
        {
            "level": 0,
            "comment": "most of the people won't go for it probably. 3090 is the max one needs for gaming tbh.\n\nMaybe 4090 will be useful for mostly productive works.\n\nI won't be surprised if they really designed it for miners considering the amount of profit they raked in during the heyday of crypto.",
            "score": 6,
            "author": "player_120"
        },
        {
            "level": 0,
            "comment": "Or, hear me out, they tried to get the most performance out of their silicon as possible by simply ramping up the clocks and power draw until it stopped making sense...\n\nAnd then they realized they made a 450-600W monster that would need an equally monstrous cooling solution to handle.\n\nAnd then they realized that people would meme the hell out of the giant GPU, and probably find them cool as hell, and undervolting would solve any thermal issues anyways, and... here we are.\n\nI'm not worried about the size, I'm worried about:\n\n1. The pricing making fuck all sense.\n\n2. The 4080 12GB being a fucking lie. That is a 4070 and I'm gonna call it that.\n\nI will say, that one advantage for having a giant GPU cooler, is that now many AIB cards are actually using 120mm fans, so they should be a lot less noisy.",
            "score": 12,
            "author": "Matasa89",
            "replies": []
        },
        {
            "level": 0,
            "comment": "Skeletor speaks the truth!",
            "score": 9,
            "author": "TheDevilsAdvokaat"
        },
        {
            "level": 0,
            "comment": "I have never been more happy for the crypto crash than I am now.\n\nGamers were the fans and avid supporters for nVidia. Then nVidia went to the dark side, abandoned the customers that made them what they are, and are now asking us to come back after they fucked us in the ass with price gouging and losing focus.\n\nAnd I'm all here for their corporate restructuring as they have to start firing execs that pushed the crypto shit.\n\n**You get what you deserve.**\n\n&amp;#x200B;\n\n&amp;#x200B;\n\nEdit: **Schadenfreude intensifies**",
            "score": 9,
            "author": "Newish_Username"
        },
        {
            "level": 0,
            "comment": "90 variants of cards wouldn't be very good for mining since they're meant to be the bridge between workstation and gaming GPUs. They've never been very power efficient with 90 variants.\n\nIf I were a miner myself I'd usually stick with the 60 or 60 ti variants since they are usually power efficient enough and have enough vram as well as being cost effective so someone could get multiple instead of just one.\n\nYes I know it's a meme, I just think too much :(",
            "score": 17,
            "author": "Mongba36",
            "replies": [
                {
                    "level": 1,
                    "comment": "100% correct. No one in their right mind would use 90 series for actually efficient mining. Even the 80 series cards are not good bang for your buck. Most miners undervolt their cards because your profits go out the window if your electricity usage is too high. They find the optimum hash-rate to power usage, and that's usually on the mid-tier cards. There are calculators that can estimate this, as well as programs that can automatically adjust your card performance to balance it for you.\n\nAnd if you take it further, **real miners** don't use GPUs for mining, they use ASICs, which are way more power efficient with way higher hash-rates. \n\nAnyone saying that this card was designed for mining has no idea what they are talking about. The cooling is so large because it's producing an extreme amount of heat. And it produces an extreme amount of heat because it's a massive processor that is running really high. It may be energy inefficient, and probably is, just like most 80/80 ti series cards. People that run these things at full tilt don't care about the energy costs or heat, they only care about the performance.",
                    "score": 9,
                    "author": "Baerog"
                }
            ]
        }
    ]
}