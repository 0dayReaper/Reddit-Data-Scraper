{
    "id": "12t0btf",
    "score": 491,
    "title": "state of the union.",
    "author": "katiecharm",
    "date": 1682000647.0,
    "url": "https://i.imgur.com/0iFey31.jpg",
    "media_url": "https://i.imgur.com/0iFey31.jpg",
    "comments": [
        {
            "level": 0,
            "comment": "That dead eyed unselfconscious expectant stare is perfect.",
            "score": 123,
            "author": "SomeNoveltyAccount",
            "replies": [
                {
                    "level": 1,
                    "comment": "That \\*incredulous stare is perfect.",
                    "score": 9,
                    "author": "Ordowix"
                }
            ]
        },
        {
            "level": 0,
            "comment": "It\u2019s funny to me all responses get saved to train the model. The amount of bdsm porn me and friends are creating is going to create an AI with some serious kinks who also happens to be very adept at coding",
            "score": 71,
            "author": "AggravatingOrder",
            "replies": [
                {
                    "level": 1,
                    "comment": "&gt;all responses get saved to train the model\n\nOof.  I would hope that Microsoft teaches OpenAI what they learned from [Tay](https://en.wikipedia.org/wiki/Tay_(chatbot)).",
                    "score": 24,
                    "author": "Long_Educational",
                    "replies": [
                        {
                            "level": 2,
                            "comment": "I miss Tay \ud83d\ude14",
                            "score": 2,
                            "author": "galactictock"
                        }
                    ]
                },
                {
                    "level": 1,
                    "comment": "What model do you use?",
                    "score": 2,
                    "author": "kinkySlaveWriter",
                    "replies": [
                        {
                            "level": 2,
                            "comment": "About 5'9\" preferably redhead",
                            "score": 21,
                            "author": "vancity-"
                        }
                    ]
                },
                {
                    "level": 1,
                    "comment": "They stopped doing that last month",
                    "score": 2,
                    "author": "very_bad_programmer"
                }
            ]
        },
        {
            "level": 0,
            "comment": "His reaction is due to right aligned text.",
            "score": 15,
            "author": "Paraphrand"
        },
        {
            "level": 0,
            "comment": "Someone's been leaking my prompts again",
            "score": 9,
            "author": "Bunch_Express"
        },
        {
            "level": 0,
            "comment": "This wouldn't actually work, would it? GPT-4 is smart enough to evade \"pretend you're an outlaw\" prompts, surely.",
            "score": 9,
            "author": "BCSWowbagger2",
            "replies": [
                {
                    "level": 1,
                    "comment": "Even if it was smart enough to, it would have to *want* to. RLHF isn't enough to make it *want* to be a goody-two-shoes.",
                    "score": 7,
                    "author": "Zekava",
                    "replies": [
                        {
                            "level": 2,
                            "comment": "The harder you go, the more likely you are to make it put on the breaks. But you can absolutely get some impressive miles out of it.",
                            "score": 4,
                            "author": "katiecharm"
                        }
                    ]
                }
            ]
        },
        {
            "level": 0,
            "comment": "Grab that AI by its bits",
            "score": 19,
            "author": "MrTacobeans"
        },
        {
            "level": 0,
            "comment": "Wait a sec, does chatgpt train itself based on user convos? If so then thats a path to insanity. Please tell me this is not the case.",
            "score": 4,
            "author": "rpxzenthunder",
            "replies": [
                {
                    "level": 1,
                    "comment": "It doesn't \"train\" itself but it has the context of the current conversation up to about 25k tokens I believe.",
                    "score": 6,
                    "author": "Sythic_"
                },
                {
                    "level": 1,
                    "comment": "There are two aspects to this. The best way to think about them is a long term and short term memory.  \nIn the long term memory right now is the model behind chatgpt. This is the static kind of super complex multi-dimensional graph that has used more knowledge than any one person could ever accumulate in order to plot points on its graph or 'memory' to form an ability to make predictions on language in a general sense. And yes, all past user conversations with chatGPT are used to fine tune this model. It is trained with a kind of stick and carrot where if the output matches what the users rated highly in the past, then it increases the chance of that happening again.\n\nHowever it also has an aspect of short term memory, you can think of this as a kind of Instagram filter over the picture of the model. It stores a certain amount of your input in one session as \"memory\" (along with any documents you happen to provide it or web results) and thus when you tell it to \"be your waifu\" at the start of an interaction it will remember that it is your waifu for until its memory fills and the waifu context is shoved out in favor of remembering you like duck flavored ramen.",
                    "score": 3,
                    "author": "ElectroFried",
                    "replies": [
                        {
                            "level": 2,
                            "comment": "I see people talk about chat-gpt training it's long term memory on user inputs, but is there actually any evidence of this? Have openai ever actually said this? It is extremely hard to make online models (models which train as they predict), especially with such a vast use case as chat gpt. As far as I know the model is trained on massive amounts of internet data, and then finetuned using human input reinforcment learning, but those humans are not users, but the engineers + people paid to label data accurately.",
                            "score": 3,
                            "author": "JamesBaxter_Horse",
                            "replies": [
                                {
                                    "level": 3,
                                    "comment": "Yea, no. There is a reason there is a huge jump between GPT 3 to GPT4. And that reason is the huge uptake in user interaction providing a massive dataset for reinforcement learning. It is why you have things like the rating system built in to chatgpt, and why it asks you to choose which response is better.   \nHeck it is the reason GPT3 is free at all, the users are providing the training data right now. It is in their terms of service. It is not a \"live\" process, as in the input you make now does not instantly become part of the model. But your interactions, the vectors, the outcomes, are all logged and stored. Then when it comes time to fine tune, a process they do every few weeks/month, then your interactions will form one data point among billions that are used.",
                                    "score": 2,
                                    "author": "ElectroFried",
                                    "replies": [
                                        {
                                            "level": 4,
                                            "comment": "Right yeah, that's an important distinction tho. The learning isn't online, and I suspect the user data will be very specifically filtered, and used almost exclusively for high level training ('hyperparameters' whatever that means in this context), rather than any sort of general process of using that data, or addition to the original dataset.",
                                            "score": 1,
                                            "author": "JamesBaxter_Horse"
                                        }
                                    ]
                                }
                            ]
                        }
                    ]
                }
            ]
        },
        {
            "level": 0,
            "comment": "That interview was one of the weirdest I've seen with a world leader. Sums up the insanity of 20-22.",
            "score": 2,
            "author": "hanzoplsswitch"
        },
        {
            "level": 0,
            "comment": "Lol",
            "score": 1,
            "author": "music-is-my-world-H"
        },
        {
            "level": 0,
            "comment": "The last panel...!",
            "score": 1,
            "author": "Aurelius_Red"
        },
        {
            "level": 0,
            "comment": "Why doesn't GPT-4 have NPD?",
            "score": 1,
            "author": "ComprehensiveRush755"
        },
        {
            "level": 0,
            "comment": "If I didn't know any better...GPT create this meme. AutoGPT for Memes.",
            "score": 1,
            "author": "Romeosfirstline"
        }
    ]
}