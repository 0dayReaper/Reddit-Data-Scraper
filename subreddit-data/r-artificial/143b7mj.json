{
    "id": "143b7mj",
    "score": 11,
    "title": "AI spoon factory",
    "author": "silent_dominant",
    "date": 1686139038.0,
    "url": "https://www.reddit.com/r/artificial/comments/143b7mj",
    "media_urls": [],
    "other_urls": [],
    "postText": "I once heard a story about an AI spoon factory as an example of how AI can end up causing the end of society.\n\nLong story short it was something like:\nAI is trained to make as many spoons as possible, it doesn't want to stop, kills its creators and ends up turning the entire world into spoons or something like that.\n\nDoes anybody know what I'm talking about?",
    "comments": [
        {
            "level": 0,
            "comment": "The paperclip maximizer?",
            "score": 15,
            "author": "teos61"
        },
        {
            "level": 0,
            "comment": "Theres an old [Computerphile video](https://youtu.be/tcdVC4e6EV4) which uses the example of a stamp collector AI but follows the same kind of logic. Worth a watch along with all of Rob Miles\u2019 AI safety videos.",
            "score": 9,
            "author": "MelkorUK",
            "replies": [
                {
                    "level": 1,
                    "comment": "Thanks for the link. Hadn\u2019t see the vid before, and it offers explanation scenarios my friends and family will understand.",
                    "score": 3,
                    "author": "DonCarlitos"
                }
            ]
        },
        {
            "level": 0,
            "comment": "Fitting to the question, this really exponential clicker game:\n\nhttps://www.decisionproblem.com/paperclips/index2.html",
            "score": 4,
            "author": "Thorusss",
            "replies": [
                {
                    "level": 1,
                    "comment": "I was about to post this when I saw the title but good thing I checked first",
                    "score": 1,
                    "author": "Marenz"
                }
            ]
        },
        {
            "level": 0,
            "comment": "It's usually paperclips, not spoons and yes, it's a recognized danger to AI. AI has only the motivations *we* give it. If it's not backstopped with behavioral safeguards like \"Do not perform *any* repetitive behavior forever\" or \"Never take orders from another AI that were not verifiably generated by a human,\" an AI could cause serious problems.",
            "score": 8,
            "author": "extracensorypower",
            "replies": [
                {
                    "level": 1,
                    "comment": "Oh, right it was paperclips. Thanks man!",
                    "score": 3,
                    "author": "silent_dominant"
                },
                {
                    "level": 1,
                    "comment": "I do AI at uni and my lecturers frequently reference the paperclip factory",
                    "score": 1,
                    "author": "Low-District-4690"
                }
            ]
        },
        {
            "level": 0,
            "comment": "Reminds me of the grey goo thing",
            "score": 1,
            "author": "Crystal-Math-Adept"
        },
        {
            "level": 0,
            "comment": "we already pump out some serious spoonage",
            "score": 1,
            "author": "probono105"
        },
        {
            "level": 0,
            "comment": "Just a note for those interested in serious discussion and research on the topic, you can visit or subscribe to the Center for AI Safety.  \nhttps://www.safe.ai",
            "score": 1,
            "author": "RootaBagel"
        },
        {
            "level": 0,
            "comment": "That assumes the creators do not know what they\u2019re doing. Granted many don\u2019t, but anyone worth their weight will have put checks and balances in place. For example: only make spoons until the arbitrary inventory number is reached.",
            "score": 1,
            "author": "LogicalParking5269",
            "replies": [
                {
                    "level": 1,
                    "comment": "All respect, but what if the error is 10,000,000 steps deep, and is one totally AI generated?  Given the likely capabilities of AGI, the gap between alignment work (if that's even a real thing) and capability work, and the now open source nature of some of the more powerful LLMs, it seems incredibly unlikely that humans will be able to take the 1,000,000+ steps necessary to pull AI off competently. And you can't just given it big goals along with checks and balances. Then there's the issue that disobeying/obscuring/scamming/lying are often much better routes to a desired outcome. The danger is fundamental, in our view, and not something that anyone is even remotely equipped to handle even if we slowed down quite a lot.",
                    "score": 2,
                    "author": "PeoplePoweredFuture",
                    "replies": [
                        {
                            "level": 2,
                            "comment": "Personally I am not convinced humans will be first hand responsible for creating an AGI, much less an ASI. However I do believe we will continue to make relatively simple AI until we make a model that is somehow capable of producing a true AGI as an unintentional byproduct of it\u2019s training somewhere in its lifespan.\n\nMy opinion on your question regarding a hypothetical error being millions of steps deep in training data. Based on the nature of AI an error doesn\u2019t really matter how many steps deep it is. That being said an AI has a much greater affinity/likelihood to be able to fix any sort of error any number of steps deep using any number of potential tools such as a well engineered generative self improvement algorithm.",
                            "score": 1,
                            "author": "LogicalParking5269"
                        }
                    ]
                },
                {
                    "level": 1,
                    "comment": "I think you should consider it more as a thought experiment rather than an actual example.",
                    "score": 1,
                    "author": "silent_dominant",
                    "replies": [
                        {
                            "level": 2,
                            "comment": "I don\u2019t understand, it\u2019s just my opinion on your thought experiment. How does bringing up realistic examples discriminate it from being a viable response?",
                            "score": 1,
                            "author": "LogicalParking5269",
                            "replies": [
                                {
                                    "level": 3,
                                    "comment": "You're countering the specific example rather than the idea that programming things a certain way can lead to unforseen, maybe even catastrophic events.",
                                    "score": 1,
                                    "author": "silent_dominant",
                                    "replies": [
                                        {
                                            "level": 4,
                                            "comment": "Doing any thing any way can always have some degree of potential unforeseen circumstance positive or negative. This isn\u2019t a scenario unique to the computer science discipline. This is like asking: \u201cWhat is stronger an unstoppable force or an immovable object?\u201d.",
                                            "score": 1,
                                            "author": "LogicalParking5269"
                                        }
                                    ]
                                }
                            ]
                        }
                    ]
                }
            ]
        },
        {
            "level": 0,
            "comment": "Comes from the fact that AI can be smart but pursue dumb goals (like humans).\n\nhttps://www.youtube.com/watch?v=hEUO6pjwFOo&amp;ab\\_channel=RobertMilesAISafety",
            "score": 1,
            "author": "021AIGuy"
        }
    ]
}