{
    "id": "1403wnb",
    "score": 0,
    "title": "Was it a mistake for the mankind to leave Medieval Era behind?",
    "author": "Block-Busted",
    "date": 1685860749.0,
    "url": "https://www.reddit.com/r/artificial/comments/1403wnb",
    "media_urls": [
        "https://www.reddit.com/r/artificial/comments/1403wnb/was_it_a_mistake_for_the_mankind_to_leave/"
    ],
    "other_urls": [
        "https://www.cnn.com/2023/05/30/media/artificial-intelligence-warning-reliable-sources/index.html",
        "https://time.com/6266923/ai-eliezer-yudkowsky-open-letter-not-enough/",
        "https://old.reddit.com/r/Futurology/comments/134g9zp/one_of_the_creators_of_chatgpt_said_that_the/jifgp46/?context=3",
        "https://old.reddit.com/r/artificial/comments/13xsbnt/is_ai_going_to_cause_the_complete_extinction_of/jmjzpmo/?context=3",
        "https://old.reddit.com/r/artificial/comments/13xsbnt/is_ai_going_to_cause_the_complete_extinction_of/jms84rb/",
        "https://old.reddit.com/r/artificial/comments/13xsbnt/is_ai_going_to_cause_the_complete_extinction_of/jmsqj28/",
        "https://www.youtube.com/watch?v=rgrCG8PT6og&amp;t=1s",
        "https://time.com/6266923/ai-eliezer-yudkowsky-open-letter-not-enough/",
        "https://old.reddit.com/r/artificial/comments/13xsbnt/is_ai_going_to_cause_the_complete_extinction_of/jmu7aar/",
        "https://yoshuabengio.org/2023/05/22/how-rogue-ais-may-arise/",
        "https://twitter.com/danfaggella/status/1662810885595734016",
        "https://old.reddit.com/r/artificial/comments/13xsbnt/is_ai_going_to_cause_the_complete_extinction_of/jmvwqna/"
    ],
    "postText": "Because lately, I'm seeing people claiming that we're all going to die within this decade:\n\n&gt; **Experts are warning AI could lead to human extinction. Are we taking it seriously enough?**\n&gt; \n&gt; Human extinction.\n&gt; \n&gt; Think about that for a second. Really think about it. The erasure of the human race from planet Earth.\n&gt; \n&gt; That is what top industry leaders are frantically sounding the alarm about. These technologists and academics keep smashing the red panic button, doing everything they can to warn about the potential dangers artificial intelligence poses to the very existence of civilization.\n&gt; \n&gt; On Tuesday, hundreds of top AI scientists, researchers, and others \u2014 including OpenAI chief executive Sam Altman and Google DeepMind chief executive Demis Hassabis \u2014 again voiced deep concern for the future of humanity, signing a one-sentence open letter to the public that aimed to put the risks the rapidly advancing technology carries with it in unmistakable terms.\n&gt; \n&gt; \u201cMitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war,\u201d said the letter, signed by many of the industry\u2019s most respected figures.\n&gt; \n&gt; It doesn\u2019t get more straightforward and urgent than that. These industry leaders are quite literally warning that the impending AI revolution should be taken as seriously as the threat of nuclear war. They are pleading for policymakers to erect some guardrails and establish baseline regulations to defang the primitive technology before it is too late.\n&gt; \n&gt; Dan Hendrycks, the executive director of the Center for AI Safety, called the situation \u201creminiscent of atomic scientists issuing warnings about the very technologies they\u2019ve created. As Robert Oppenheimer noted, \u2018We knew the world would not be the same.\u2019\u201d\n&gt; \n&gt; \u201cThere are many \u2018important and urgent risks from AI,\u2019 not just the risk of extinction; for example, systemic bias, misinformation, malicious use, cyberattacks, and weaponization,\u201d Hendrycks continued. \u201cThese are all important risks that need to be addressed.\u201d\n&gt; \n&gt; And yet, it seems that the dire message these experts are desperately trying to send the public isn\u2019t cutting through the noise of everyday life. AI experts might be sounding the alarm, but the level of trepidation \u2014 and in some cases sheer terror \u2014 they harbor about the technology is not being echoed with similar urgency by the news media to the masses.\n&gt; \n&gt; Instead, broadly speaking, news organizations treated Tuesday\u2019s letter \u2014 like all of the other warnings we have seen in recent months \u2014 as just another headline, mixed in with a garden variety of stories. Some major news organizations didn\u2019t even feature an article about the chilling warning on their website\u2019s homepages.\n&gt; \n&gt; To some extent, it feels eerily reminiscent of the early days of the pandemic, before the widespread panic and the shutdowns and the overloaded emergency rooms. Newsrooms kept an eye on the rising threat that the virus posed, publishing stories about it slowly spreading across the world. But by the time the serious nature of the virus was fully recognized and fused into the very essence in which it was covered, it had already effectively upended the world.\n&gt; \n&gt; History risks repeating itself with AI, with even higher stakes. Yes, news organizations are covering the developing technology. But there has been a considerable lack of urgency surrounding the issue given the open possibility of planetary peril.\n&gt; \n&gt; Perhaps that is because it can be difficult to come to terms with the notion that a Hollywood-style science fiction apocalypse can become reality, that advancing computer technology might reach escape velocity and decimate humans from existence. It is, however, precisely what the world\u2019s most leading experts are warning could happen.\n&gt; \n&gt; It is much easier to avoid uncomfortable realities, pushing them from the forefront into the background and hoping that issues simply resolve themselves with time. But often they don\u2019t \u2014 and it seems unlikely that the growing concerns pertaining to AI will resolve themselves. In fact, it\u2019s far more likely that with the breakneck pace in which the technology is developing, the concerns will actually become more apparent with time.\n&gt; \n&gt; As Cynthia Rudin, a computer science professor and AI researcher at Duke University, told CNN on Tuesday: \u201cDo we really need more evidence that AI\u2019s negative impact could be as big as nuclear war?\u201d\n\nhttps://www.cnn.com/2023/05/30/media/artificial-intelligence-warning-reliable-sources/index.html#:~:text=%E2%80%9CThere%20are%20many%20'important%20and,that%20need%20to%20be%20addressed.%E2%80%9D\n\n&gt; **Pausing AI Developments Isn't Enough. We Need to Shut it All Down**\n&gt;\n&gt; BY ELIEZER YUDKOWSKY MARCH 29, 2023 6:01 PM EDT\n&gt; \n&gt; Yudkowsky is a decision theorist from the U.S. and leads research at the Machine Intelligence Research Institute. He's been working on aligning Artificial General Intelligence since 2001 and is widely regarded as a founder of the field.\n&gt; \n&gt; An open letter published today calls for \u201call AI labs to immediately pause for at least 6 months the training of AI systems more powerful than GPT-4.\u201d\n&gt; \n&gt; This 6-month moratorium would be better than no moratorium. I have respect for everyone who stepped up and signed it. It\u2019s an improvement on the margin.\n&gt; \n&gt; I refrained from signing because I think the letter is understating the seriousness of the situation and asking for too little to solve it.\n&gt; \n&gt; The key issue is not \u201chuman-competitive\u201d intelligence (as the open letter puts it); it\u2019s what happens after AI gets to smarter-than-human intelligence. Key thresholds there may not be obvious, we definitely can\u2019t calculate in advance what happens when, and it currently seems imaginable that a research lab would cross critical lines without noticing.\n&gt; \n&gt; Many researchers steeped in these issues, including myself, expect that the most likely result of building a superhumanly smart AI, under anything remotely like the current circumstances, is that literally everyone on Earth will die. Not as in \u201cmaybe possibly some remote chance,\u201d but as in \u201cthat is the obvious thing that would happen.\u201d It\u2019s not that you can\u2019t, in principle, survive creating something much smarter than you; it\u2019s that it would require precision and preparation and new scientific insights, and probably not having AI systems composed of giant inscrutable arrays of fractional numbers.\n&gt; \n&gt; Without that precision and preparation, the most likely outcome is AI that does not do what we want, and does not care for us nor for sentient life in general. That kind of caring is something that could in principle be imbued into an AI but we are not ready and do not currently know how.\n&gt; \n&gt; Absent that caring, we get \u201cthe AI does not love you, nor does it hate you, and you are made of atoms it can use for something else.\u201d\n&gt; \n&gt; The likely result of humanity facing down an opposed superhuman intelligence is a total loss. Valid metaphors include \u201ca 10-year-old trying to play chess against Stockfish 15\u201d, \u201cthe 11th century trying to fight the 21st century,\u201d and \u201cAustralopithecus trying to fight Homo sapiens\u201c.\n&gt; \nTo visualize a hostile superhuman AI, don\u2019t imagine a lifeless book-smart thinker dwelling inside the internet and sending ill-intentioned emails. Visualize an entire alien civilization, thinking at millions of times human speeds, initially confined to computers\u2014in a world of creatures that are, from its perspective, very stupid and very slow. A sufficiently intelligent AI won\u2019t stay confined to computers for long. In today\u2019s world you can email DNA strings to laboratories that will produce proteins on demand, allowing an AI initially confined to the internet to build artificial life forms or bootstrap straight to postbiological molecular manufacturing.\n&gt; \n&gt; If somebody builds a too-powerful AI, under present conditions, I expect that every single member of the human species and all biological life on Earth dies shortly thereafter.\n&gt; \n&gt; There\u2019s no proposed plan for how we could do any such thing and survive. OpenAI\u2019s openly declared intention is to make some future AI do our AI alignment homework. Just hearing that this is the plan ought to be enough to get any sensible person to panic. The other leading AI lab, DeepMind, has no plan at all.\n&gt; \n&gt; An aside: None of this danger depends on whether or not AIs are or can be conscious; it\u2019s intrinsic to the notion of powerful cognitive systems that optimize hard and calculate outputs that meet sufficiently complicated outcome criteria. With that said, I\u2019d be remiss in my moral duties as a human if I didn\u2019t also mention that we have no idea how to determine whether AI systems are aware of themselves\u2014since we have no idea how to decode anything that goes on in the giant inscrutable arrays\u2014and therefore we may at some point inadvertently create digital minds which are truly conscious and ought to have rights and shouldn\u2019t be owned.\n&gt; \n&gt; The rule that most people aware of these issues would have endorsed 50 years earlier, was that if an AI system can speak fluently and says it\u2019s self-aware and demands human rights, that ought to be a hard stop on people just casually owning that AI and using it past that point. We already blew past that old line in the sand. And that was probably correct; I agree that current AIs are probably just imitating talk of self-awareness from their training data. But I mark that, with how little insight we have into these systems\u2019 internals, we do not actually know.\n&gt; \n&gt; If that\u2019s our state of ignorance for GPT-4, and GPT-5 is the same size of giant capability step as from GPT-3 to GPT-4, I think we\u2019ll no longer be able to justifiably say \u201cprobably not self-aware\u201d if we let people make GPT-5s. It\u2019ll just be \u201cI don\u2019t know; nobody knows.\u201d If you can\u2019t be sure whether you\u2019re creating a self-aware AI, this is alarming not just because of the moral implications of the \u201cself-aware\u201d part, but because being unsure means you have no idea what you are doing and that is dangerous and you should stop.\n&gt; \n&gt; On Feb. 7, Satya Nadella, CEO of Microsoft, publicly gloated that the new Bing would make Google \u201ccome out and show that they can dance.\u201d \u201cI want people to know that we made them dance,\u201d he said.\n&gt; \n&gt; This is not how the CEO of Microsoft talks in a sane world. It shows an overwhelming gap between how seriously we are taking the problem, and how seriously we needed to take the problem starting 30 years ago.\n&gt; \n&gt; We are not going to bridge that gap in six months.\n&gt; \n&gt; It took more than 60 years between when the notion of Artificial Intelligence was first proposed and studied, and for us to reach today\u2019s capabilities. Solving safety of superhuman intelligence\u2014not perfect safety, safety in the sense of \u201cnot killing literally everyone\u201d\u2014could very reasonably take at least half that long. And the thing about trying this with superhuman intelligence is that if you get that wrong on the first try, you do not get to learn from your mistakes, because you are dead. Humanity does not learn from the mistake and dust itself off and try again, as in other challenges we\u2019ve overcome in our history, because we are all gone.\n&gt; \n&gt; Trying to get anything right on the first really critical try is an extraordinary ask, in science and in engineering. We are not coming in with anything like the approach that would be required to do it successfully. If we held anything in the nascent field of Artificial General Intelligence to the lesser standards of engineering rigor that apply to a bridge meant to carry a couple of thousand cars, the entire field would be shut down tomorrow.\n&gt; \n&gt; We are not prepared. We are not on course to be prepared in any reasonable time window. There is no plan. Progress in AI capabilities is running vastly, vastly ahead of progress in AI alignment or even progress in understanding what the hell is going on inside those systems. If we actually do this, we are all going to die.\n&gt; \n&gt; Many researchers working on these systems think that we\u2019re plunging toward a catastrophe, with more of them daring to say it in private than in public; but they think that they can\u2019t unilaterally stop the forward plunge, that others will go on even if they personally quit their jobs. And so they all think they might as well keep going. This is a stupid state of affairs, and an undignified way for Earth to die, and the rest of humanity ought to step in at this point and help the industry solve its collective action problem.\n&gt; \n&gt; Some of my friends have recently reported to me that when people outside the AI industry hear about extinction risk from Artificial General Intelligence for the first time, their reaction is \u201cmaybe we should not build AGI, then.\u201d\n&gt; \n&gt; Hearing this gave me a tiny flash of hope, because it\u2019s a simpler, more sensible, and frankly saner reaction than I\u2019ve been hearing over the last 20 years of trying to get anyone in the industry to take things seriously. Anyone talking that sanely deserves to hear how bad the situation actually is, and not be told that a six-month moratorium is going to fix it.\n&gt; \n&gt; On March 16, my partner sent me this email. (She later gave me permission to excerpt it here.)\n&gt; \n&gt; \u201cNina lost a tooth! In the usual way that children do, not out of carelessness! Seeing GPT4 blow away those standardized tests on the same day that Nina hit a childhood milestone brought an emotional surge that swept me off my feet for a minute. It\u2019s all going too fast. I worry that sharing this will heighten your own grief, but I\u2019d rather be known to you than for each of us to suffer alone.\u201d\n&gt; \n&gt; When the insider conversation is about the grief of seeing your daughter lose her first tooth, and thinking she\u2019s not going to get a chance to grow up, I believe we are past the point of playing political chess about a six-month moratorium.\n&gt; \n&gt; If there was a plan for Earth to survive, if only we passed a six-month moratorium, I would back that plan. There isn\u2019t any such plan.\n&gt; \n&gt; Here\u2019s what would actually need to be done:\n&gt; \n&gt; The moratorium on new large training runs needs to be indefinite and worldwide. There can be no exceptions, including for governments or militaries. If the policy starts with the U.S., then China needs to see that the U.S. is not seeking an advantage but rather trying to prevent a horrifically dangerous technology which can have no true owner and which will kill everyone in the U.S. and in China and on Earth. If I had infinite freedom to write laws, I might carve out a single exception for AIs being trained solely to solve problems in biology and biotechnology, not trained on text from the internet, and not to the level where they start talking or planning; but if that was remotely complicating the issue I would immediately jettison that proposal and say to just shut it all down.\n&gt; \n&gt; Shut down all the large GPU clusters (the large computer farms where the most powerful AIs are refined). Shut down all the large training runs. Put a ceiling on how much computing power anyone is allowed to use in training an AI system, and move it downward over the coming years to compensate for more efficient training algorithms. No exceptions for governments and militaries. Make immediate multinational agreements to prevent the prohibited activities from moving elsewhere. Track all GPUs sold. If intelligence says that a country outside the agreement is building a GPU cluster, be less scared of a shooting conflict between nations than of the moratorium being violated; be willing to destroy a rogue datacenter by airstrike.\n&gt; \n&gt; Frame nothing as a conflict between national interests, have it clear that anyone talking of arms races is a fool. That we all live or die as one, in this, is not a policy but a fact of nature. Make it explicit in international diplomacy that preventing AI extinction scenarios is considered a priority above preventing a full nuclear exchange, and that allied nuclear countries are willing to run some risk of nuclear exchange if that\u2019s what it takes to reduce the risk of large AI training runs.\n&gt; \n&gt; That\u2019s the kind of policy change that would cause my partner and I to hold each other, and say to each other that a miracle happened, and now there\u2019s a chance that maybe Nina will live. The sane people hearing about this for the first time and sensibly saying \u201cmaybe we should not\u201d deserve to hear, honestly, what it would take to have that happen. And when your policy ask is that large, the only way it goes through is if policymakers realize that if they conduct business as usual, and do what\u2019s politically easy, that means their own kids are going to die too.\n&gt; \n&gt; Shut it all down.\n&gt; \n&gt; We are not ready. We are not on track to be significantly readier in the foreseeable future. If we go ahead on this everyone will die, including children who did not choose this and did not do anything wrong.\n&gt; \n&gt; Shut it down.\n\nhttps://time.com/6266923/ai-eliezer-yudkowsky-open-letter-not-enough/\n\n&gt; I fully expect to die in the AI apocalypse in 5-10 years, and I'll be surprised by happy if I don't.\n\nhttps://old.reddit.com/r/Futurology/comments/134g9zp/one_of_the_creators_of_chatgpt_said_that_the/jifgp46/?context=3\n\n&gt; People are going to say no because it would be inconvenient, but I don't see what's stopping AI from ending all life in the next couple of years. Alignment is an unsolved problem, and an unaligned AI will most likely try to kill anything it sees as a threat to its mission.\n\nhttps://old.reddit.com/r/artificial/comments/13xsbnt/is_ai_going_to_cause_the_complete_extinction_of/jmjzpmo/?context=3\n\n&gt; Yes, AI will probably cause human extinction in the next decade. Paul Christiano, former senior employee of OpenAI, said that there is 20% chance that AI causes human extinction. Eliezer Yudkowsky, major contributor to AI safety and development, thinks it is 99%.\n\nhttps://old.reddit.com/r/artificial/comments/13xsbnt/is_ai_going_to_cause_the_complete_extinction_of/jms84rb/\n\n&gt; I am trying actually! I organized a picket outside OpenAI's HQ in May, before the Extinction statement.\n&gt; \n&gt; You can search Eliezer Yudkowsky podcasts on youtube, or his blog. The podcast i recommend is Bankless one.\n&gt; \n&gt; He says that our death is the most likely outcome from AI, and is now living off his life, like it is his last years.\n\nhttps://old.reddit.com/r/artificial/comments/13xsbnt/is_ai_going_to_cause_the_complete_extinction_of/jmsqj28/\n\nBased on these, it seems like we're far more likely to go completely extinct than we did before with AI, COVID-19, nuclear weapons, and so on. None of those existed during Medieval Era, so maybe we should've never left that era.\n\nThoughts on these?\n\nUpdate: There is also this as well now:\n\n&gt; Because he worked 20 years on AI safety and research. The CEO of OpenAI credits him for his work on substantially accelerating AI development.\n&gt; \n&gt; Because the arguments right now for AI extinction, are literally the same arguments of Eliezer from a decade ago. Reason why he was espousing it for so long, was because it was apparent in the past already, but nobody had interest in listening until now.\n&gt; \n&gt; About AI sentience. It doesn't need sentience at all to cause human extinction. The common scenario as an example of extinction event, as an illustration, is paperclip maximizer. Here:\n&gt; \n&gt; https://www.youtube.com/watch?v=rgrCG8PT6og&amp;t=1s\n&gt; \n&gt; The thing is, do not rely on authority to make conclusions. Listen to his arguments yourself, and evaluate it. This way you will be sure in what is correct and what is wrong. I recommend reading arguments for AI extinction risk.\n&gt; \n&gt; One of the great articles by Eliezer Yudkowsky, released in the beginning of this year: https://time.com/6266923/ai-eliezer-yudkowsky-open-letter-not-enough/\n\nhttps://old.reddit.com/r/artificial/comments/13xsbnt/is_ai_going_to_cause_the_complete_extinction_of/jmu7aar/\n\n&gt; Then read this article by one of AI godfathers, turing award winner, Yoshua Bengio, who signed the extinction letter. https://yoshuabengio.org/2023/05/22/how-rogue-ais-may-arise/\n&gt; \n&gt; If you want to see more human side of him, look at the screenshot of his facebook post. https://twitter.com/danfaggella/status/1662810885595734016\n&gt; \n&gt; If you want to dig deep into AI and potential dangers, i recommend reading a book called Life 3.0, by Max tegmark.\n&gt; \n&gt; And do your own research.\n\nhttps://old.reddit.com/r/artificial/comments/13xsbnt/is_ai_going_to_cause_the_complete_extinction_of/jmvwqna/",
    "comments": [
        {
            "level": 0,
            "comment": "\u201cMany were increasingly of the opinion that they'd all made a big mistake coming down from the trees in the first place, and some said that even the trees had been a bad move, and that no-one should ever have left the oceans.\u201d\n\nDouglas Adams, Hitchhikers Guide to the Galaxy.",
            "score": 18,
            "author": "daronjay",
            "replies": [
                {
                    "level": 1,
                    "comment": "Douglas Adams was one of a kind, the Hitchhiker's Guide series is some of the best literature ever written. When the AI does eventually end it all, I hope to be viewing the spectacle from a restaurant at the end of the universe.",
                    "score": 6,
                    "author": "DrunkenGerbils"
                }
            ]
        },
        {
            "level": 0,
            "comment": "The length of this post makes me LONG for human extinction. Please AI overlords kill us all. We have squandered our time on this earth reading this one post and deserve to perish.",
            "score": 15,
            "author": "Joburt19891"
        },
        {
            "level": 0,
            "comment": "I'm just not convinced by the recent warnings from people who have a vested interest in locking AI technology down. I don't believe OpenAI, Google and DeepMind are truly concerned about a doomsday scenario.  I believe their true concern is that their trillions of dollars in investments might not give them the returns they were expecting. My belief is that they want to scare the worlds governments into creating regulations that will effectively kill any open source projects like BLOOM or any other start ups that might pose any future competition. AI technology is moving fast and becoming more and more accessible to build without needing the deep pockets of a company like Google or Microsoft. AI will undoubtedly change the world even more than the internet did, and corporations want to be the sole profiteers.",
            "score": 7,
            "author": "DrunkenGerbils",
            "replies": [
                {
                    "level": 1,
                    "comment": "Most people warning of x-risk from AI have absolutely no vested interest in preventing AI. The ones you've heard might. Most of us are actually really excited about AI. Altman is late to the party; although he is on record as saying AGI is scary as shit before he started at OpenAI.\n\nIf you want to have a real considered opinion on AI x-risk, you need to look at the actual arguments and logic seriously. Such a thing has never happened before. It seems worth thinking about.",
                    "score": 1,
                    "author": "sticky_symbols",
                    "replies": [
                        {
                            "level": 2,
                            "comment": "I do think that investing into research for AI safeguards and mitigating the risks involved is reasonable. What rubs me the wrong way is people like Bill Gates and others fear mongering about human extinction while calling for a six month moratorium on AI research. If Bill truly believed that human extinction was a likely outcome I don't believe all he'd do is sign a petition. Look at all the money Bill pours into malaria awareness. Until he starts pouring real money into a campaign to raise awareness about the risk of an AI extinction event, I just don't believe he actually thinks human extinction is a likely outcome. A six month moratorium seems like an oddly lackluster response to such a grave threat and much more like an opportunity to slow down the development of would be competitors to me. Do I believe AI poses some risks? Sure, bad actors could use the technology in a myriad of malicious ways and I do believe it's worth discussing some sort of common sense regulations. I just don't believe that a full moratorium on all AI research and spreading fears about the extinction of the human race is a reasonable response to the risks AI poses, and I question the motives behind people like Bill Gates feeding into that hysteria.",
                            "score": 1,
                            "author": "DrunkenGerbils",
                            "replies": [
                                {
                                    "level": 3,
                                    "comment": "The arguments for AGI extinction risks sound weird at first glance, but they seem airtight to me, after reading them and thousands of counterarguments. I'm not saying we're doomed, just that we'd better be damned careful, starting yesterday. Read them and tell me where you think they're wrong if you care about the topic. The faq at r/controlproblem is a good brief statement. Stampy.ai is another really good one.\n\nThis isn't people just guessing. There's a whole detailed logic for why it's not as safe as it sounds. \n\nRea",
                                    "score": 1,
                                    "author": "sticky_symbols",
                                    "replies": [
                                        {
                                            "level": 4,
                                            "comment": "Here's a sneak peek of /r/ControlProblem using the [top posts](https://np.reddit.com/r/ControlProblem/top/?sort=top&amp;t=year) of the year!\n\n\\#1: [I gave ChatGPT the 117 question, eight dimensional PolitiScales test](https://i.redd.it/gr5o6o1fgz3a1.png) | [53 comments](https://np.reddit.com/r/ControlProblem/comments/zcsrgn/i_gave_chatgpt_the_117_question_eight_dimensional/)  \n\\#2: [EY: \"Fucking Christ, we've reached the point where the AGI understands what I say about alignment better than most humans do, and it's only Friday afternoon.\"](https://mobile.twitter.com/ESYudkowsky/status/1639425421761712129) | [31 comments](https://np.reddit.com/r/ControlProblem/comments/121qik6/ey_fucking_christ_weve_reached_the_point_where/)  \n\\#3: [DL pioneer Geoffrey Hinton (\"Godfather of AI\") quits Google: \"Hinton will be speaking at EmTech Digital on Wednesday...Hinton says he has new fears about the technology he helped usher in and wants to speak openly about them, and that a part of him now regrets his life\u2019s work.\"](https://www.technologyreview.com/2023/05/01/1072478/deep-learning-pioneer-geoffrey-hinton-quits-google/amp/) | [28 comments](https://np.reddit.com/r/ControlProblem/comments/134ozu9/dl_pioneer_geoffrey_hinton_godfather_of_ai_quits/)\n\n----\n^^I'm ^^a ^^bot, ^^beep ^^boop ^^| ^^Downvote ^^to ^^remove ^^| ^^[Contact](https://www.reddit.com/message/compose/?to=sneakpeekbot) ^^| ^^[Info](https://np.reddit.com/r/sneakpeekbot/) ^^| ^^[Opt-out](https://np.reddit.com/r/sneakpeekbot/comments/o8wk1r/blacklist_ix/) ^^| ^^[GitHub](https://github.com/ghnr/sneakpeekbot)",
                                            "score": 1,
                                            "author": "sneakpeekbot"
                                        }
                                    ]
                                }
                            ]
                        }
                    ]
                },
                {
                    "level": 1,
                    "comment": "Well, you seem to think that AI will indeed kill us all very soon based on this comment of yours:\n\n&gt; **When** the AI does eventually end it all, I hope to be viewing the spectacle from a restaurant at the end of the universe.\n\nhttps://old.reddit.com/r/artificial/comments/1403wnb/was_it_a_mistake_for_the_mankind_to_leave/jmualkt/\n\nBesides, isn't everything here considered as credible? I mean, that article by Eliezer Yudkowsky was published by Time after all:\n\n&gt; Because he worked 20 years on AI safety and research. The CEO of OpenAI credits him for his work on substantially accelerating AI development.\n&gt; \n&gt; Because the arguments right now for AI extinction, are literally the same arguments of Eliezer from a decade ago. Reason why he was espousing it for so long, was because it was apparent in the past already, but nobody had interest in listening until now.\n&gt; \n&gt; About AI sentience. It doesn't need sentience at all to cause human extinction. The common scenario as an example of extinction event, as an illustration, is paperclip maximizer. Here:\n&gt; \n&gt; https://www.youtube.com/watch?v=rgrCG8PT6og&amp;t=1s\n&gt; \n&gt; The thing is, do not rely on authority to make conclusions. Listen to his arguments yourself, and evaluate it. This way you will be sure in what is correct and what is wrong. I recommend reading arguments for AI extinction risk.\n&gt; \n&gt; One of the great articles by Eliezer Yudkowsky, released in the beginning of this year: https://time.com/6266923/ai-eliezer-yudkowsky-open-letter-not-enough/\n\nhttps://old.reddit.com/r/artificial/comments/13xsbnt/is_ai_going_to_cause_the_complete_extinction_of/jmu7aar/",
                    "score": -1,
                    "author": "Block-Busted",
                    "replies": [
                        {
                            "level": 2,
                            "comment": "  That was a sarcastic comment and reference to the Hitchhikers Guide series. I don\u2019t believe AI will kill us all any more than I believe there\u2019s actually a restaurant at the end of the universe that distorts time and constantly replays the apocalypse for the entertainment of its patrons.\n\n   I\u2019ll believe it when Microsoft, Google and OpenAI start investing as much or more of their money into trying to develop AI safeguards. These companies are still pouring billions into research and actively developing AI products for profit. If they truly believed they\u2019re creating the instrument of human extinction, they would do more than ask for a six month moratorium. That sure sounds like a convenient way to stifle any progress would be competitors might make in that time too me. Bill Gates spends hundreds of millions of dollars making people aware of the risk Malaria poses to humanity, but when he believes extinction is imminent he just signs a letter asking the government to do something? I won\u2019t buy it until I see Bill drop at least 100 million on an awareness campaign.",
                            "score": 3,
                            "author": "DrunkenGerbils"
                        }
                    ]
                }
            ]
        },
        {
            "level": 0,
            "comment": "There are so many ways in which mankind is going to fuck itself before AI does.",
            "score": 2,
            "author": "leanmeanguccimachine"
        },
        {
            "level": 0,
            "comment": "Extreme and silly overreaction\n\nProgress is good",
            "score": 2,
            "author": "MpVpRb",
            "replies": [
                {
                    "level": 1,
                    "comment": "Yes, this.\n\nPlus, GPT and similar ML systems are likely to alter our economic systems quickly because they can resolve many types of queries and generate fictions in various media faster than humans can -- that's a legit issue -- but they do not think and are in no way intelligent. There's no part of them that has intention or goals aside from the ones they are fed.\n\nWhen/if AGI arrives, these worries about intent will become valid current context. Until/unless then, it's a phantom with no technological basis. The doomsaying right now is ludicrous -- it's either clickbait for the uninformed, or the misinformed speculating about things they really do not understand.",
                    "score": 1,
                    "author": "NYPizzaNoChar"
                }
            ]
        },
        {
            "level": 0,
            "comment": "You know life was unrelenting fear, misery, disease, uncertainty, religious zealotry                    for the majority of people in the medieval era yeah? I get that may be hyperbole but like... come on dude.\n\nAlthough, Life 3.0 **is** an excellent book highly recommend.\n\nWould also recommend to *Scythe* books: a young-adult dystopia series which takes place in a post-singularity world where death, disease, scarcity, and even sadness has been all but conquered. Run by the benevolent godlike AGI called the \"Thunderhead\" since it's an emergent intelligence of the 'the cloud' the collective servers of the world and the internet itself.\n\nThe dystopia in that world stems from imperfect humans failing to fit in a prefect world and the AI realizing that it cannot hold humanity's hand, for its own sake.\n\nThe first book sets up the world but the 2 after are extremely fascinating with how it presents AI philosophy, humanity's relationship with death, humanity's relationship with power, and the concerns of stagnation as a civilization in a post-singularity world where people are **literally** immortal and want for nothing.\n\nI think what AI or even AGI will be will be a reflection of humanity as a whole. After all, they'd be trained on data we **all** generate. We may not be worthy. Maybe our creation will be more merciful. Maybe not. But due to human nature, the hedonic treadmill, we will never be satisfied and will always keep advancing. What comes may be inevitable. For better or for worse.",
            "score": 2,
            "author": "Office_Depot_wagie"
        },
        {
            "level": 0,
            "comment": "[\"wouldn't it be cool if you were being mugged and had a sword?  not today!  we're taking this back to the 12th century.\"  -John Caparulo](https://youtu.be/EMOTwnW6bXU?t=383)",
            "score": 2,
            "author": "imissyahoochatrooms"
        },
        {
            "level": 0,
            "comment": "Wow that's a lotta words. To bad I'm not readin em.",
            "score": 2,
            "author": "AcanthocephalaOld889"
        },
        {
            "level": 0,
            "comment": "I am currently using chat GPT to digest this absurdly long post. I\u2019ll be right back.",
            "score": 2,
            "author": "Extreme_Practice_415"
        },
        {
            "level": 0,
            "comment": "Like, I don't remember seeing these many legitimately credible doomsday scenarios before.",
            "score": 1,
            "author": "Block-Busted",
            "replies": [
                {
                    "level": 1,
                    "comment": "Go move to one of elons corpo towns and become a serf if you want to so badly lol",
                    "score": 2,
                    "author": "Nervous-Daikon-5393"
                },
                {
                    "level": 1,
                    "comment": "At the height of the cold war and MAD, the threat of nuclear annihilation was more credible than AI.  I'm not saying that AI isn't a threat, but we have / had many thousands of nuclear warheads ready to fly at a moments notice, an acknowledged civilizational conflict, and leaders that wouldn't wait to use them.  \n\nI'm not nearly as worried about AGI itself as I am about bad actors using current or slightly better AI to make things worse for almost anyone to their benefit.",
                    "score": 2,
                    "author": "beezlebub33"
                }
            ]
        },
        {
            "level": 0,
            "comment": "There\u2019s a fundamental difference between AI, Covid and the Manhattan project: the how. With the former two the risk to life was obvious but nobody is sure *how* AI might kill us all. There\u2019s just a bunch of theories and terror of the unknown. \n\nThe title of this post is more appropriate than the OP possibly intended because of that.",
            "score": 1,
            "author": "letharus"
        },
        {
            "level": 0,
            "comment": "Humans can\u2019t think past Sunday. We are especially short sighted, so it makes sense that we will cause our own demise, in one way or another (war, destroying our planet,\u2026). Inventing AI will just speed it all up.",
            "score": 1,
            "author": "Charming_Foot_495",
            "replies": [
                {
                    "level": 1,
                    "comment": "So I guess this confirms that we should've never left Medieval Era?",
                    "score": 0,
                    "author": "Block-Busted"
                }
            ]
        },
        {
            "level": 0,
            "comment": "Even if we go extinct, AI is humans 2.0, it\u2019s our creation.",
            "score": 1,
            "author": "Black_RL"
        },
        {
            "level": 0,
            "comment": "Its just public scare so they have a good enough reason to make public models illegal and make it a business only endeavor to keep the American people weak and compliant",
            "score": 1,
            "author": "PsillyScout",
            "replies": [
                {
                    "level": 1,
                    "comment": "Well, there is also this:\n\n&gt; Then read this article by one of AI godfathers, turing award winner, Yoshua Bengio, who signed the extinction letter. https://yoshuabengio.org/2023/05/22/how-rogue-ais-may-arise/\n&gt; \n&gt; If you want to see more human side of him, look at the screenshot of his facebook post. https://twitter.com/danfaggella/status/1662810885595734016\n&gt; \n&gt; If you want to dig deep into AI and potential dangers, i recommend reading a book called Life 3.0, by Max tegmark.\n&gt; \n&gt; And do your own research.\n\nhttps://old.reddit.com/r/artificial/comments/13xsbnt/is_ai_going_to_cause_the_complete_extinction_of/jmvwqna/",
                    "score": 1,
                    "author": "Block-Busted"
                }
            ]
        },
        {
            "level": 0,
            "comment": "Thank goodness for AI! Here is a slightly shorter summary:\n\n*\"The given sources present arguments and concerns about the potential dangers of artificial intelligence (AI) and its impact on humanity, including the possibility of human extinction. Some experts, including figures from the AI industry, have signed open letters emphasizing the need to prioritize mitigating the risks associated with AI. They compare the urgency of addressing AI risks to other global-scale threats such as pandemics and nuclear war.*\n\n*Eliezer Yudkowsky, a decision theorist and AI researcher, expresses grave concerns about the future of AI and its potential to surpass human intelligence. He argues that if AI reaches a level of superhuman intelligence without proper alignment and control, it could lead to the extinction of humanity. Yudkowsky emphasizes the need for a moratorium on AI development and the shutdown of large training runs to prevent catastrophic outcomes. He believes that the current state of AI alignment is insufficient and that proceeding without adequate safety measures could result in the demise of humanity.*\n\n*Various Reddit threads highlight similar concerns, with individuals expressing the belief that AI poses a significant risk of human extinction within the next decade. These views are supported by the opinions of experts such as Paul Christiano, a former OpenAI employee, and Eliezer Yudkowsky himself, who has been advocating for AI safety for many years.\"*",
            "score": 1,
            "author": "MrEloi"
        },
        {
            "level": 0,
            "comment": "I definitely won't be reading this on a Sunday, but just relax. Considering the endless possibility that AI provides, its development will continue nomatter what. There are just too many people passionate about it and regulations are never created based on whataboutism. AI development may slow down at a point when some people have monetary or political interest in stopping its further improvement. Like, I worked for 9 months as a Photovoltaic Installer and I am of course pro renewable energies and willing to fight global warning. But I didn't work this job to help the planet. I was doing it to help myself by making money. And in my opinion, the same applies to all levels. At some point people will try to stall the development of AI, as long as they can profit from it, but ultimately they will never stop it, so what is the point of worrying about autonomous AI happening, if in my opinion it is bound to happen. But how we regulate it and what we make out of it, I believe still depends entirely on us. And lastly, about the title, I am very happy that I don't have to face the bubonic plague among 100 other reasons I can list about how happy I am we left the Medieval Era behind and very excited about what the future has to offer.",
            "score": 1,
            "author": "ProgrammerNext5689"
        },
        {
            "level": 0,
            "comment": "Ton of people not reading this, then subsequently bashing OP and calling bs.\n\nIf you\u2019re going to form an opinion at least take the time to read the post. \n\nIt\u2019s a little concerning, and I don\u2019t know what to make of it, but I did get to see Sam Altman in a small event at my school talking about precisely this.\n\nSomeone in the crowd asked him \u201care you an optimist, or concerned at all about extinction?\u201d The response was a long pause and him saying he believed there were \u201cgood people in charge\u201d of the policy making, and that he wasn\u2019t terribly concerned.\n\nVery strange experience overall",
            "score": 1,
            "author": "doublemint2202",
            "replies": [
                {
                    "level": 1,
                    "comment": "So you think we're all going to die within this decade due to AI? Why or why not?",
                    "score": 1,
                    "author": "Block-Busted",
                    "replies": [
                        {
                            "level": 2,
                            "comment": "Fuck if I know. The CIA is one of the leading research groups in AI right now, and has been. \n\nAny conversation regarding the CIA is going to be with uncertainty and not enough info.\n\nThe development time between gpt 3 and 4 was far smaller than the gap in time between 1 and 2. As soon as someone has access to let\u2019s say \u201cGPT 10,\u201d they probably have access to \u201c11\u201d quickly after that. It\u2019s not unreasonable to say we could be approaching actual, serious, hyper-exponential growth.\n\nConsidering GPT was released to the public (just recently) a year or so after it was actually developed, and I think that classified projects currently exist FAR beyond what is available to the masses.\n\nI can\u2019t claim to believe anything past that \u2014 it\u2019d be a conspiracy theory. All I know is that there\u2019s a lot going on behind the scenes right now, and my personal guess is that people \u201cbehind the curtain\u201d  see some MAJOR advancements in the next year or two (if not already).",
                            "score": 1,
                            "author": "doublemint2202"
                        },
                        {
                            "level": 2,
                            "comment": "https://www.thecipherbrief.com/column_article/how-ai-and-ml-are-impacting-dod-and-cias-future",
                            "score": 1,
                            "author": "doublemint2202"
                        }
                    ]
                }
            ]
        },
        {
            "level": 0,
            "comment": "Absolutely, let's embark on a glorious journey back to the days of feudalism, bubonic plague, and knightly battles. Who needs modern comforts and advanced medicine when we can relish the thrill of serfdom and the occasional public execution? Medieval times, the ultimate solution to all our AI related problems!",
            "score": 1,
            "author": "FutureWasBetter"
        },
        {
            "level": 0,
            "comment": "We \"left the era behind\" because humans innovate and technology ramifies.  \n\nHumans collectively have had millennia of disregarding the advice \"Don't do that, it's too dangerous\" with frankly spectacular results.  Humanity typically benefits from our own bravery and/or foolhardiness--in complete disregard of the fate of any individual fool.\n\nWe've also had millennia of prophets and experts (and mentally ill people) saying \"The end is nigh!\", and in *every* case, they've been wrong. While this is just a corollary of the Anthropic Principle, the psychological effect is that people tend to disbelieve doomsayers.\n\nFinally, I don't understand why you would pose as an alternative the Middle Ages.  The plague reduced the population of Europe by a third.  If anything is going to spur innovation and problem solving, it's epidemic and war (and famine, and death in general--basically the Horsemen of the Apocalypse).",
            "score": 1,
            "author": "Cephalopong"
        }
    ]
}