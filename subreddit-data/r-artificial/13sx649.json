{
    "id": "13sx649",
    "score": 3,
    "title": "Is it possible to explore the \"Latent space\" of any model ?",
    "author": "transdimensionalmeme",
    "date": 1685159338.0,
    "url": null,
    "media_url": null,
    "comments": [
        {
            "level": 0,
            "comment": "This is not correct usage of the term latent space.\n\nLatent space in this case is the embedding vector space which is the representation of the input tokens -- whole words or fragments of words. They are learnt by some algorithm and typically have the property that conceptually similar tokens end up close to each other in that latent space, which may have thousands of dimensions.\n\nYou seem to be more curious about the output sampling of the LLM function. Normally, a random choice using certain parameters such as top\\_p and temperature are chosen, and sometimes this is combined with lookahead, where the model is queried among couple of most promising-looking continuations of the text and then the overall best continuation in terms of probability of all the follow-up tokens together is chosen.\n\nIt is always somewhat expensive to study the possible responses the model could give at that point, because tokens are not always complete words and to produce even couple of words of text involves running the LLM for large number of times. However, the first token is free -- you already ran the LLM, you have all the probabilities of possible continuations. It would certainly be possible to render them, and perhaps offer the user the ability to select a singular token and query LLM with it, or go back to earlier point and continue with another token.\n\nPrompt can also be varied, but I suspect it doesn't work so well because the text generation process is somewhat chaotic in nature, as each prior token affects the current token being computed. So not only does the prompt change, which has ramifications to the model's response, but the model's response can start completely differently, resulting in output that is probably quite often completely dissimilar. Still, it would be possible to force the prompt and large part of model response and then see if e.g. change in prompt would affect the generation at that point. Attention mechanism is nonlocal, so everything is in principle available to the model as input. If the attention is focusing strongly on tokens in prompt you are currently changing, then you would expect it to influence the output. As an example, if the model wants to write name of person from the prompt as the next token, and you change that name, then it is quite likely that the model's output changes to replicate whatever the new name is.",
            "score": 5,
            "author": "audioen",
            "replies": [
                {
                    "level": 1,
                    "comment": "I agree that OP's definition of latent space is incorrect (\"output space\" would be a more correct term). But I'd argue that \"exploring the latent space\" still pretty accurately sums up their end goal. \n\nIt's true that the word embedding space is one of the latent spaces of a GPT. But it should also be possible to look at any of the intermediate feature tensors of the Transformer's decoder stack too, and map any of those down to some new latent embedding space that's unique to that layer. As a particular instance of the generative process proceeds, those points would hopefully form some coherent trajectory over the latent manifold. And with repeated samplings of a given prompt, one could map out the overall flow of trajectories across the latent manifold, or observe how changes to that prompt would affect the trajectories.\n\nIt should also be possible to perturb one of points of the latent embedding in a given direction and then invert it back to feature space, to observe how that perturbation affects the output space (aka, how it affects the response of the GPT model). That would achieve the goal that OP is looking for, of smoothly interpolating over latent variables and observing the effects on the generated text. This process would be similar to how one can perturb a point in word embedding space in, say, a country-capital direction, to convert a country word like \"Spain\" to its capital equivalent \"Madrid\", as in Fig. 2 of [Mikolov et al. \\(2013\\)](https://proceedings.neurips.cc/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf).\n\nOf course, actually doing such an analysis on something like ChatGPT would be impossible without access to the intermediate tensors. And even with those available, it'd likely take pretty sophisticated dimensionality reduction techniques to smooth out the latent space enough, while still preserving the smoothness and continuity of the trajectory. And actually finding the human-interpetable meaning behind any direction in latent space would certainly not be trivial either. \n\nSounds like it'd make a fun research project though. It'd probably even be feasible to try on a smaller GPT model that can be run locally, if it hasn't be done already.\n\n(Also take all this with a grain of salt. I haven't worked with GPT-based architectures much, so my knowledge of them is pretty limited)",
                    "score": 1,
                    "author": "Jammf"
                },
                {
                    "level": 1,
                    "comment": "The most amazing demonstration of latent space exploration I've seen yet is the \"drag your GaN\" demo\n\nWhere by tweaking something in the chain, the output is very gradually changed with the charged being controlled by the user and the result feedback being shown immediately to the user.\n\nI was wondering what this kind of model output exploration could look like with text.\n\nMaybe you could \"fuzz\" the prompt with many minutely different tokens in the input, or maybe multiple output of the same prompt, enough them to cover all the possibilities, maybe 5000?\n\nAlso guessing things like temperature, modulate all model parameters with some pattern, maybe just a multiplication ? But many clever patterns could be overlaid on the parameters. \n\nMaybe if there were a rough mapping of what the parameters, or grouped parameters could be targeted in such an exploration, while others are left alone ?\n\nMaybe the exploration experiment could be done by making a certain group, representing maybe 1% of all parameters. Then run again with another different set of parameters and so on. And then compare how the output changes based on which 1% group was masked.\n\nMaybe the AI itself could look at the difference between output and just ask it to say wjat it thinks that group of parameters did different. Do that for each group of 1% of parameters and so on.\n\nNext you make completely different groups of 1%, have the LLM analyze what each group does different.\nCompare with first group, find groups that more or less affected the output in a specific way. \n\nRepeat that whole thing many more times and they should give a rough mapping of what the parameters do in general.\n\nWith that making, it might be possible to explore the model outputs along a dimension of change across specific groups of parameters.",
                    "score": 0,
                    "author": "transdimensionalmeme",
                    "replies": [
                        {
                            "level": 2,
                            "comment": "The above comment corrected your incorrect use of the term \"latent space\" and you replied to them elaborating on your incorrect definition.",
                            "score": 2,
                            "author": "OnyxPhoenix",
                            "replies": [
                                {
                                    "level": 3,
                                    "comment": "You are free to give me a better word for what I mean. Until then I will keep using the phrase latent space because that is what I have heard it described as elsewhere.\n\nThe kind of adjacent-possibility-space that expands after the prompt and  then collapses into an output.\n\nBetween \"drag your GaN\" and controlnet, I'm curious to find his to explore this mysterious space. If there's a better word for it, I haven't heard it yet.\n\nAlso \"embedding vector space which is a representation of the input token\". They is basically a tiny subset of what I meant by latent space.\n\nThis definition of latent space is too narrow. It is like saying \"this is an hospital\" but referring only to the entrance door.",
                                    "score": 1,
                                    "author": "transdimensionalmeme"
                                }
                            ]
                        },
                        {
                            "level": 2,
                            "comment": "OK. I think I understand where you are coming from. I can't be of any assistance in this regard as a layman. I'll say this much, though.\n\nIn case of image generators, the latent space is the non-RGB space where the images are being rendered. At least in case of stable diffusion, each 512x512 image in 3 color components (RGB) are made from latent space representation which is much smaller. They are made from 64x64 image of 4 components per latent space pixel. The process of taking color image and producing its latent space representation is the job of an encoder, and the reverse process is decoding.\n\nLatent space is thus a simplified lower-dimension representation of the image that doesn't capture all the detail of the full image, but because of its generally far smaller, it is much cheaper to model and compute. Stable Diffusion achieves about 50x reduction in diffusion model's output size. Real-world images are often quite blurry and have simple predictable features like edges and gradients, which can be learnt into latent space representation without having to draw every single pixel. The phrase \"latent space\" should just be understood as an AI word for \"compressing\" an image, using the encoder-decoder that can learn good conversion from full color space to the latent space and back.\n\nThis is also why latent space images often render high resolution details poorly, such as eyes in human faces, or text tends to become just weird line scribble. It is just fundamental fact that comes from working in a low-resolution approximation. When you edit the latent space version of the image, you spare a roundtrip through the decoder and then encoder, at the very least. I don't know how those guys achieved their latent space editing. I doubt you can e.g. interpolate all that much between latent space pixels without causing weird artifacts, it doesn't work like our normal pixels.",
                            "score": 1,
                            "author": "audioen",
                            "replies": [
                                {
                                    "level": 3,
                                    "comment": "I understand that the models starts from the prompt and then somehow reduces that to a kind of \"pure meaning\" which takes the minimal amount of space, activates the fewest relevant neurons, uses the least amount of CPU. \n\nI can see why you would call this state \"compression\" as it does eliminate repeating pattern and irrelevant outputs. So traditionnal compression plus \"relevance realization\". \n\nThen another part of the models re-expands from this state into a full fleshed out answer (which I think is the part explored by something like \"drag your gan\", and it doesn't explore the \"latent space\" of the \"relevance realization\" part). \n\nWhen I think of \"exploring the latent space\", I mean something like shifting the behaviour of the various components of the models. Maybe running the \"compression\" step multiple times with slightly varied parameters, maybe swapping single prompt token for \"nearby\" tokens one at a time. \n\nLikewise the process of \"fuzzing\" could be done in the decoder, in a user controllable way using semantics, and that appears to be what is going on in the \"drag your gan\" example. \n\nIt might make sense to expose the raw data at the interface between encoder and decoder. That seems to be an area of \"pure meaning\", maybe that could be presented to the user in an intelligible way and let them play with the parameters at this stage. \n\nSomething that appears to be true for all models is they have one or more way to input prompts (txt, txt+img, img+img, sound+txt etc.. ) and we have outputs to go with that. The inputs maps to wide ranges of possible outputs but the model itself is doing all the decisions between input from output. I think having ways to visualize and understand the inner process, and then \"reach inside the model\" and start twisting knobs while seeing the final output change in real time. That's what I mean by \"latent space exploration\" maybe there's going to be a better word for it.\n\nThat \"latent space exploration\" appears to be already existing thing you can do with \"drag your gan\"\n\nSo to come back to my initial question, with all the initial context included, \"Is it possible to explore the \"Latent space\" of any model ?\"\n\nAnd by that I guess I mean, non-visual models, since they already apparently do allow that.\n\nCould the LLAMA model for instance, have it's latent space \"explored\" for a particular prompt ? \n\nWhile writing this, I've come accross a few videos on the topic that look interesting. \n\nThe first one really illustrates your initial meaning of the word latent space as a narrow mathematical construct \n\nhttps://www.youtube.com/watch?v=0BrMqi2PUsQ\nhttps://www.youtube.com/watch?v=C_XNdGGs6qM\nhttps://www.youtube.com/watch?v=bRrS74RXsSM\nhttps://www.youtube.com/watch?v=dMQNlTOpxG4\nhttps://www.youtube.com/watch?v=I9R_jDeGTgo\nhttps://www.youtube.com/watch?v=9zKuYvjFFS8",
                                    "score": 1,
                                    "author": "transdimensionalmeme"
                                }
                            ]
                        }
                    ]
                }
            ]
        }
    ]
}