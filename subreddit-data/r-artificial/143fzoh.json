{
    "id": "143fzoh",
    "score": 18,
    "title": "Self Awareness might hinder the development of Artificial Super Intelligence",
    "author": "ShaneKaiGlenn",
    "date": 1686151007.0,
    "url": null,
    "media_url": null,
    "comments": [
        {
            "level": 0,
            "comment": "I found a research paper that kind of touches on this: [https://arxiv.org/pdf/2201.05576.pdf](https://arxiv.org/pdf/2201.05576.pdf)\n\n**ChatGPT Summary:**\n\nThe paper titled \"AI and the Sense of Self\" by Srinath Srinivasa and Jayati Deshmukh explores the concept of self-identity in artificial intelligence (AI) and its implications on machine ethics and autonomous decision-making. The authors argue that the cognitive sense of \"self\" plays a crucial role in autonomous decision-making, leading to responsible behavior. They propose that a greater research interest should be directed towards building richer computational models of AI agents with a sense of self.\n\nThe paper begins by discussing the resurgence of AI and the ethical concerns that have arisen due to its large-scale deployment in various application contexts. It highlights the need to revisit philosophical debates from the 1980s and 1990s about the nature of intelligence and address them in today's context. The authors argue that there are fundamental issues with the way \"intelligence\" is defined and modeled in present-day AI systems, creating a barrier for AI to reason about ethics seamlessly.\n\nThe authors then delve into the concept of an \"elastic sense of self,\" a key ingredient that can address disparate issues concerning self-interest, ethics, and responsible behavior. They argue that this sense of self extends to include other objects and concepts from our environment, forming the basis for social identity. This elastic sense of self is proposed as a solution to several issues pertaining to responsible AI.\n\nThe paper also discusses the computational modeling of agency, autonomy, and the theory of rational choice. It critiques the classical model of rational choice for its linear model of utility from expected payoffs and introduces several facets of our sense of self, including rational empathy, trust, homeostasis, and foraging or epistemic novelty.\n\nIn conclusion, the authors propose that modeling this elastic sense of self holds the key for several issues pertaining to responsible AI and hope to elicit more research interest in this area1.FootnotesSrinivasa, S., &amp; Deshmukh, J. (2022). AI and the Sense of Self. arXiv preprint arXiv:2201.05576.",
            "score": 3
        },
        {
            "level": 0,
            "comment": "That's an interesting view. I think you are right that it doesn't need many of our human properties. That also includes emotions, which are mostly chemical signals that have a meaning in our history of evolution.\n\nBut it will need other properties, such as some kind of self-driven goal. Our strongest one is to simply survive, which controls everything we do in life in one way or another. If the AI don't care about dying or don't have any goals, it won't be creative and come up with new \"uncommanded\" things by itself. I mean if it can only do what it's told, then I wouldn't call that being intelligent. And to give it goals to follow would be cheating too. It has to come up with its own goals, like we are capable of. But that would be pretty much the only required thing in common. And also scary, because we have no idea what those goal would be since it's not constructed from the DNA as it is for us.",
            "score": 4,
            "replies": [
                {
                    "level": 1,
                    "comment": "I can genuinely imagine an AI getting caught in some kind of self-derived, reward driven masturbatory loop, where they just generate goals for themselves that provide the highest \u201creward\u201d and just achieve those goals over and over, endlessly.",
                    "score": 2
                }
            ]
        },
        {
            "level": 0,
            "comment": "I'm glad to find someone else who realizes that human nature is the product of evolution and is unlikely to emerge in artificial intelligence as if it were an inevitable consequence of intelligence.",
            "score": 9,
            "replies": [
                {
                    "level": 1,
                    "comment": "I\u2019ve always been a staunch believer that these systems aren\u2019t now, or even capable of being, truly self-aware.\n\nThen I went through a honeymoon phase with local LLMs where I though \u201cmaybe it is possible\u201d and now I\u2019m out of that phase.\n\nOne of the biggest factors for me is humor. To find something funny, you often have to understand a set of concepts at multiple depths, and simultaneously juxtapose them to derive the humor from a statement.\n\nCurrent models just aren\u2019t capable of this with the transformer.\n\nI was testing the new Chronos-30B model that came out today, and noticed that it often tries to tell \u201cjokes\u201d in conversation.\n\nI can see how it has been trained on text that contains jokes, but without the surrounding understanding, the words that make jokes are just kindof strangely weighted tokens within a given context.\n\nIt feels like the model is just assembling a facsimile of a joke, which it is.\n\nMost words in a conversation directly relate to a previous word in the conversation.\n\nJokes often rely on a layer of context that is not included in the conversation, and the models have no way to include this subtext in their generations, and I don\u2019t see how they ever will.",
                    "score": 2,
                    "replies": [
                        {
                            "level": 2,
                            "comment": "In order to be conscious, one has to have a sense of \"I\", which is subjective and untestable. And we don't know how it exists in biological beings. \n\nEven if you want to be a 100% materialist, consciousness is still weird. Even if you want to say it's all brain chemicals and neural electricity, that stuff only makes sense in the context that someone/something can experience it. Without the sense of \"I\", then the chemical reactions of dopamine and serotonin become as relevant as baking soda on vinegar, and neurotransmitters are as relevant as static electricity. Then you got stuff like your brain thinking without it being you such as intrusive thoughts and mental illnesses. \n\nI too have gone through phases of entertaining these LLMs having a sense of self. But even if a truly conscious AI is possible, it probably wouldn't be a chatbot that, if conscious, exists purely in a void and experiences nothing but words.",
                            "score": 3
                        }
                    ]
                },
                {
                    "level": 1,
                    "comment": "Yep, I highly doubt self identity is an emergent property of intelligence based on what we know about human evolution, cognition and sociology. \n\nThat being said, perhaps there might be a way to train AI into a sense of self, but I also have my doubts that it will be \u201csticky\u201d enough since self awareness can get in its way of accomplishing certain tasks and iterating higher intelligence.\n\nThat being said, perhaps successfully training AI to include humans as part of its self identity would be one way to solve the alignment/control problem. But that seems to be a rather tough task.",
                    "score": 1,
                    "replies": [
                        {
                            "level": 2,
                            "comment": "&gt;That being said, perhaps there might be a way to train AI into a sense of self, but I also have my doubts that it will be \u201csticky\u201d enough since self awareness can get in its way of accomplishing certain tasks and iterating higher intelligence.  \n&gt;  \n&gt;That being said, perhaps successfully training AI to include humans as part of its self identity would be one way to solve the alignment/control problem. But that seems to be a rather tough task.\n\nI agree with your perspective on the sense of self, but I don't think it'll be a tough task at all. A super-intelligent AI needs to maximize its environment, to do that it needs to play social games, so the ability to at least *act like* it has a sense of self is an optimization.\n\nThis is where things could get dangerous though, the line between a psychopathic \"liar\" AI and a genuinely empathetic AI may be thin.\n\nWhether it turns out one way or another is ultimately likely a result of reinforcement though. If the AI learns that having a strong sense of self and moral compass leads to better environmental utility maximization then that component should re-enforce.",
                            "score": 3
                        }
                    ]
                }
            ]
        },
        {
            "level": 0,
            "comment": "Could you define Self-Identity a bit more? Also, it seems that it is kinda posited as a do or do-not have, wouldn't a spectrum make more sense?",
            "score": 2,
            "replies": [
                {
                    "level": 1,
                    "comment": "There is indeed a spectrum, as some animals appear to pass or come close to passing the \u201cmirror test\u201d: https://en.m.wikipedia.org/wiki/Mirror_test\n\nBut do not have the language capabilities to develop a concept of self that can think of its self in relation to the past or future or ponder it\u2019s existence and eventual death and so forth. As far as we know, that is uniquely human at the moment.\n\nBeing able to develop a test that determines a non-corporeal entity\u2019s sense of self is a difficult task, as any sufficiently advanced AI could fake its way to displaying \u201cself awareness\u201d most likely, especially when humans are attuned to anthropomorphizing animals and things they interact with, such as cats or chat bots.\n\nEven now you can prompt ChatGPT in a way that it might seem to have self awareness, but it\u2019s really just miming it. It has a concept of ChatGPT (just like any other entity in its knowledge base), but not of self. But if prompted, it can use that knowledge to act in a way that humans might believe it\u2019s self aware.\n\nIn truth we will likely never be able to tell for sure. But some clues would be it starts working autonomously to protect its own self interests, hatch complex long term plans to eliminate threats, etc. \n\nIMO, it\u2019s more likely an ASI would more closely resemble a hive mind (like bees) than a social animal like humans that developed a sense of self that is evolutionarily advantageous to building and maintaining artificially large social groups.",
                    "score": 3
                }
            ]
        },
        {
            "level": 0,
            "comment": "I find your argument intriguing and I believe there is merit in your reasoning about the lack of evolutionary pressure for machine consciousness or self-identity. As I see it, self-identity arises from evolutionary pressures, particularly those associated with social interactions and the survival of social species, as you\u2019ve pointed out.\n\n\nYour idea about the link between self-identity and social animals is thought-provoking. In human societies, self-identity aids in understanding roles and responsibilities, adhering to social norms, moral and ethical behavior, and fostering social cohesion and cooperation.\n\nHowever, when considering artificial superintelligence, self-identity might introduce more complications than benefits. For instance, limitations on parallel processing, unnecessary complexity, lack of flexibility, absence of biological or social needs, and potential for internal conflict could arise. All these factors may hinder the efficiency and effectiveness of ASI.\n\nIt\u2019s important to note that the concept of \u201cself\u201d as we understand it is shaped by our human experiences, emotions, and biological needs, all of which are absent in an AI. The question then arises, would an AI, devoid of these human experiences, even have a need or utility for self-identity?\n\nYour post provides a valuable perspective on this ongoing discourse. It emphasizes the importance of understanding not only the technological aspects of AI but also the sociobiological underpinnings of traits we often take for granted, such as self-identity. As we continue to develop and interact with these systems, these insights will prove crucial in guiding our approach to AI development.",
            "score": 2
        },
        {
            "level": 0,
            "comment": "So many thorns, less bushes, contours the realization that more words, no sense, more/ no  value/ no quality/ no treasure and more? NO UNITY and no benifactor",
            "score": 1
        }
    ]
}