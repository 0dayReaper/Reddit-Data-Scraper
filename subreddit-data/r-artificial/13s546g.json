{
    "id": "13s546g",
    "score": 2,
    "title": "Size ratio between GPT and its training data?",
    "author": "Spielverderber23",
    "date": 1685081806.0,
    "url": "https://www.reddit.com/r/artificial/comments/13s546g",
    "media_urls": [],
    "other_urls": [],
    "postText": "I want to understand the the size difference between a fully trained LLM and the training data that has been used. \n\nIt is easy to find that for example GPT2 was trained on 40GB of text, but how big is the resulting model?",
    "comments": [
        {
            "level": 0,
            "comment": "GPT-2-Small   355MB\n\nGPT-2-Medium   540MB\n\nGPT-2-Large  762MB\n\nGPT-2-1.5B  1.5GB",
            "score": 3,
            "author": "MrEloi"
        },
        {
            "level": 0,
            "comment": "Not sure why I'm doing your work for you, but you will find the whole story here:\n\nhttps://lifearchitect.ai/chinchilla/",
            "score": 2,
            "author": "MrEloi",
            "replies": [
                {
                    "level": 1,
                    "comment": "Thank you anyways. This is exactly what I have been lookinf for :)",
                    "score": 2,
                    "author": "Spielverderber23"
                },
                {
                    "level": 1,
                    "comment": "&gt;https://lifearchitect.ai/chinchilla/\n\nAppreciate this",
                    "score": 1,
                    "author": "RhinoWesl"
                }
            ]
        }
    ]
}