{
    "id": "142e8j0",
    "score": 0,
    "title": "Apples Pro Vision's will model the users eyes and appearance with AI/ML in realtime. That is a VERY STUPID use case.",
    "author": "Spielverderber23",
    "date": 1686053912.0,
    "url": "https://www.reddit.com/r/artificial/comments/142e8j0",
    "media_urls": [],
    "other_urls": [],
    "postText": "I am not here to judge whether the Pro Vision in general is a good idea or will work.\n\nBut when I watched the advertisement,  I was amazed by *where* they bring the AI in.\n\n&amp;#x200B;\n\n1. Modelling the part of the owners face that is covered by the Pro Vision, so other people can \"see\" it\n2. Modelling the whole of the users upper body for video calls.\n\n&amp;#x200B;\n\nTo me, this seems like exactly the domain where AI algorithms should *not* be employed.\n\nML algorithms, even if they do work well, approximate their task with a rough and opaque model. This is very useful for tasks where getting *close* to the real thing is good enough. Like recognizing dogs in a picture with, say 95% accuracy. Or generating images where the details do not count as long as the general thing convinces most people. Or writing certain genres of texts where factual accuracy is not a complete dealbreaker.\n\nBut looking another person in the face is not one of these scenarios. It is precisely a case where every detail matters. Where simplification, coherence and typicality employed by *every* ML algorithm is exactly what you do not want. You want raw data, not a representation. Facial expression and interpretation is about the nuances. It is an interface of communication that all of us carry. We cannot fully control it willfully, so it is a relatively direct link to our emotions for others. And human emotions are complex, non-linear, contradictory, individual. You do not want a ML model to tell a convincing visual story of a persons true emotions.\n\nI even feel this should be a general rule. DO NOT MODEL HUMAN INDIVIDUALS *AND* OFFER THE RESULT AS AN \"EQUAL\", REALTIME DROP-IN FOR THE REAL THING. Especially if your use case is not some kind of virtual world where everyone and everything is known to be virtual, but where you want to blend reality and virtual by introducing fake eyes or bodies of a real person beside real ones in a room or a video call.\n\nI don't even have to try the Pro Vision to make this point. Or know how good it actually fares. I would even argue that the better it will turn out to work, the better the illusion, the worse it actually is.\n\nSo why did Apple do this? Maybe they realized the problem, especially with video calls, rather late in the design process. \"Shit, you cannot sit in front of a camera with this thing!\". So ML could be a bodge here, a dirty fix. If it is like that, it does a disservice to the field and sets a dangerous precedent.",
    "comments": [
        {
            "level": 0,
            "comment": "Using an encoder decoder (decoding at the receiving end) is in fact a great compression technique for video conferences.",
            "score": 9,
            "author": "RealPerro"
        },
        {
            "level": 0,
            "comment": "Every mainstream video conferencing tool uses ML somewhere in its compression or cleanup processing. You\u2019re never \u201cjust seeing the person.\u201d\n\n\nhttps://blogs.nvidia.com/blog/2020/10/05/gan-video-conferencing-maxine/",
            "score": 6,
            "author": "TDaltonC",
            "replies": [
                {
                    "level": 1,
                    "comment": "Yeah cleanup and the like I get it. But is this already used to decompose and compose faces, in the wild?",
                    "score": 1,
                    "author": "Spielverderber23"
                }
            ]
        },
        {
            "level": 0,
            "comment": "I think it\u2019s a pretty good use case but I\u2019m not impressed so far with the actual implementation. There is no reason ML could not reproduce your face exactly as it really is just without the goggles on \u2013 through prior knowledge of your face and current data from what the cameras CAN see.",
            "score": 2,
            "author": "Aquillyne",
            "replies": [
                {
                    "level": 1,
                    "comment": "The cameras don't see anything. There is no internal camera, just eye tracking. They're doing exactly as you propose - scanning your face first, then outputting the eye tracking data.",
                    "score": 2,
                    "author": "w0lfiesmith",
                    "replies": [
                        {
                            "level": 2,
                            "comment": "External cameras must also be tracking lower half of your face especially lips.",
                            "score": 1,
                            "author": "Aquillyne"
                        }
                    ]
                }
            ]
        },
        {
            "level": 0,
            "comment": "Just say you don\u2019t have $3500",
            "score": 6,
            "author": "No-Winter6673",
            "replies": [
                {
                    "level": 1,
                    "comment": "Bro makes a good point and gets called poor, gotta love the internet.",
                    "score": 24,
                    "author": "HolyBanana818",
                    "replies": [
                        {
                            "level": 2,
                            "comment": "its a dopey point",
                            "score": 1,
                            "author": "intolerablesayings23"
                        }
                    ]
                },
                {
                    "level": 1,
                    "comment": "Bruh, brainless",
                    "score": 2,
                    "author": "Calm_Phase_9717"
                },
                {
                    "level": 1,
                    "comment": "Just say you want to burn 3500 dollars. I'd happily take that off your hands.",
                    "score": 1,
                    "author": "Nervous-Daikon-5393"
                },
                {
                    "level": 1,
                    "comment": "who would downvote this?",
                    "score": 0,
                    "author": "RealAI22"
                },
                {
                    "level": 1,
                    "comment": "Bruh, brainless",
                    "score": -2,
                    "author": "Capable-Durian3895"
                },
                {
                    "level": 1,
                    "comment": "Bruh, brainless",
                    "score": 1,
                    "author": "Calm_Phase_9717"
                },
                {
                    "level": 1,
                    "comment": "Bruh, brainless",
                    "score": -1,
                    "author": "Capable-Durian3895"
                }
            ]
        },
        {
            "level": 0,
            "comment": "I completely agree with you. And Apple's modeling of the user does not stop with appearance, check this out: https://twitter.com/sterlingcrispin/status/1665792422914453506\n\nThey change how apps behave based on their guessing the user's mental state. That's an exponential new level of \"personal data\" they will be generating, and very useful to advertisers and those engaged in professional opinion influence (aka political consultants).",
            "score": 3,
            "author": "bsenftner"
        },
        {
            "level": 0,
            "comment": "I read your post three times now trying to understand. I think I see what you\u2019re getting at? Regardless, I thought the demonstration looked incredible. I personally would like to see a side by side comparison of a person\u2019s real face, their Apple Vision face, and their MetaHuman face.",
            "score": 2,
            "author": "DandyDarkling"
        },
        {
            "level": 0,
            "comment": "Dear Silicon Valley,\n\nI'm not wearing goggles on my face.\n\nSincerely,\n\nAll the people that laughed at Meta.",
            "score": 1,
            "author": "Snohoman"
        },
        {
            "level": 0,
            "comment": "Executives and generals that don\u2019t know better will deploy ML solutions in tons of sources where they don\u2019t belong because people sell them on it. Get ready to be further disappointed I think",
            "score": 1,
            "author": "twilsonco",
            "replies": [
                {
                    "level": 1,
                    "comment": "Yes, BUT I think we are reaaallly close to the user being shown in a near-perfect (undetected) way. This is just a first-to-market necessity. Version 2.0 probably solves for it. (Hell, V1 probably might even get download upgrades that solve for it.)",
                    "score": 1,
                    "author": "smooth-brain_Sunday",
                    "replies": [
                        {
                            "level": 2,
                            "comment": "I\u2019m waiting on them to solve iOS problems from ten years ago. There\u2019s two things at Apple; gimmicks to help sell products, and neglected and unfinished features.",
                            "score": 1,
                            "author": "twilsonco"
                        }
                    ]
                }
            ]
        },
        {
            "level": 0,
            "comment": "Seeing the eyes is important for use cases where people are working together in the same space. It also is beneficial to have this tech if they want to represent someone\u2019s digital presence like on an avatar. All that non-verbal communication is gold. \n\nWhy not just have clear, see-through lenses like the HoloLens? It would reduce the field of view and visual quality (mostly color) for the user.",
            "score": 1,
            "author": "Crab_Shark"
        },
        {
            "level": 0,
            "comment": "If you want more corporate buy in for full time remote work, this is one of the steps.  They are looking at hosting the world\u2019s remote work.  I think they know what they are doing.",
            "score": 1,
            "author": "EverythingGoodWas"
        },
        {
            "level": 0,
            "comment": "I don\u2019t understand how it works, from a practical side. It looks like you can only see things and move things in the virtual sphere. But you can\u2019t really DO anything productivity wise, like typing?",
            "score": 1,
            "author": "SarW100",
            "replies": [
                {
                    "level": 1,
                    "comment": "You can do typing. They said it\u2019s got a virtual keyboard like iPhone, controlled by moving your fingers through the air. And it can also connect with other input devices like the Mac keyboard and the Mac mouse, and the PS5 and Xbox controllers.",
                    "score": 1,
                    "author": "PM_ME_ENFP_MEMES",
                    "replies": [
                        {
                            "level": 2,
                            "comment": "Is there a video released with the further explanation?",
                            "score": 1,
                            "author": "SarW100",
                            "replies": [
                                {
                                    "level": 3,
                                    "comment": "They explained that at WWDC",
                                    "score": 1,
                                    "author": "PM_ME_ENFP_MEMES"
                                }
                            ]
                        },
                        {
                            "level": 2,
                            "comment": "I found a video -- [https://youtu.be/TX9qSaGXFyg](https://youtu.be/TX9qSaGXFyg) \\-- it appears you still have to have a computer typepad, such as a laptop, that connects with the screen interface. The Pro Vision is largely a screen enhancement.",
                            "score": 1,
                            "author": "SarW100",
                            "replies": [
                                {
                                    "level": 3,
                                    "comment": "That\u2019s not what they said or even implied. They referred to the Vision as a stand-alone computer. I guess we\u2019ll all find out in January when we buy it for ourselves!",
                                    "score": 1,
                                    "author": "PM_ME_ENFP_MEMES",
                                    "replies": [
                                        {
                                            "level": 4,
                                            "comment": "Uh, that\u2019s what they say [here, on the official site](https://www.apple.com/apple-vision-pro/?&amp;&amp;cid=wwa-us-kwgo-avalanche--slid---Brand-AppleVision-Announce-&amp;mtid=20925qtb42335&amp;aosid=p238&amp;mnid=GS4SEdjw-dm_mtid_20925qtb42335_pcrid_%7Badid%7D_pgrid_152466493640_). The thing is a screen enhancement.",
                                            "score": 1,
                                            "author": "SarW100",
                                            "replies": [
                                                {
                                                    "level": 5,
                                                    "comment": "You are misunderstanding their point. Would you call the iPad a screen enhancement?",
                                                    "score": 1,
                                                    "author": "PM_ME_ENFP_MEMES",
                                                    "replies": [
                                                        {
                                                            "level": 6,
                                                            "comment": "How would you define the Vision Pro? \n\nFor me, it\u2019s more like a touch screen option because you are essentially expanding the view or changing the view of the screen within its interface. Is it like the iPad in functionality? No, because it doesn\u2019t seem to have a built in type pad for gesture typing. It still requires the computer connection to be traditionally productive, like typing in spreadsheets, etc. Though it does appear to have voice capabilities that could render AI text, but then how would you edit that text without additional attached apparatuses? Maybe there will be a voice function for that added? \n\nWith my iPad I can be all-inclusive, though if I want to be more productive I do add a keypad accessory. The Vision Pro\u2019s focus seems to be more on the AR/VR screen \u201ccanvas,\u201d as they mention on the website and in the promo videos.",
                                                            "score": 0,
                                                            "author": "SarW100"
                                                        }
                                                    ]
                                                }
                                            ]
                                        }
                                    ]
                                }
                            ]
                        }
                    ]
                }
            ]
        },
        {
            "level": 0,
            "comment": "meaningless gibberish",
            "score": 1,
            "author": "intolerablesayings23",
            "replies": [
                {
                    "level": 1,
                    "comment": "Seeing that other people managed to understand the idea and react positive or negative, you might be the problem.",
                    "score": 1,
                    "author": "Spielverderber23"
                }
            ]
        },
        {
            "level": 0,
            "comment": "I mean, a stupid use case is a use case nonetheless. I\u2019ll wait to see more and let the early adopters fund the inevitable price decrease.",
            "score": 0,
            "author": "PotentiallyAPickle"
        },
        {
            "level": 0,
            "comment": "Reminds me of the task bar.",
            "score": 0,
            "author": "roundearthervaxxer"
        },
        {
            "level": 0,
            "comment": "this is just the natural extension to Shannon\u2019s channel definition",
            "score": 0,
            "author": "FlipDetector",
            "replies": [
                {
                    "level": 1,
                    "comment": "&gt;Shannon\u2019s channel definition\n\nIsn't that whole idea developed around the idea of getting the maximum efficency without losing information? Lossy compression only is a way of minimizing information before it goes through the channel. \n\nI am not against lossy compression, that would be stupid. But currently, we use algorithms for that which we fully understand. They are deductive. They cannot add details or fill a missing detail with what it deems plausible.",
                    "score": 1,
                    "author": "Spielverderber23",
                    "replies": [
                        {
                            "level": 2,
                            "comment": "I am not sure about these details, I approach this from systems design and traffic flow perspective and how many facets a system can have that are all channels of perception. \n\nThe last time this concept was updated was Ericsson Round the year 2000 who introduced the non-linear channel, that enabled Netflix and dynamic compression based on inconsistent channel capacity. I see this as just Z added to the X and Y, projecting frames into 3D space. \n\nWhy this is interesting? To create a mental model we can use to understand sensory and information overload. That is where Signal to Noise ration starts making sense in the context of 3D and more.\n\nI work on a similar project (group video streaming) so this is really close to my field of expertise but I deal with the Engineering aspect and the flow of information where the content is irrelevant. Generative AI could handle temporary packet loss with this technology so details are still going to be added by predicting them.",
                            "score": 1,
                            "author": "FlipDetector"
                        }
                    ]
                }
            ]
        }
    ]
}