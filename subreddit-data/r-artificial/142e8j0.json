{
    "id": "142e8j0",
    "score": 0,
    "title": "Apples Pro Vision's will model the users eyes and appearance with AI/ML in realtime. That is a VERY STUPID use case.",
    "author": "Spielverderber23",
    "date": 1686053912.0,
    "url": null,
    "media_url": null,
    "comments": [
        {
            "level": 0,
            "comment": "Using an encoder decoder (decoding at the receiving end) is in fact a great compression technique for video conferences.",
            "score": 9
        },
        {
            "level": 0,
            "comment": "Every mainstream video conferencing tool uses ML somewhere in its compression or cleanup processing. You\u2019re never \u201cjust seeing the person.\u201d\n\n\nhttps://blogs.nvidia.com/blog/2020/10/05/gan-video-conferencing-maxine/",
            "score": 7,
            "replies": [
                {
                    "level": 1,
                    "comment": "Yeah cleanup and the like I get it. But is this already used to decompose and compose faces, in the wild?",
                    "score": 1
                }
            ]
        },
        {
            "level": 0,
            "comment": "I think it\u2019s a pretty good use case but I\u2019m not impressed so far with the actual implementation. There is no reason ML could not reproduce your face exactly as it really is just without the goggles on \u2013 through prior knowledge of your face and current data from what the cameras CAN see.",
            "score": 2,
            "replies": [
                {
                    "level": 1,
                    "comment": "The cameras don't see anything. There is no internal camera, just eye tracking. They're doing exactly as you propose - scanning your face first, then outputting the eye tracking data.",
                    "score": 2,
                    "replies": [
                        {
                            "level": 2,
                            "comment": "External cameras must also be tracking lower half of your face especially lips.",
                            "score": 1
                        }
                    ]
                }
            ]
        },
        {
            "level": 0,
            "comment": "Just say you don\u2019t have $3500",
            "score": 6,
            "replies": [
                {
                    "level": 1,
                    "comment": "Bro makes a good point and gets called poor, gotta love the internet.",
                    "score": 25,
                    "replies": [
                        {
                            "level": 2,
                            "comment": "its a dopey point",
                            "score": 1
                        }
                    ]
                },
                {
                    "level": 1,
                    "comment": "Bruh, brainless",
                    "score": 2
                },
                {
                    "level": 1,
                    "comment": "Just say you want to burn 3500 dollars. I'd happily take that off your hands.",
                    "score": 1
                },
                {
                    "level": 1,
                    "comment": "who would downvote this?",
                    "score": 1
                },
                {
                    "level": 1,
                    "comment": "Bruh, brainless",
                    "score": -1
                },
                {
                    "level": 1,
                    "comment": "Bruh, brainless",
                    "score": 1
                },
                {
                    "level": 1,
                    "comment": "Bruh, brainless",
                    "score": -2
                }
            ]
        },
        {
            "level": 0,
            "comment": "I completely agree with you. And Apple's modeling of the user does not stop with appearance, check this out: https://twitter.com/sterlingcrispin/status/1665792422914453506\n\nThey change how apps behave based on their guessing the user's mental state. That's an exponential new level of \"personal data\" they will be generating, and very useful to advertisers and those engaged in professional opinion influence (aka political consultants).",
            "score": 2
        },
        {
            "level": 0,
            "comment": "I read your post three times now trying to understand. I think I see what you\u2019re getting at? Regardless, I thought the demonstration looked incredible. I personally would like to see a side by side comparison of a person\u2019s real face, their Apple Vision face, and their MetaHuman face.",
            "score": 1
        },
        {
            "level": 0,
            "comment": "Dear Silicon Valley,\n\nI'm not wearing goggles on my face.\n\nSincerely,\n\nAll the people that laughed at Meta.",
            "score": 1
        },
        {
            "level": 0,
            "comment": "Executives and generals that don\u2019t know better will deploy ML solutions in tons of sources where they don\u2019t belong because people sell them on it. Get ready to be further disappointed I think",
            "score": 1,
            "replies": [
                {
                    "level": 1,
                    "comment": "Yes, BUT I think we are reaaallly close to the user being shown in a near-perfect (undetected) way. This is just a first-to-market necessity. Version 2.0 probably solves for it. (Hell, V1 probably might even get download upgrades that solve for it.)",
                    "score": 1,
                    "replies": [
                        {
                            "level": 2,
                            "comment": "I\u2019m waiting on them to solve iOS problems from ten years ago. There\u2019s two things at Apple; gimmicks to help sell products, and neglected and unfinished features.",
                            "score": 1
                        }
                    ]
                }
            ]
        },
        {
            "level": 0,
            "comment": "Seeing the eyes is important for use cases where people are working together in the same space. It also is beneficial to have this tech if they want to represent someone\u2019s digital presence like on an avatar. All that non-verbal communication is gold. \n\nWhy not just have clear, see-through lenses like the HoloLens? It would reduce the field of view and visual quality (mostly color) for the user.",
            "score": 1
        },
        {
            "level": 0,
            "comment": "If you want more corporate buy in for full time remote work, this is one of the steps.  They are looking at hosting the world\u2019s remote work.  I think they know what they are doing.",
            "score": 1
        },
        {
            "level": 0,
            "comment": "I don\u2019t understand how it works, from a practical side. It looks like you can only see things and move things in the virtual sphere. But you can\u2019t really DO anything productivity wise, like typing?",
            "score": 1,
            "replies": [
                {
                    "level": 1,
                    "comment": "You can do typing. They said it\u2019s got a virtual keyboard like iPhone, controlled by moving your fingers through the air. And it can also connect with other input devices like the Mac keyboard and the Mac mouse, and the PS5 and Xbox controllers.",
                    "score": 1,
                    "replies": [
                        {
                            "level": 2,
                            "comment": "Is there a video released with the further explanation?",
                            "score": 1,
                            "replies": [
                                {
                                    "level": 3,
                                    "comment": "They explained that at WWDC",
                                    "score": 1
                                }
                            ]
                        },
                        {
                            "level": 2,
                            "comment": "I found a video -- [https://youtu.be/TX9qSaGXFyg](https://youtu.be/TX9qSaGXFyg) \\-- it appears you still have to have a computer typepad, such as a laptop, that connects with the screen interface. The Pro Vision is largely a screen enhancement.",
                            "score": 1,
                            "replies": [
                                {
                                    "level": 3,
                                    "comment": "That\u2019s not what they said or even implied. They referred to the Vision as a stand-alone computer. I guess we\u2019ll all find out in January when we buy it for ourselves!",
                                    "score": 1,
                                    "replies": [
                                        {
                                            "level": 4,
                                            "comment": "Uh, that\u2019s what they say [here, on the official site](https://www.apple.com/apple-vision-pro/?&amp;&amp;cid=wwa-us-kwgo-avalanche--slid---Brand-AppleVision-Announce-&amp;mtid=20925qtb42335&amp;aosid=p238&amp;mnid=GS4SEdjw-dm_mtid_20925qtb42335_pcrid_%7Badid%7D_pgrid_152466493640_). The thing is a screen enhancement.",
                                            "score": 1,
                                            "replies": [
                                                {
                                                    "level": 5,
                                                    "comment": "You are misunderstanding their point. Would you call the iPad a screen enhancement?",
                                                    "score": 1,
                                                    "replies": [
                                                        {
                                                            "level": 6,
                                                            "comment": "How would you define the Vision Pro? \n\nFor me, it\u2019s more like a touch screen option because you are essentially expanding the view or changing the view of the screen within its interface. Is it like the iPad in functionality? No, because it doesn\u2019t seem to have a built in type pad for gesture typing. It still requires the computer connection to be traditionally productive, like typing in spreadsheets, etc. Though it does appear to have voice capabilities that could render AI text, but then how would you edit that text without additional attached apparatuses? Maybe there will be a voice function for that added? \n\nWith my iPad I can be all-inclusive, though if I want to be more productive I do add a keypad accessory. The Vision Pro\u2019s focus seems to be more on the AR/VR screen \u201ccanvas,\u201d as they mention on the website and in the promo videos.",
                                                            "score": 0
                                                        }
                                                    ]
                                                }
                                            ]
                                        }
                                    ]
                                }
                            ]
                        }
                    ]
                }
            ]
        },
        {
            "level": 0,
            "comment": "meaningless gibberish",
            "score": 1,
            "replies": [
                {
                    "level": 1,
                    "comment": "Seeing that other people managed to understand the idea and react positive or negative, you might be the problem.",
                    "score": 1
                }
            ]
        },
        {
            "level": 0,
            "comment": "I mean, a stupid use case is a use case nonetheless. I\u2019ll wait to see more and let the early adopters fund the inevitable price decrease.",
            "score": 0
        },
        {
            "level": 0,
            "comment": "Reminds me of the task bar.",
            "score": 0
        },
        {
            "level": 0,
            "comment": "this is just the natural extension to Shannon\u2019s channel definition",
            "score": 0,
            "replies": [
                {
                    "level": 1,
                    "comment": "&gt;Shannon\u2019s channel definition\n\nIsn't that whole idea developed around the idea of getting the maximum efficency without losing information? Lossy compression only is a way of minimizing information before it goes through the channel. \n\nI am not against lossy compression, that would be stupid. But currently, we use algorithms for that which we fully understand. They are deductive. They cannot add details or fill a missing detail with what it deems plausible.",
                    "score": 1,
                    "replies": [
                        {
                            "level": 2,
                            "comment": "I am not sure about these details, I approach this from systems design and traffic flow perspective and how many facets a system can have that are all channels of perception. \n\nThe last time this concept was updated was Ericsson Round the year 2000 who introduced the non-linear channel, that enabled Netflix and dynamic compression based on inconsistent channel capacity. I see this as just Z added to the X and Y, projecting frames into 3D space. \n\nWhy this is interesting? To create a mental model we can use to understand sensory and information overload. That is where Signal to Noise ration starts making sense in the context of 3D and more.\n\nI work on a similar project (group video streaming) so this is really close to my field of expertise but I deal with the Engineering aspect and the flow of information where the content is irrelevant. Generative AI could handle temporary packet loss with this technology so details are still going to be added by predicting them.",
                            "score": 1
                        }
                    ]
                }
            ]
        }
    ]
}