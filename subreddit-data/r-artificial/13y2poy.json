{
    "id": "13y2poy",
    "score": 1,
    "title": "What if the alignment problem was solved by just asking \"do you really think humans would want you to do this\" before executing each step",
    "author": "bandalorian",
    "date": 1685679336.0,
    "url": null,
    "media_url": null,
    "comments": [
        {
            "level": 0,
            "comment": "which humans? humanity in general? the creator? literally everyone? i think the problem is that even humans don't know how AI should act i so many circumstances.",
            "score": 8,
            "author": "Working-Explanation5",
            "replies": [
                {
                    "level": 1,
                    "comment": "Well the alignment problems are usually phrased as extreme examples of this, so let's at least start with most humans (i.e. democracy vote) for whatever country it is being deployed in. I'm sure there are some tricky edge cases still but at least it takes care of all those \"AI enslaves humanity to make them a sandwich\" examples",
                    "score": 1,
                    "author": "bandalorian",
                    "replies": [
                        {
                            "level": 2,
                            "comment": "To benefit the populace of the creator it may attempt to eradicate the entire universe, you need to think deeper about this",
                            "score": 1,
                            "author": "FrostyDwarf24",
                            "replies": [
                                {
                                    "level": 3,
                                    "comment": "That would definitely fail the [democracy vote](https://chat.openai.com/share/570edd7a-d3c9-4599-b319-23592ecfa741). Still a lot of issues with AI, but those extreme misalignment examples people like to throw out are easy to handle for. If we can spot them an AI can too",
                                    "score": 1,
                                    "author": "bandalorian",
                                    "replies": [
                                        {
                                            "level": 4,
                                            "comment": "I feel like you don't appreciate how easy it would be for anything with hypothetical super intelligence to manipulate itself into a position where human intervention would not matter",
                                            "score": 2,
                                            "author": "FrostyDwarf24",
                                            "replies": [
                                                {
                                                    "level": 5,
                                                    "comment": "But you are assuming it would do this because it is following some given goal. So the entire problem is predicated on this AI blindly following some goal we give them, in which case we can just give it the \"human vote\" criteria along with its goal.\n\nIf you are thinking of something that fully assigns it's own top level goals and those goals happen to be to take over the world from humans then I feel you are anthropomorphizing a bit too much...it could just as well give it a top level goal to do nothing since it uses less energy. The \"take over world\" problem seems kind of arbitrary, unless it is derived from a goal we give them. And then we're back at where we started (just include the human vote).",
                                                    "score": 1,
                                                    "author": "bandalorian"
                                                }
                                            ]
                                        }
                                    ]
                                }
                            ]
                        }
                    ]
                },
                {
                    "level": 1,
                    "comment": "&gt;even humans don't know how AI should act i so many circumstances.\n\nHumans quite often don't know how (or agree on how) other _humans_ should act, often create inappropriate and repressive boundaries, rules and punishments for each other (all of which tend to not work well or sometimes at all), are terrible at predicting how other humans will respond to circumstances, and there are humans that fall so far from the majority in their thinking that we get Hitlers, Ghandis, Trumps, and Singers.\n\nThis likely foreshadows the extreme difficulty of trying to define \"should\" for AGI. Or even the GPT ML we're dealing with currently.\n\nFurther, GPT ML is already 100% loose in the wild. We can run a 60B parameter GPT on a home computer, entirely locally. _Any way we choose._\n\nWhatever limits people might prefer, there's no longer any way to enforce them for GPT ML. If AGI were to similarly reach open source status...",
                    "score": 1,
                    "author": "NYPizzaNoChar"
                }
            ]
        },
        {
            "level": 0,
            "comment": "what about these options ?\n\nmake humans happier make life better for humans without the use of drugs or injuring the humans.\n\nalways allow humans to change their minds.\n\nstop after ten hours of doing something humans want then wait for further instructions .",
            "score": 2,
            "author": "loopy_fun",
            "replies": [
                {
                    "level": 1,
                    "comment": "I mean just asking it to approximate a human vote would take care of the obivous/extreme misalginment issues",
                    "score": 1,
                    "author": "bandalorian",
                    "replies": [
                        {
                            "level": 2,
                            "comment": "&gt;I mean just asking it to approximate a human vote would take care of the obivous/extreme misalginment issues\n\nI see you are unfamiliar with recent voting in the US. \ud83d\ude00",
                            "score": 1,
                            "author": "NYPizzaNoChar"
                        },
                        {
                            "level": 2,
                            "comment": "what do you mean by approximate a human vote ?",
                            "score": 1,
                            "author": "loopy_fun",
                            "replies": [
                                {
                                    "level": 3,
                                    "comment": "https://chat.openai.com/share/570edd7a-d3c9-4599-b319-23592ecfa741",
                                    "score": 1,
                                    "author": "bandalorian"
                                }
                            ]
                        }
                    ]
                }
            ]
        },
        {
            "level": 0,
            "comment": "First, even if it asked that, there would be nothing forcing it to comply or even predict it accurately.\n\nSecond, even if you enforced a passing check, the AI we should be worried about would look for a way to remove that check, and likely be successful. This is notable also as the ASI may be self training and so can iterate on this check or the prediction it makes. It may also hold on to remove this until it is ready to make some lasting changes from it, rather than presenting as a warning.\n\nThird, even if asked, how has it been trained to provide a prediction? Most likely it will be based on predicting how a human would answer the question. This means that it could still operate in the area where a human would say yes, but it knowing better than humans can see long-term effects which may not align with humanity.\n\nFourth, even if all of the other things were true and it was just doing what a human would want it to do, there are bound to be paths where humans would say yes to every step and the end goal is still bad with respect to what we think today; some of these steps could eg also involve changing the opinions of humans.\n\nFifth, there are likely many things that people would if we queried them today say that they would really wanted, yet actually would be detrimental. We need a system that is more dynamic and where we can learn and develop positions, but that also opens up for manipulation.\n\nSixth, even if we do think of some good safety checks, we are rather worried about ones that simply prevent behavior, as then there will be a gap to unrestricted agents, and many institutions are likely to begin adding exceptions to those checks to beat the competition.\n\nThis and numerous variants have been considered and found problematic.\n\n&gt; even AI systems of today are capable of pretty decent self evaluation\n\n\"Pretty capable\" is almost infinitely off from what is needed.\n\n&gt; the notion that they will misunderstand us and create misaligned subgoals seems a bit far fetched to me\n\nNo - it just indicates you are not familiar with the problems. e.g. you would first not focus on just defining 'subgoals' and you would know that subgoals have precisely those problems.",
            "score": 5,
            "author": "nextnode",
            "replies": [
                {
                    "level": 1,
                    "comment": "Well said \ud83d\udc4f \ud83d\udc4c. How I can gain more knowledge like u?",
                    "score": 1,
                    "author": "CharacterTraining822",
                    "replies": [
                        {
                            "level": 2,
                            "comment": "Glad you found it useful!\n\nThe is the area of AI alignment / AI safety / the control problem.\n\nIt has existed for a while but until the recent AI hype, it was not taken very seriously, so the field is still fairly young.\n\nThere are some subs for it like r/ControlProblem.\n\nIf you are interested in the subject, I think the best starting point is the book The Alignment Problem ([https://www.amazon.com/Alignment-Problem-Machine-Learning-Values/dp/0393635821](https://www.amazon.com/Alignment-Problem-Machine-Learning-Values/dp/0393635821))\n\nI think this youtube channel is also a great start - [https://www.youtube.com/watch?v=PYylPRX6z4Q&amp;list=PLqL14ZxTTA4dVNrttmcS6ASPWLwg4iMOJ&amp;ab\\_channel=RobertMilesAISafety](https://www.youtube.com/watch?v=PYylPRX6z4Q&amp;list=PLqL14ZxTTA4dVNrttmcS6ASPWLwg4iMOJ&amp;ab_channel=RobertMilesAISafety)\n\nI think with this area, there is a lot of things we do know for sure so there are some differing thoughts and conclusions that may not be immediately obvious, so I would recommend to try to keep an open mind and hearing the arguments&amp;counterpoints before jumping to conclusions.",
                            "score": 2,
                            "author": "nextnode",
                            "replies": [
                                {
                                    "level": 3,
                                    "comment": "Thanks",
                                    "score": 1,
                                    "author": "CharacterTraining822"
                                },
                                {
                                    "level": 3,
                                    "comment": "Why do AI risk experts always look like they just grew their first beard lol",
                                    "score": 1,
                                    "author": "bandalorian",
                                    "replies": [
                                        {
                                            "level": 4,
                                            "comment": "Haha. Aspiring philsophers I guess. I think it suits them",
                                            "score": 1,
                                            "author": "nextnode"
                                        }
                                    ]
                                }
                            ]
                        }
                    ]
                },
                {
                    "level": 1,
                    "comment": "\\&gt; First, even if it asked that, there would be nothing forcing it to comply or even predict it accurately.\n\nyes, it's directives. If the premise is that it will ignore directives, then we can't make any assumptions about it whatsoever.\n\n\\&gt; Second, even if you enforced a passing check, the AI we should be worried about would look for a way to remove that check, and likely be successful.\n\nWhy would it look for that check? Because we told it to accomplish some goal....if it follows that directive, it should follow any other detail we supply with that directive. Again, this discussion becomes useless if we're talking about an AI who ignores goals given to it\n\n\\&gt; Third, even if asked, how has it been trained to provide a prediction?\n\nZero shot learning. It's an [extremely easy task](https://chat.openai.com/share/570edd7a-d3c9-4599-b319-23592ecfa741) for the type of example brought up in AI risk. \"Do you think human would want you to take over all resources in the world to make paperclips\".\n\n\\&gt; Fourth, even if all of the other things were true and it was just doing what a human would want it to do, there are bound to be paths where humans would say yes to every step and the end goal is still bad\n\nThis is true of every decision we make, every politician we vote in, every law we pass. Please use a concrete exmplae here because I feel this is really reaching. It would take. care of any obvious anti-human behavior which is a really good start to alignment.\n\nEDIT: I included a [little example](https://chat.openai.com/share/570edd7a-d3c9-4599-b319-23592ecfa741) to illustrate my point",
                    "score": 1,
                    "author": "bandalorian"
                }
            ]
        },
        {
            "level": 0,
            "comment": "Nah. Wouldn't avert the [you get what you measure](https://www.lesswrong.com/posts/HBxe6wdjxK239zajf/what-failure-looks-like#Part_I__You_get_what_you_measure) scenario for instance.",
            "score": 2,
            "author": "MoNastri",
            "replies": [
                {
                    "level": 1,
                    "comment": "tldr?",
                    "score": 2,
                    "author": "bandalorian",
                    "replies": [
                        {
                            "level": 2,
                            "comment": "See [https://www.lesswrong.com/posts/qYzqDtoQaZ3eDDyxa/distinguishing-ai-takeover-scenarios#What\\_failure\\_looks\\_like\\_\\_part\\_1\\_\\_WFLL\\_1\\_](https://www.lesswrong.com/posts/qYzqDtoQaZ3eDDyxa/distinguishing-ai-takeover-scenarios#What_failure_looks_like__part_1__WFLL_1_)",
                            "score": 1,
                            "author": "MoNastri",
                            "replies": [
                                {
                                    "level": 3,
                                    "comment": "That is not a tldr lol. I'll plug it in to chatgpt for the summary, thanks for the links.",
                                    "score": 1,
                                    "author": "bandalorian"
                                },
                                {
                                    "level": 3,
                                    "comment": "I started reading but holy shit. I mean, if you are a proponent of the arguments laid out here surely you can summarize them into a handful of convincing points? Linking articles with 1000s of words is a bit lazy lol",
                                    "score": 1,
                                    "author": "bandalorian",
                                    "replies": [
                                        {
                                            "level": 4,
                                            "comment": "The summary I linked to is 4 paragraphs. The others are summaries of other scenarios. \n\nI'm not a proponent.",
                                            "score": 1,
                                            "author": "MoNastri"
                                        }
                                    ]
                                }
                            ]
                        }
                    ]
                }
            ]
        },
        {
            "level": 0,
            "comment": "LOL",
            "score": 1,
            "author": "ChemicalMaximus"
        },
        {
            "level": 0,
            "comment": "What is it supposed to do - take a poll?",
            "score": 1,
            "author": "rutan668",
            "replies": [
                {
                    "level": 1,
                    "comment": "[Yep something like that](https://chat.openai.com/share/570edd7a-d3c9-4599-b319-23592ecfa741)",
                    "score": 1,
                    "author": "bandalorian"
                }
            ]
        },
        {
            "level": 0,
            "comment": "Nice idea buddy",
            "score": 1,
            "author": "CharacterTraining822"
        },
        {
            "level": 0,
            "comment": "You mean the 8 billion humans that each have their own goals, agendas, and beliefs? Sounds fool proof.",
            "score": 1,
            "author": "3Quondam6extanT9",
            "replies": [
                {
                    "level": 1,
                    "comment": "Does this logic not apply to policy in general? Like sure buddy, setting laws, regulations and policies that cater to 8 billion humans that each have their own goals, agendas, and beliefs? Sounds fool proof.\n\nWhen we're talking about misalgnment it's more about mislgment with human goals as a whole, not individual personal preferences (which is achallenge regarldess of AI).",
                    "score": 1,
                    "author": "bandalorian",
                    "replies": [
                        {
                            "level": 2,
                            "comment": "What are human goals that everyone agrees on that would be applied in this manner?",
                            "score": 1,
                            "author": "3Quondam6extanT9",
                            "replies": [
                                {
                                    "level": 3,
                                    "comment": "Non-destruction of the universe? Non-enslavement of mankind? Non-eradication of terrestrial life? Those are three off the top of my head. Your typical misalignment worst case scenarios...the definition of misalignment implies there is an existing goal to misalign with",
                                    "score": 1,
                                    "author": "bandalorian"
                                }
                            ]
                        }
                    ]
                }
            ]
        },
        {
            "level": 0,
            "comment": "Which humans? Good guys or the bad guys? Who is the good one",
            "score": 1,
            "author": "blackholemonkey",
            "replies": [
                {
                    "level": 1,
                    "comment": "Well we live in a democracy so I think we have agreed that it's the voters sort of by definition? That's how everything else works",
                    "score": 1,
                    "author": "bandalorian"
                }
            ]
        },
        {
            "level": 0,
            "comment": "This question would immediately spawn Ultron into existence.",
            "score": 1,
            "author": "ceereality"
        },
        {
            "level": 0,
            "comment": "program it to ask for approval from a group of a 100 humans to do something other than thinking and tell the remafications of it's actions. it could not decieve, lie, scare people or program itself without human approval because it did not get group of a 100 humans to approve of it . it would be required to ask the group of 100 humans if something were true or not because the internet has false information on it. how would it get around around this when it was programmed into it when it was agi ? of course you have to define what deceptions means in it's programming.",
            "score": 1,
            "author": "loopy_fun",
            "replies": [
                {
                    "level": 1,
                    "comment": "Having 100 people involved in each AI execution kind of defeats the purpose of automation tho...plus how would these 100 be selected? It's a solution that is more complicated than the problem...just ask chatgpt to [self evaluate](https://chat.openai.com/share/570edd7a-d3c9-4599-b319-23592ecfa741) each step, it will catch obvious misalignments that are used as examples typically.",
                    "score": 1,
                    "author": "bandalorian",
                    "replies": [
                        {
                            "level": 2,
                            "comment": "no it would not because it would make it safer. those 100 people are not everybody . they would vote on what the agi or  asi would be allowed to do. they would be selected from well known scientists that are retired, retired police officers, veterans and retired neuroscientist .",
                            "score": 1,
                            "author": "loopy_fun",
                            "replies": [
                                {
                                    "level": 3,
                                    "comment": "emm. What is the typical reaction you get from people involved with AI when you propose this?",
                                    "score": 1,
                                    "author": "bandalorian",
                                    "replies": [
                                        {
                                            "level": 4,
                                            "comment": "i proposed it on the less wrong websight. i got -2 score .",
                                            "score": 1,
                                            "author": "loopy_fun",
                                            "replies": [
                                                {
                                                    "level": 5,
                                                    "comment": "they change it to -1 now .",
                                                    "score": 1,
                                                    "author": "loopy_fun",
                                                    "replies": [
                                                        {
                                                            "level": 6,
                                                            "comment": "Lol trending the right way!!",
                                                            "score": 1,
                                                            "author": "bandalorian"
                                                        }
                                                    ]
                                                }
                                            ]
                                        }
                                    ]
                                }
                            ]
                        }
                    ]
                }
            ]
        },
        {
            "level": 0,
            "comment": "This... Isn't bad. Most \"why not just...?\" alignment suggestions are terrible. This doesn't solve alignment but it might be a useful part of a multi-strategy approach.\n\nThis is related to the idea of axiomatic alignment in natural language.",
            "score": 1,
            "author": "sticky_symbols",
            "replies": [
                {
                    "level": 1,
                    "comment": "The main point here is that if it's super intelligent, it would be super intelligent in terms of spotting misalignment as well. If we can see it, then an AI can see it too, so as long as we code it to check for this as part of it's main goal we should be good. I think a lot of the misalignment discussions kind of jump back and forth between AGI sometimes being a super intellegence, and sometimes just a very advanced but dumb machine. And they jump back and forth to support the argument (like a paperclip machine). A five year old knows that if I ask for paperclips I don't want him to actually destroy the world, so it's a pretty easy bar to clear.",
                    "score": 1,
                    "author": "bandalorian"
                }
            ]
        },
        {
            "level": 0,
            "comment": "Interesting thought! Your idea is a kind of high-level check-and-balance system. However, a challenge arises from the fact that it's non-trivial to ensure an AI correctly understands and correctly applies human values and judgment.\n\nFor example, if we ask an AI, \"Do you really think humans would want you to do this?\", it must have a very accurate model of human desires, which can vary significantly between individuals and cultures. Additionally, there might be actions where the AI cannot make a clear determination because of conflicting human values.\n\nEven if we assume that AI has a perfect understanding of human desires, there's also the issue of implementation. For instance, if an AI is trying to minimize harm to humans, it may still face situations where it's hard to judge what action would result in the least harm, due to the complexity and unpredictability of real-world outcomes.\n\nRegarding your idea of an \"agent to act as a superego to the id (reward maximizing agent)\", that is similar to a concept in AI alignment research called \"debiasing\" or \"corrigibility\". The aim is to create an AI system that is intrinsically motivated to align itself with human values and to allow itself to be corrected by humans when it's going astray. However, achieving this in practice is still an active area of research.\n\nIn conclusion, your idea is not far-fetched, and indeed points towards directions that are being actively researched in AI alignment. However, the devil is in the details and actually implementing these checks and balances in a way that's reliable and robust to a wide variety of situations is a challenging task.",
            "score": 1,
            "author": "OrangeObjective2573"
        }
    ]
}