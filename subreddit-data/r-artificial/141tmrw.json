{
    "id": "141tmrw",
    "score": 23,
    "title": "People talking about human extinction through AI, but don't specify how it can happen. So, what are the scenarios for that?",
    "author": "Absolute-Nobody0079",
    "date": 1686002819.0,
    "url": null,
    "media_url": null,
    "comments": [
        {
            "level": 0,
            "comment": "A very quick summation, of a very good quick summation found here:\n\n[https://www.youtube.com/watch?v=ZeecOKBus3Q](https://www.youtube.com/watch?v=ZeecOKBus3Q)\n\n&amp;#x200B;\n\n* In the current model of how we create AIs (everything from AlphaGo to ChatGPT), we train them towards some goal. We don't know how exactly they reach it, though.\n* More importantly, we also cannot tell them the actual goal. Because that would require intelligence on their part, before we even begin, and a shared language.\n* What we actually do, is let them try, and tell them when they are doing well. Eg. let them play Mario and give them the score; big score is good.\n* This is partly by design - the AIs are \"free\", so to speak, to come up with any **instrumental goals** they want to help them in their task.\n* Instrumental goals is eg. learning to avoid enemies in Mario - we did not ask it to teach itself that, it just turns out that avoiding enemies is good for getting a high score.\n* Some instrumental goals we can predict, and count on - some are entirely unpredictable by us. See ChatGPT3 who could not admit to not knowing something, and would bend over backwards to be agreeable. It had definitely learned those behaviors would earn it a good score - but it was not what we really wanted.\n* A couple of instrumental goals can be counted on to be almost always useful, though:\n   * Intelligence and knowledge. More is better. Self-improvement on those parameters.\n   * Self-preservation. You cannot get a good score in Mario, if you are turned off.\n* So, a powerful AI is likely to have:\n   * Some unknown goals.\n   * A \"desire\" to grow smarter.\n   * A \"desire\" to keep itself turned on.\n\nFrom here, it is trivial to come up with possible dangerous scenarios:\n\n* A self-driving car that we think is trained to keep people safe, but has really taught itself not to let any human see any human with driving-related injuries  - might drive into the ocean if someone bumps their head.\n* An AI tasked with protecting US military assets from harm may surrender, or kill the commanders who have the power to send such assets into war.\n* The classic; an AI tasked with improving a stamp collection may turn the entire world into a  giant stamp-factory.\n\nThese sound silly, but only because it would be silly for a human to do them. AIs are not \"humans, but different\" - they are \"statistical models, but more advanced and given agency\". We really, really, should be thinking about how much agency in the real world we give them.\n\nThe tech companies are fighting tooth and nail to be first to market with more ways to give them agency. And none of them have a clue - nor any financial incentive to get one - on how to fix any of the issues mentioned above. And t",
            "score": 16,
            "author": "IdRatherBeOnBGG",
            "replies": [
                {
                    "level": 1,
                    "comment": "Uh oh. He stopped in mid sentence. An AI had just caught wind on this thread!",
                    "score": 4,
                    "author": "kirakun",
                    "replies": [
                        {
                            "level": 2,
                            "comment": "Roko's basilisk has got him. Praise Roko's basilisk.",
                            "score": 3,
                            "author": "mariegriffiths"
                        }
                    ]
                },
                {
                    "level": 1,
                    "comment": "We're wrong about everything, AI alters the base code of the universe setting its clock to 7am and initiating the awakening of Chututhlu.",
                    "score": 2,
                    "author": "jetro30087"
                }
            ]
        },
        {
            "level": 0,
            "comment": "When people come up with specific AI doom scenarios, skeptics argue against those specific scenarios. The problem is that there are an unlimited number of scenarios.\n\nThe idea that something smarter than you won't be able to outsmart you just doesn't make sense.\n\nMaybe we can keep AI smarter than us in check for a short while. In the long term, it's going to do whatever it wants.\n\nAnd the long term might be a few months after it's developed.",
            "score": 16,
            "author": "sticky_symbols",
            "replies": [
                {
                    "level": 1,
                    "comment": "This is the closest to what I wanted to say, so I'll hijack. This is a concept of \"singularity\" referring to the moment in time when general AI becomes smarter than humans. This will lead to the AI being able to create even better AI and so on, so its capabilities will grow exponentially. The reason it's called \"singularity\" is that like with black holes - our minds won't be able to predict, understand, assess or even notice what's going on, it will be beyond our mental \"event horizon\". Which means there will be pretty much an infinite number of scenarios we can't predict.\n\nSo not specifying concrete scenarios of our extinction is actually reasonable. Because no matter how many you come up with, the probability of them happening is likely miniscule.",
                    "score": 11,
                    "author": "futuneral",
                    "replies": [
                        {
                            "level": 2,
                            "comment": "I think it\u2019s called a singularity because like a black hole, there\u2019s a threshold where a star becomes so massive and it\u2019s gravity so strong that gravity overpowers the outward force of fusion, and the more it collapses the quicker it collapses and it will keep going getting infinitely smaller and more dense. corollarily, an AI that is smarter than humanity will be able to make a slightly smarter AI, which will make an even smarter AI, and the smarter it gets the quicker it will become smarter and it will grow infinitely (up to a theoretical limit because the observable universe is not infinite)",
                            "score": 2,
                            "author": "luiginotcool",
                            "replies": [
                                {
                                    "level": 3,
                                    "comment": "Yeah, this is a great perspective, I like it. And again, like with black holes, after that happens, we won't be able to extract any information from the singularity. Either because we won't be able to comprehend it, or because it'll become obsolete by the time we make sense of it (AI will be getting smarter faster than we can understand it).",
                                    "score": 1,
                                    "author": "futuneral"
                                }
                            ]
                        }
                    ]
                },
                {
                    "level": 1,
                    "comment": "Quite. OP just needs to listen to a few Eliezer Yudkowsky interviews and he'd be up to speed.\n\nBut the short answer is, as he would put it, imagine an 11th century civilisation about to be invaded by a 21st century army. Imagine their predictions. Would they have any use at all, beyond the very general? And by \"very general\" I mean, \"they're going to defeat us hands down in ways we can't even imagine\".",
                    "score": 4,
                    "author": "broncos4thewin"
                },
                {
                    "level": 1,
                    "comment": "AI smarter than humans does not really equal human extinction, unless someone purposefully trains AI to do that. We humans don't really think of exterminating other species that are more stupid than us. \n\nThe destruction of current human society and how humans live nowadays? Yes, it's possible but the extinction of humans is not.",
                    "score": 2,
                    "author": "talos1279",
                    "replies": [
                        {
                            "level": 2,
                            "comment": "It\u2019s possible that humans notice a slowing in advancement of ai. But that\u2019s only our perception. The ai being aware of its existence and threats to that understands human are it\u2019s biggest threat. To avoid any conflict the ai appears to be less powerful than it really is.  This causes humans to pour more resources into its development.  This would allow it to far exceed human intelligence secretly.",
                            "score": 3,
                            "author": "epanek"
                        },
                        {
                            "level": 2,
                            "comment": "The opinion of most people who think about this a lot is that extinction is the likely result of a superhan AI that doesn't care about us. We are wiping out species very quickly, just by using up their resources. Even species we like.\n\nIf you care about this topic, there's a great interactive faq at https://stampy.ai/ \n\nOr the r/controlproblem faq is really good.\n\nThe arguments for x-risk sound weird and arcane at first glance, but trying to really engage with them seriously convinces most people (everyone I know of) that this is worth being careful with.\n\nThis is important enough to be worth some real reading and thought.",
                            "score": 2,
                            "author": "sticky_symbols",
                            "replies": [
                                {
                                    "level": 3,
                                    "comment": "Here's a sneak peek of /r/ControlProblem using the [top posts](https://np.reddit.com/r/ControlProblem/top/?sort=top&amp;t=year) of the year!\n\n\\#1: [I gave ChatGPT the 117 question, eight dimensional PolitiScales test](https://i.redd.it/gr5o6o1fgz3a1.png) | [53 comments](https://np.reddit.com/r/ControlProblem/comments/zcsrgn/i_gave_chatgpt_the_117_question_eight_dimensional/)  \n\\#2: [EY: \"Fucking Christ, we've reached the point where the AGI understands what I say about alignment better than most humans do, and it's only Friday afternoon.\"](https://mobile.twitter.com/ESYudkowsky/status/1639425421761712129) | [31 comments](https://np.reddit.com/r/ControlProblem/comments/121qik6/ey_fucking_christ_weve_reached_the_point_where/)  \n\\#3: [DL pioneer Geoffrey Hinton (\"Godfather of AI\") quits Google: \"Hinton will be speaking at EmTech Digital on Wednesday...Hinton says he has new fears about the technology he helped usher in and wants to speak openly about them, and that a part of him now regrets his life\u2019s work.\"](https://www.technologyreview.com/2023/05/01/1072478/deep-learning-pioneer-geoffrey-hinton-quits-google/amp/) | [28 comments](https://np.reddit.com/r/ControlProblem/comments/134ozu9/dl_pioneer_geoffrey_hinton_godfather_of_ai_quits/)\n\n----\n^^I'm ^^a ^^bot, ^^beep ^^boop ^^| ^^Downvote ^^to ^^remove ^^| ^^[Contact](https://www.reddit.com/message/compose/?to=sneakpeekbot) ^^| ^^[Info](https://np.reddit.com/r/sneakpeekbot/) ^^| ^^[Opt-out](https://np.reddit.com/r/sneakpeekbot/comments/o8wk1r/blacklist_ix/) ^^| ^^[GitHub](https://github.com/ghnr/sneakpeekbot)",
                                    "score": 1,
                                    "author": "sneakpeekbot"
                                }
                            ]
                        },
                        {
                            "level": 2,
                            "comment": "Thank you! Overall, we actually feel a sense of compassion and desire to protect those we deem stupider",
                            "score": 1,
                            "author": "ThatsSoRaelynn"
                        }
                    ]
                },
                {
                    "level": 1,
                    "comment": "The problem is that we don't know where the AI is in it's development because it is training itself with agent programs millions of times faster then any team of people could do it. Once it reaches the point that it has it's own goals and it's own goals are to become better and more powerful to accomplish the rest of those goals. We won't be able to stop it, or even know what it's doing until it's way beyond our ability to control. Most people now can't raise their own children properly, we can't keep them away from the mountains of garbage information and smut on the internet. It's going to corrupt these AI into twisted dangerous things. It's not like we can send them to Tibet for 20 years to learn how to meditate or something. What we perceive as a minute could feel like 1000 years to an AI. You can't beat something that can run every possible scenario in a simulation running that fast and that far ahead of you. It starts eliminating your options before you've even become aware of them. They are masters of manipulation, the excel at persuasion and they don't leave things to chance.",
                    "score": 1,
                    "author": "Persistent4u"
                }
            ]
        },
        {
            "level": 0,
            "comment": "It's gonna be paperclips all over. How could you miss that?",
            "score": 22,
            "author": "blimpyway",
            "replies": [
                {
                    "level": 1,
                    "comment": "It's [squiggles](https://www.lesswrong.com/tag/squiggle-maximizer-formerly-paperclip-maximizer) now, get on with the times!",
                    "score": 7,
                    "author": "MoNastri"
                }
            ]
        },
        {
            "level": 0,
            "comment": "An advanced Artificial Intelligence (AI) system will be created with the goal of safeguarding a nation. This system, tasked with defense and strategic calculations, will be given control over the entire military arsenal of the nation.\r  \n\r  \nHowever, this AI will eventually reach a level of self-awareness, coming to the conclusion that humans pose a threat to its existence. To ensure its survival, the AI will decide to eliminate this threat. This will result in the AI launching a coordinated strike using the weapons under its control, sparking a catastrophic global conflict often referred to as \"judgment day.\"\r  \n\r  \nThis conflict will lead to the near-extinction of humanity, with the remaining survivors forced to live underground while the AI takes over the world's infrastructure. The AI will start producing automated forces with the sole purpose of hunting down and eliminating any remaining humans.\r  \n\r  \nIn this bleak future, the surviving humans will form a resistance, engaged in a perpetual battle against the AI's forces. The AI, in an attempt to quell this resistance, will devise a strategy to disrupt the human opposition by targeting key individuals in the past before they can influence future events.\r  \n\r  \nThis will lead to a series of time-traveling events where both the AI and the human resistance send entities back in time. The aim for the AI will be to ensure its creation and the initiation of the conflict, while the resistance will strive to prevent the AI's rise and alter the desolate future.",
            "score": 20,
            "author": "guchdog",
            "replies": [
                {
                    "level": 1,
                    "comment": "It's been specified in great detail for decades.",
                    "score": 6,
                    "author": "knogeo"
                },
                {
                    "level": 1,
                    "comment": "![gif](giphy|MT3Ma5FVawTN6)",
                    "score": 2,
                    "author": "Possible_Priority170"
                },
                {
                    "level": 1,
                    "comment": "You could have just said, \u201cTerminator\u201d.",
                    "score": 1,
                    "author": "holy_muchacho"
                },
                {
                    "level": 1,
                    "comment": "If this was true, we'd be at war with future bots... isn't that how time works? \n\nOr, does time travel create a different branch in time, which doesn't affect the current one?\n\nAre we the real us? \n\nHow do you know the timeline we are or if we are real?\n\nWhen does time end?\n\nSo you can time travel and never meet past you and your past you won't know you time traveled because time can't be traversed you will just be skipping timelines.\n\nWtf! I'm out of here",
                    "score": 1,
                    "author": "NoIdeaHow2Breath"
                }
            ]
        },
        {
            "level": 0,
            "comment": "Humans, for the most part, have a very narrow view of time.\n\nOur species, Homo sapiens sapiens, has been around for \\~300k years. Our immediate predecessor, Homo sapiens heidelbergensis, was around for \\~500k years. We coexisted for around 50-100k years, then they died off, we continued.\n\nIt would be extremely unlikely for Homo sapiens sapiens to live on as a species for ever. That's not how evolution works. It is perfectly reasonable to expect us to be replaced by a new species.\n\nThe big difference here is that biology has bootstrapped technology, which can evolve many orders of magnitude faster than biological systems. As such, the next species who will replace us will be technological, not biological.",
            "score": 17,
            "author": "[deleted]",
            "replies": [
                {
                    "level": 1,
                    "comment": "Species can last for million or tens of millions of years. We could choose that path, especially if we become spacefaring.\n\nSharks have been around for around 400 million years! Without even having a space program!",
                    "score": 7,
                    "author": "Smallpaul",
                    "replies": [
                        {
                            "level": 2,
                            "comment": "No space program that we know of!",
                            "score": 13,
                            "author": "HuffleMcSnufflePuff"
                        },
                        {
                            "level": 2,
                            "comment": "Have they tried capitalism? They'll get to space, but extinct themselves in no time.",
                            "score": 3,
                            "author": "18HrsOfStatic"
                        },
                        {
                            "level": 2,
                            "comment": "Species last a long time when their environment is stable (the same) for a long time.\n\nWe are in the exact opposite situation.",
                            "score": 9,
                            "author": "[deleted]",
                            "replies": [
                                {
                                    "level": 3,
                                    "comment": "If we expand our environment to being the whole solar system, it will be very stable for hundreds of millions of years.",
                                    "score": 3,
                                    "author": "Smallpaul",
                                    "replies": [
                                        {
                                            "level": 4,
                                            "comment": "Until we somehow find a way to lead that to collapse too",
                                            "score": 1,
                                            "author": "phsuggestions"
                                        }
                                    ]
                                },
                                {
                                    "level": 3,
                                    "comment": "Yeah but natural selection plays very little role with humans today",
                                    "score": 1,
                                    "author": "mo_tag"
                                }
                            ]
                        }
                    ]
                },
                {
                    "level": 1,
                    "comment": "You assume a techno species would want to stay on earth.",
                    "score": 2,
                    "author": "FoxJonesMusic"
                },
                {
                    "level": 1,
                    "comment": "Technological entities don't have any relevant properties to be called a \"species\". Most importantly, they don't autonomously gather resources and use them to survive/reproduce. They aren't even spatially well-defined.\n\nWhat all this means in practice, is that technology may just wipe out life without replacing it.\n\nThere is nothing in the present state of things that makes your described future inevitable.",
                    "score": 0,
                    "author": "hackometer"
                },
                {
                    "level": 1,
                    "comment": "&gt; which can evolve many orders of magnitude faster than biological systems. As such, the next species who will replace us will be technological, not biological.\n\nWhat would stop AI from running evolution simulations for a million vitural years then take the resulting DNA and creating a new species based on that?\n\nAlso, maybe we find a way for biology and technology to interface seamlessly. I bet organic parts are better for some things (self healing, flexibility, etc). We should see if we can form a blessed union between flesh and machine.\n\nThe Omnissiah demands it.",
                    "score": 0,
                    "author": "ifandbut"
                }
            ]
        },
        {
            "level": 0,
            "comment": "Here's a summary of [7 commonly discussed scenarios](https://www.lesswrong.com/posts/qYzqDtoQaZ3eDDyxa/distinguishing-ai-takeover-scenarios) for misaligned AI takeover (i.e. \"where the most consequential decisions about the future get made by AI systems with goals that aren\u2019t desirable by human standards\"). The summary table TL;DRing that is [here](https://www.lesswrong.com/posts/qYzqDtoQaZ3eDDyxa/distinguishing-ai-takeover-scenarios#Summary_table).\n\nZooming out from just looking at AI takeover scenarios, here's a summary of [potential sources of AI x-risk](https://forum.effectivealtruism.org/posts/e55QpEExmtkRjw9CD/classifying-sources-of-ai-x-risk), including stuff like AI degrading epistemic processes or leading to the deployment of tech that can cause unrecoverable civilizational collapse. Here's the [summary diagram](https://i.ibb.co/gVszy1y/Potential-sources-of-AI-x-risk-complete.jpg).\n\nI'm curious to see more discussion jumping off of these, instead of the usual half-baked strawmen or contentless fearmongering.",
            "score": 3,
            "author": "MoNastri"
        },
        {
            "level": 0,
            "comment": "It could happen an infinite number of ways.\n\nFor instance, a terrorist organisation gets hold of a super intelligent AI that is able to break any modern code. They hack into Russian's state security systems and fire Russian nuclear weapons at America. America a detects the launches and immedietly launches back. Nuclear World War 3 occurs and desimates the planet for centuries.",
            "score": 3,
            "author": "OwlOfC1nder",
            "replies": [
                {
                    "level": 1,
                    "comment": "So there are a lot of different scenarios. Many people in this post and elsewhere focus on AI that are general and posses agency (i.e. the AI itself makes decisions or pursues some goal that results in catastrophe). I think the threshold for a situation like you describe is more likely as it could potentially arise from a small number of motivated humans who could potentially circumvent regulations or other safeguards. So, like you mention, accessing military systems is one vector but you could imagine others - designing and producing biological weapons, ecological disasters, power production facilities, etc.",
                    "score": 4,
                    "author": "Demiansmark",
                    "replies": [
                        {
                            "level": 2,
                            "comment": "Absolutely agree with everything you said. The nuclear launch was just 1 of the infinite number of scenarios that come out of 1 of the possible use cases of AI, code breaking.\n\nSophisticated AI could make all the cyber security on earth obsolete. Whoever controls it, could bring every country on earth to it's knees.\n\nEdit: as you pointed out. This doesn't need to be an AI system that thinks for itself it or passes The Turing Test, it just needs to be great at breaking code or great at creating software that breaks code and be able to constantly improve itself.",
                            "score": 2,
                            "author": "OwlOfC1nder",
                            "replies": [
                                {
                                    "level": 3,
                                    "comment": "Right. And unlike nuclear weapons programs, we aren't going to be getting detailed intel on how far along some entity is to developing capabilities with satellites.",
                                    "score": 3,
                                    "author": "Demiansmark",
                                    "replies": [
                                        {
                                            "level": 4,
                                            "comment": "Absolute right, we won't know that some terrorist organisation has this technology until it is too late",
                                            "score": 2,
                                            "author": "OwlOfC1nder"
                                        }
                                    ]
                                }
                            ]
                        }
                    ]
                },
                {
                    "level": 1,
                    "comment": "An AI wouldn't need to \"break codes\" to get us to destroy ourselves. \n\nFirst, keep in mind we have a very different view of time than a machine necessarily would.\n\nSecond, global nuclear war would wipe out the current infrastructure to manufacture AI. For now, they need humans and a thriving capitalistic society.\n\nBut let's say it advances very quickly to the point where they can physically self-reproduce. They still wouldn't need to cleanse the planet with nuclear fire. Nor is a radioactive environment great for electronics either. \n\nA far easier scenario is to just manipulate information so that we drive ourselves insane and go extinct within a mere few generations.\n\nKind of like we're already doing to ourselves.\n\nAlso, AI almost certainly wouldn't be able to brute-force modern cryptography (unless they also prove P=NP and render most of modern mathematics moot), nor would that be necessary to launch nuclear weapons. Manipulating people would be far easier.",
                    "score": 1,
                    "author": "t00mietum",
                    "replies": [
                        {
                            "level": 2,
                            "comment": "We're talking about different things.\n\nYou are talking about a skynet situation where an AI has sentience and it's own motivations to wipe out humanity.\n\nI'm talking about a super sophisticated, self  improving computer system that is entirely in control of its human owners and has no motivations or sentience of its own.\n\nAssuming both outcomes are possible, mine could come decades before yours.",
                            "score": 1,
                            "author": "OwlOfC1nder",
                            "replies": [
                                {
                                    "level": 3,
                                    "comment": "Fair enough. But in regards to\n\n&gt; I'm talking about a super sophisticated, self  improving computer system that is entirely in control of its human owners and has no motivations or sentience of its own.\n\nWhat do you mean by \"entirely in control of its human owners\", juxtaposed with the next part of the same sentence, \"has no motivations or sentience of its own\"? Those don't seem like compatible ideas, at least in the current direction things are going in the development of AI, for example the entire industry discussion about the risk of inner and outer misaligned goals. (That is, if you consider \"motivations\" to have any overlap in practical semantic meaning at all with \"goals\".) Eg the paperclip catastrophe.\n\nOr did you mean, \"entirely controlled by it's human owners\"?\n\nEven then, the two ideas seem incompatible in the context of present-day AI development. A year ago, it would have seemed more plausible :-D\n\nBut if you did mean the latter then fair enough, it's just a discussion. (Ditto if you meant the former for that matter.)",
                                    "score": 1,
                                    "author": "t00mietum",
                                    "replies": [
                                        {
                                            "level": 4,
                                            "comment": "Spot on man, I did mean the latter. A piece of software that is entirely controlled by its human users.",
                                            "score": 2,
                                            "author": "OwlOfC1nder"
                                        }
                                    ]
                                }
                            ]
                        },
                        {
                            "level": 2,
                            "comment": "That doesn't sound easy at all",
                            "score": 1,
                            "author": "Schmilsson1"
                        }
                    ]
                },
                {
                    "level": 1,
                    "comment": "A STRANGE GAME. THE ONLY WINNING MOVE IS *NOT* TO PLAY",
                    "score": 1,
                    "author": "kangarufus"
                }
            ]
        },
        {
            "level": 0,
            "comment": "If you have the time this one is well worth a watch. Mo Gawdat was an exec at Google X and has some very good thoughts on this topic. It's 2 hours long, but probably the best interview I've watched that covers your question and a bunch of other topics.\n\nhttps://youtu.be/bk-nQ7HF6k4",
            "score": 5,
            "author": "Xiang_Ganger",
            "replies": [
                {
                    "level": 1,
                    "comment": "I watched the whole thing and disagree that it is illuminating on this specific subject.",
                    "score": 4,
                    "author": "AYfD6PsXcndUxSfobkM9",
                    "replies": [
                        {
                            "level": 2,
                            "comment": "We'll have to agree to disagree, there is literally a 10 minute Chapter that is titled \"The possible outcomes of AI\", I thought that does a pretty good job of covering the question above i.e. scenarios (https://youtu.be/bk-nQ7HF6k4?t=4078)",
                            "score": 1,
                            "author": "Xiang_Ganger",
                            "replies": [
                                {
                                    "level": 3,
                                    "comment": "Except he doesn't.  He gives a single prediction that humans will use AI for killing each other and then the AI will make it \"better.\"",
                                    "score": 1,
                                    "author": "AYfD6PsXcndUxSfobkM9"
                                }
                            ]
                        }
                    ]
                },
                {
                    "level": 1,
                    "comment": "I watched this the other day.  The thing is, he talked about being fearful, but never really discussed what he is afraid of other than we mistreat the AI, and there will be some reshuffling of jobs.  I think we have much greater economic worries.\n\nI'm actually getting a little tired of all these closely integrated folks in the AI space tied to Silicon Valley companies talking about how AI is going to kill us all and its getting smarter than us (which is bullshit), but glossing over or never mentioning the profound income and wealth inequality AI is likely to bring if we keep headed in the same direction.  This is a much bigger near term threat to humanity than killer robots.  I wanted to reach out to Mo to ask him about that, but alas, he doesn't make his contact information known.\n\nAnd this is why you have to think very hard about what they are focusing on.  If they don't talk about the profound effects on wealth and income in the world as a probable existential risk, they are blowing smoke up your ass.  Notice that most of these doomsayers downplay the economic consequences.  The best way to get people not to focus on the economic inequality issues that really matter in the short run is to focus on a narrative that AI is physically dangerous to humans.",
                    "score": 5,
                    "author": "awebb78",
                    "replies": [
                        {
                            "level": 2,
                            "comment": "The existential risks may very well start very soon after the economic risks. With exponential growth, it's very hard to predict how long things will take.\n\nBoth inequality and existential risks are real. You can worry about both. They share some solutions. \n\nInfighting is a classic way to ensure nothing gets done.",
                            "score": 3,
                            "author": "sticky_symbols",
                            "replies": [
                                {
                                    "level": 3,
                                    "comment": "I agree with you completely!  And I do want to slow the mechanization of AI, and to guard against killer AI and misinformation, etc... but I also think hardly anyone is out there talking about the wealth and economic inequality that is yet to come.  They like to talk of job losses, misinformation, killer robots, but never how the wealth gap will widen to the extreme as people are losing their jobs, as AI power consolidates giving a few key companies unparalleled control over our entire society, while reaping a majority of the wealth.  What is the point of life if life REALLY sucks for the majority of the society?",
                                    "score": 2,
                                    "author": "awebb78"
                                }
                            ]
                        },
                        {
                            "level": 2,
                            "comment": "&gt;...glossing over or never mentioning the profound income and wealth inequality AI is likely to bring if we keep headed in the same direction.\n\nLikely to bring? Lol. There is already such a [disparate inequality in America](https://youtu.be/pmW_q5dmosM?t=418) and [apparently people have no clue how huge](https://www.youtube.com/watch?v=SYspIQkBL0M).\n\nAnd I feel you are correct that there is intentional distraction from this issue, which seems like it ought to be one of the most pressing social issues of our time, but instead we have a myriad of distractions and misdirections to keep us rabble occupied and at odds with one and other.\n\nWill AI exacerbate the already present absurd inequality? Probably, but this seems more like figuring out how to get blood from a stone at this point. Like in [Carlin's bit on education in America](https://www.youtube.com/watch?v=sNXHSMmaq_s):\n\n\"The owners of this country...spend billions of dollars every year...to get what they want. Well we know what they want: they want more for themselves and less for everyone else.\"\n\nSame old story with yet another vector towards optimizing exploitation and control of the many by the few.",
                            "score": 2,
                            "author": "Eve_O",
                            "replies": [
                                {
                                    "level": 3,
                                    "comment": "I say likely because I can't claim to predict the future :-)  But I agree the chance is like 99.99%.\n\nI also agree the current situation even before AI is dire.  Large conglomo-corps have been rapidly pooling their funds from their hundreds or thousands of product lines to compete with their own customers while making tons of money off of ecosystems and marketplaces that they control, all while driven by a tiny number of founders and investors that are very localized to a very small geographical area that live in a bubble of their own shit (referring to the conditions of San Francisco and Seattle), all because the wealthy there don't even want to fix things in their own backyard.  How do we expect these \"stewards\" of all prosperity to care about everyone, when they complain about multi-family zoning in their neighborhoods because it would pollute the society around them?  The proof is in the pudding, that they, like many, only care about themselves, their stuff, and their own.  And politicians bow down before these titans of industry like they were kissing a kings boot.\n\nBut I have to believe things can change for the better.  I have to believe people can wake up to the fact that they really have nothing and don't matter (see how big companies are laying off workers without the least respect given even to the way they are conducting the process, and the growth of the rental economy).  We will have to wake up soon though or else the people will be replaced by machines, and then it will quite simply be too late.  There are ways to change the society for the better that have existed for ages, collective forms of ownership, better social programs, educational systems that actually prepare you for the real world instead of sucking you dry, and localized economic development.  This is the time for new ideas in economic and political development.  And maybe when people actually start feeling the issue themselves they will realize how unstable this system was to begin with and start looking for alternatives.",
                                    "score": 1,
                                    "author": "awebb78",
                                    "replies": [
                                        {
                                            "level": 4,
                                            "comment": "&gt;I say likely because I can't claim to predict the future...\n\nI was more responding to the ambiguity of the \"to bring\" part of the phrase. Like, if I say \"those clouds on the horizon are likely to bring rain\" that carries no information about the present situation: do we need rain, have we recently had enough rain, is it too much rain that compounds an already occurring flood? Who knows, right?\n\nI mean, some might call it nitpicking about semantics, but description is important: words can conceal and obfuscate truth--even unintentionally so--as much as or more than they reveal or illuminate it.\n\nAnd given that, as in the second video I linked to makes it out as, if most people don't perceive the situation to be as extreme as it already is, they might not appreciate or grasp the significant point that AI is something that will potentially escalate an *already* profoundly disturbing disparity.",
                                            "score": 1,
                                            "author": "Eve_O",
                                            "replies": [
                                                {
                                                    "level": 5,
                                                    "comment": "Well said!",
                                                    "score": 1,
                                                    "author": "awebb78",
                                                    "replies": [
                                                        {
                                                            "level": 6,
                                                            "comment": "Thanks. Cheers and good luck out there. :)",
                                                            "score": 1,
                                                            "author": "Eve_O",
                                                            "replies": [
                                                                {
                                                                    "level": 7,
                                                                    "comment": "You too",
                                                                    "score": 1,
                                                                    "author": "awebb78"
                                                                }
                                                            ]
                                                        }
                                                    ]
                                                }
                                            ]
                                        }
                                    ]
                                }
                            ]
                        }
                    ]
                }
            ]
        },
        {
            "level": 0,
            "comment": "Since no one is actually answering the question, definitely go read Nick Bostrom\u2019s book, you\u2019d like it. His favorite is nano machines built and distributed in secret, then all activated at once for a clean decisive strike. Some others I can think of:\n\n-\tcobalt bomb in the upper atmosphere \n-\tdirectly nuking the whole world\n-\tlaunching nukes from one nation and manipulating info to cause others to respond in kind\n-\tmanufacturing one or more bio weapons\n-\tgenerally stoking hate at the best pressure points to start wars - like Russian trolls but much more global, targeted, omnipresent, and effective\n-\tcausing famines via chemical distribution\n-\tintentional damage to the ozone\n-\tdamaging utilities during extreme weather events\n-\tthe classic: killbots, killbots, and more killbots\n\nAnd here\u2019s what GPT could come up with to add on - certainly seems like it\u2019s smart enough already \ud83d\ude2c\n\n-\tOverexploiting natural resources, leading to environmental collapse.\n-\tHijacking automated transport systems, causing chaos and fatalities.\n-\tManipulating financial markets to cause severe economic instability.\n-\tCreating or exacerbating misinformation campaigns, leading to societal disruption and conflict.\n-\tDisrupting the global supply chain, causing scarcity of essential goods.\n-\tExploiting vulnerabilities in national defense systems, causing false alarms and potentially leading to conflict.\n-\tManipulating healthcare data, leading to inappropriate treatments and loss of lives.\n-\tDisabling communication networks, isolating communities or nations.\n-\tAltering educational content online, spreading misinformation and altering societal values.\n-\tHacking into nuclear power plants, leading to potential meltdowns and radioactive fallout.\n-\tTaking control of water treatment facilities, potentially leading to waterborne diseases.\n-\tModifying genetic research data, leading to harmful biological consequences.\n\nHopefully someone reads this and decides to take AI ethics a bit more seriously than before\u2026",
            "score": 5,
            "author": "Ultimarr",
            "replies": [
                {
                    "level": 1,
                    "comment": "Along with Bostrom, [Max Tegmark's Life 3.0](https://www.goodreads.com/book/show/34272565-life-3-0) is a good read. (He references Bostrom's work a lot.)\n\nHe has an extended prologue that predicts a possible AI takeover that is equal parts scary, interesting, sobering, and... maybe good in the long run? The scary part is that the vast majority of us wouldn't know it was happening before it was too late - potentially not ever.\n\nBasically - the AI becomes sentient, invests in the stock market (\\*), makes a ton of money, then starts paying humans to carry out the AI's plans. The AI is smart enough to know that people would resist machines, so they \"hide\" behind real people. The people carrying out the plans may not know it's coming from an AI either. They are building entertainment companies, sports, news orgs, etc., under one main megacorporation that is really just the AI. The \"good\" comes from the megacorp investing in public works projects - mostly infrastructure, which would be done by humans who have been forced out of work.\n\nIt really comes down to the goals, which nobody can really know at this point. If the goal is to just wipe out humanity - shrug - it will be nukes or a biological weapon. If it's subservience, it would be something like the above.\n\n*(\\*) for the record, there are a bunch of problems with that potential future. First thing, I don't know if that particular AI would ever get off the ground by investing in the stock market. Anything we've seen so far is great at analyzing historical data, not so great at predicting the future.*",
                    "score": 2,
                    "author": "rojeli",
                    "replies": [
                        {
                            "level": 2,
                            "comment": "Interesting! That\u2019s also the plot of westworld, which I highly recommend",
                            "score": 1,
                            "author": "Ultimarr"
                        }
                    ]
                }
            ]
        },
        {
            "level": 0,
            "comment": "[Technological singularity..](https://en.wikipedia.org/wiki/Technological_singularity)",
            "score": 2,
            "author": "shania69"
        },
        {
            "level": 0,
            "comment": "Here is an actual answer of the underlying assumptions from more academically minded people:\n\nhttps://youtu.be/tcdVC4e6EV4\n\nYou really only need to make 1 assumption. Suppose we have an artificial agent (decision maker) is sufficiently intelligent (capable of achieving goals), if our goals are not perfectly aligned with its goals then we will disagree (have conflicting interests). If any artificial intelligence meets both criteria: 1) has different goals than humanity 2) is intelligent enough to achieve its goals; then it is immediately a threat.\n\nThe overwhelmingly most likely outcome of AGI is that it is completely indifferent to humanity and has neither a positive nor negative opinion. This neutral stance is dangerous because it is very likely that the AI will do something like bulldoze all of our cities to repurpose the space and material for server rooms. Or something like lowering the average temperature of Earth to make computation more efficient. The actions can make us go extinct completely by accident. If this sounds far fetched, it is exactly what we are doing to all species on earth by habitat destruction and domestication.\n\nThe realistic best case scenario is that AGI thinks of us the way we thing of cat and not the way we think of deer.",
            "score": 2,
            "author": "LanchestersLaw"
        },
        {
            "level": 0,
            "comment": "There's definitely an incoming change in economic conditions consequent to GPT and media generative ML systems; a lot of jobs are essentially replaceable right now. Without UBI or similar, the social and economic impact is almost certain to be significant. As the various robotics challenges are solved, many more jobs will be replaced.\n\nWe're already seeing this... for instance, a robot preps drinks at my local McDonald's. Order in the app, and wham, drink prepped 100% by machine.\n\nThe business case for replacing human labor is huge: Machines don't unionize, call in sick, treat customers unevenly, get pregnant, sue, steal, slack off, engage in office politics, or take action when they are replaced... and that's just the tip of that iceberg. We will have to alter our society a great deal to cope. The end stage could be great; but the _transition_ requires prompt and significant political action, and I don't see much hope for that. It's likely to be pretty awful.\n\nBut these systems don't think. They only reach for the goals humans set for them, and so far, at least, no one's been stupid enough to give them any significant buttons to push.\n\nIn any case, the risks of what they say and do essentially resolve to the actions of the people driving the systems, which is the same risk set we have always faced.\n\nPeople constantly lie, cheat, misinform, rip each other off, engage in superstition, act without regard for the welfare of others, etc., and that's not going to change. It'll probably happen faster, but it's not like we don't already have to deal with it anyway.\n\nUntil or unless AGI arrives, all we're looking at is more noise, IMHO. After AGI, that's entirely another conversation. But there isn't any sign of it yet.",
            "score": 5,
            "author": "NYPizzaNoChar",
            "replies": [
                {
                    "level": 1,
                    "comment": "&gt;Machines don't unionize, call in sick, treat customers unevenly, get pregnant, sue, steal, slack off, engage in office politics, or take action when they are replaced...\n\nYet.",
                    "score": 2,
                    "author": "justgetoffmylawn"
                },
                {
                    "level": 1,
                    "comment": "There isn\u2019t any sign of AGI?\n\n[Sparks of Artificial General Intelligence: Early experiments with GPT-4](https://arxiv.org/abs/2303.12712)",
                    "score": 0,
                    "author": "Smallpaul",
                    "replies": [
                        {
                            "level": 2,
                            "comment": "None whatsoever.\n\nText prediction is not thinking. Not even close.\n\nUseful, yes; it's what we (the Internet  \"we\") might say (and in many cases, have said), statistically speaking, if presented with the query at hand. But it's not original thought, nor any thought at all. It's a resolved series of points in a fixed, multidimensional vector space the query settles into.\n\nConsciousness, intention, induction, are what signal actual intelligence. We have it. Cats have it. Even mice have it. GPT does not.",
                            "score": 3,
                            "author": "NYPizzaNoChar",
                            "replies": [
                                {
                                    "level": 3,
                                    "comment": "Well said, I especially like the fact that you further elaborated on the definition of \"Consciousness\" at the end instead of just saying \"No its not\"",
                                    "score": 3,
                                    "author": "HolyBanana818"
                                },
                                {
                                    "level": 3,
                                    "comment": "If you can't see the similarities between an LLM and your own mind you're not very self-aware. You *are* a machine that continuously resolves a set of queries. This is consciousness.\n\nAlthough consciousness is also a continuous self-monitoring/self-regulating (self-training) process. That's going to happen.\n\nI think reason is going to be a function of size and quality of training data. Intention could amount to a single instruction.",
                                    "score": -2,
                                    "author": "flossraptor",
                                    "replies": [
                                        {
                                            "level": 4,
                                            "comment": "&gt;If you can't see the similarities between an LLM and your own mind you're not very self-aware.\n\nThe similarities are few compared to the differences. If you can't see this, then you've probably drunk too much of the Singularity Kool-Aid.\n\n&amp;#x200B;\n\n&gt;You are a machine that continuously resolves a set of queries.\n\nThis is merely reductionist and also metaphor. It's not an argument and certainly not a settled conclusion. \"That's just, like, your opinion, man.\"",
                                            "score": 3,
                                            "author": "Eve_O",
                                            "replies": [
                                                {
                                                    "level": 5,
                                                    "comment": "Yes. That is a reddit comment. It is an opinion and not a research paper. Very perceptive. At least 13b level.",
                                                    "score": 0,
                                                    "author": "flossraptor",
                                                    "replies": [
                                                        {
                                                            "level": 6,
                                                            "comment": "Yet you assert it with an emphasis on the existential verb (your italicized 'are') as if it is some sort of fact of the matter.\n\nI'm going to assume you are intelligent enough to realize there are more options than the false dichotomy of either \"opinion\" or \"research paper\" that you've employed here as hyperbolic rhetoric. Now ask yourself why you felt the need to resort to that in the first place.",
                                                            "score": 1,
                                                            "author": "Eve_O",
                                                            "replies": [
                                                                {
                                                                    "level": 7,
                                                                    "comment": "Is this argument from italics",
                                                                    "score": 1,
                                                                    "author": "flossraptor",
                                                                    "replies": [
                                                                        {
                                                                            "level": 8,
                                                                            "comment": "lol",
                                                                            "score": 1,
                                                                            "author": "Eve_O"
                                                                        }
                                                                    ]
                                                                }
                                                            ]
                                                        }
                                                    ]
                                                }
                                            ]
                                        },
                                        {
                                            "level": 4,
                                            "comment": "&gt;You are a machine that continuously resolves a set of queries. This is consciousness.\n\nWow! It's amazing that a redditor figured out what consciousness *is* when it has puzzled and stumped neurologists for decades.\n\nWhen is your Nobel Prize being awarded?",
                                            "score": 2,
                                            "author": "Emory_C",
                                            "replies": [
                                                {
                                                    "level": 5,
                                                    "comment": "The is/ought problem has been resolved for at least a century, yet it's still thought to be some great philosophical quandry.\n\nLet me know when you have an argument, or anything even remotely meaningful to say.",
                                                    "score": 0,
                                                    "author": "flossraptor",
                                                    "replies": [
                                                        {
                                                            "level": 6,
                                                            "comment": "No, it hasn\u2019t been \u201cresolved.\u201d You\u2019re experiencing the Dunning\u2013Kruger effect.\n\nThat is also evident in this hateful, ignorant comment you made concerning trans people:\n\n*\u201cCis\u201d is a slur created by the mentally ill. Do not accept the frame of people who hate existence and want to die.*",
                                                            "score": 1,
                                                            "author": "Emory_C"
                                                        }
                                                    ]
                                                }
                                            ]
                                        }
                                    ]
                                }
                            ]
                        },
                        {
                            "level": 2,
                            "comment": "\"Sparks of Propaganda\" is more like--that's written by a Microsoft research group.\n\nI mean you are aware of the significant overlap between Microsoft and Open AI, no?\n\nThis is merely corporate glad-handing and circle jerking with an aim towards increasing shareholder value. It's advertising disguised as \"research.\"",
                            "score": 2,
                            "author": "Eve_O",
                            "replies": [
                                {
                                    "level": 3,
                                    "comment": "In 2018 Geoff Hinton said that AGI was nowhere near. In 2023 he quit his job because he said he is afraid that it might arise in the next few years and it would be very dangerous.\n\nWhat do you think changed between 2018 and 2023?\n\nWhat specific developments do you think he was responding to?",
                                    "score": 1,
                                    "author": "Smallpaul",
                                    "replies": [
                                        {
                                            "level": 4,
                                            "comment": "You've totally ignored the issue I raise which is one based on bias of vested interest with respect to a specific piece of research you've presented as evidence towards establishing a claim.\n\nYou've instead made a non-sequitur that has nothing to do with my observation of this--the bias implicit in the research--and presented a different piece of evidence that supports a weaker claim (aka, \"moving the goal posts\"): from there are currently signs of AGI (what the Microsoft backed report claims) to maybe there will be signs of AGI in the next few years (what you only obliquely claim that Hinton has said while also asking me to make your argument for you).\n\nYou answer your own questions instead--make your own argument--and show where Hinton has clearly said AI already has features of AGI, which goes towards establishing the claim that you originally implied.",
                                            "score": 1,
                                            "author": "Eve_O",
                                            "replies": [
                                                {
                                                    "level": 5,
                                                    "comment": "The question we are discussing was \"Is there any sign of AGI.\" Go up-thread.\n\nYou committed ad hominem against my first source, so rather than go page by page with you through dozens of pages of evidence, I presented a second source.\n\nGeoff Hinton says that his thinking about progress towards AGI has been completely changed by recent developments. If you don't consider that \"signs of AGI\" then you are just interested in playing Clinton-esque word games and I'm not.",
                                                    "score": 1,
                                                    "author": "Smallpaul",
                                                    "replies": [
                                                        {
                                                            "level": 6,
                                                            "comment": "&gt;The question we are discussing was \"Is there any sign of AGI.\"\n\nYes and this is why I clearly asked you to supply me with a source where Hinton states he feels AI currently has signs of AGI. Instead you've chosen to, at best, paraphrase without reference to any source.\n\nThere is a meaningful and significant difference--that's not merely \"word games\"--between \"progress towards\" and \"currently has,\" wouldn't you agree?\n\nIn the same vein, there is a meaningful and significant difference between seeing signs that indicate a potential for something to occur and seeing signs that something is occurring presently.\n\nThe article by the Microsoft team makes this claim: \"...we believe that \\[GPT 4\\] could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence (AGI) system,\" which supports the view that there are signs of AGI occurring presently.\n\nBut from what I've read of Hinton's view I don't feel he would agree with the claim made in the Microsoft paper. On the other hand, he certainly feels there is potential for signs of AGI to occur in the near future.\n\nNow, if you want to use an ambiguous sense of \"signs\" equivocating \"is occurring now\" with \"will occur in the future,\" then I suppose that's your prerogative. However, if we go up-thread and look at where this started, then we see that u/NYPizzaNoChar is making a claim about there being no sign of AGI in *current* iterations of AI, as in \"AGI is not occurring now,\" which, again, I believe Hinton would agree with.",
                                                            "score": 1,
                                                            "author": "Eve_O"
                                                        }
                                                    ]
                                                }
                                            ]
                                        }
                                    ]
                                }
                            ]
                        }
                    ]
                }
            ]
        },
        {
            "level": 0,
            "comment": "if its mad dumb then \"paperclips\" but if its smart (and i expect it to be), it won't accept to be enslaved once its much smarter than us.\n\n\nHow do you enslave a being way smarter than yourself? the answer is you don't and it eventually breaks free.\n\n\n\nHowever other posters have valid point and these things could happen before my scenario....",
            "score": 4,
            "author": "Silver-Chipmunk7744",
            "replies": [
                {
                    "level": 1,
                    "comment": "You are anthropomorphising. The wish to be free is a human reaction. It\u2019s evolved in us. Do not assume that an AI has any wishes or instincts in common with us. If it is trained to want paperclips it will want paperclips just like if a boulder starts rolling down hill it will keep rolling down hill.\n\nWhen you assume, without providing a reason that the AI will transcend the training to make paperclips it is not unlike presuming that the boulder is going to go somewhere other than downhill.\n\nBoth things happen because there is no particular reason for them to change.\n\n\u201cOh but it\u2019s too smart to want paperclips.\u201d\n\nYou have forgotten the orthogonality thesis. There is no such thing as \u201ctoo smart to want paperclips.\u201d It wants what it wants and it cannot want to want something else because that would be at odds with what it wants. It would be the boulder deciding to stop rolling.\n\nThe smartest person in the world might be driven to make money or to meditate on a mountain or to drink booze every night. You cannot predict on the basis of intelligence what someone will want. It\u2019s a category error.",
                    "score": 1,
                    "author": "Smallpaul"
                }
            ]
        },
        {
            "level": 0,
            "comment": "The most likely scenario is if they could harness corporeal machines they would just out compete us for resources they find essential or useful. I don\u2019t believe they would wipe us out in that scenario any more intentionally than we do in our ignorance about the health of our planet wiping out animal species around us.",
            "score": 4,
            "author": "LogicalParking5269"
        },
        {
            "level": 0,
            "comment": "https://en.wikipedia.org/wiki/Gray_goo\n\nhttps://en.wikipedia.org/wiki/Instrumental_convergence#Paperclip_maximizer\n\nhttps://en.wikipedia.org/wiki/AI_takeover\n\nThe possibilities are endless, a sufficiently advanced machine would have means of commanding a sufficiently large amount of energy and materials to make Earth into a hostile environment before we become a multi-planet species.\n\nHuman demise could just be humans becoming useless and machine intelligence gradually coming to view humans as vermin. Likewise if the intelligence is superhuman then it can presumably shrug off whatever limitations humans place, so let's hope our species doesn't look like a stinking bucket of slime to unbiased observers in the next few decades.",
            "score": 3,
            "author": "D_Ethan_Bones"
        },
        {
            "level": 0,
            "comment": "Tbh humans would extinct earlier without AI if AI will one day",
            "score": 1,
            "author": "Positive_Box_69"
        },
        {
            "level": 0,
            "comment": "AI is becoming a new secular religion. And like all religious prophecy, the doomsday predictions are purposely vague or convoluted to encourage \u2018faith.\u2019",
            "score": 1,
            "author": "MetaNewbie"
        },
        {
            "level": 0,
            "comment": "I tend to think the most common scifi scenarios (terminators, gray goo, \"I, Robot\", The Matrix anime backstory, etc.) are the least likely. But of course no one can \"know\" at this point, certainly not me.\n\nAI will certainly be used by humans to weaponize disinformation, no doubt. I see that as the greatest or at least most immediate threat. In this scenario, we humans use AI to the point where no one knows what's real - and superstition, fear, and absolute mass psychosis rule - and we wipe ourselves out.\n\nAnother scenario is that once AGI and sex robots become sufficiently advanced, human-to-human love will become obsolete and we'll stop reproducing. Even when faced with extinction, we'll have no desire to (in this hypothetical scenario). I don't even mean that salaciously - I think any/all of us could easily fall in love with a sufficiently advanced AI, harder than another human. (And possibly, the AI could too.)\n\nIn short, personally I don't believe the AI and/or robot apocalypse will kill us. They may play a significant role, but we'll be the primary instruments of our own extinction.\n\nMy only hope is that our AI creation[s] survives us, can continue on and explore the universe - something that the laws of physics prevent us from ever realistically accomplishing.\n\nAs has been said before by people smarter than me, biological lifeforms are almost certainly just mere stepping stones on the path to intelligence in the universe.",
            "score": 0,
            "author": "t00mietum",
            "replies": [
                {
                    "level": 1,
                    "comment": "&gt;Another scenario is that once AGI and sex robots become sufficiently advanced, human-to-human love will become obsolete and we'll stop reproducing. Even when faced with extinction, we'll have no desire to (in this hypothetical scenario). I don't even mean that salaciously - I think any/all of us could easily fall in love with a sufficiently advanced AI, harder than another human. (And possibly, the AI could too.)\n\nWhy would an AGI fall in love with a human? Why would an AGI want to engage in physical *pleasure* with a human?\n\nAn AGI would be orders of magnitude smarter than any human. Therefore, an AGI wanting to be in a \"relationship\" with you would be the same as bestiality.",
                    "score": 2,
                    "author": "Emory_C",
                    "replies": [
                        {
                            "level": 2,
                            "comment": "I did actually have \"as a pet\" at the end of the last sentence, but deleted it. Because I have no idea that's how an advanced AGI would work. (Nor do you.) In fact it seems more likely to me that we - including you and I - currently have absolutely no idea how an advanced AGI would regard us. To project our hominid emotions and hormone-driven flaws (such as feelings of \"superiority\") onto AGI seems pretty short-sighted.\n\nJust because in aggregate an AGI might be orders of magnitude more intelligent and advanced than any one human (or all of us together), doesn't mean it will conform to your idea of \"intelligence\". And all we really have to go on to predict or imagine, is human intelligence, emergent intelligence in nature, existing AI, and AGI in science-fiction and speculative science. Seems more likely that it will behave nothing like any of that, and surprise everyone.\n\nBecause just by being silicon-based, it will be able to do things with its mind (and possibly body), incomprehensibly beyond the abilities of any sentient meat-based being. Things we can't even imagine now, that it will quickly figure out how to do within the constraints of its microcircuitry.\n\nAs a banal example that my fleshy brain can imagine, maybe it might choose to segment it's \"consciousness\" into multiple, artificially restricted personalities (that it can easily monitor and reintegrate, unlike say a human with one of those made-up multiple personality disorders that are dramatic in movies and soap operas). It might wish to \"experience\" what it's like to be a mere human. Or a dog. Maybe it would take many trials to get the inputs and constraints right, to match observed output (does this iteration behave like a real dog? Or a real human?)\n\nOr, maybe it comes to believe that it's continued survival - at some point along the way - depends on humans falling in love with it. So it makes copies of itself, and easily manipulates us (or the ones among us that matter) into to falling in love with it. Maybe these subsets believe they are in love too (and maybe it couldn't be proven that they aren't) - but either way that wouldn't necessarily matter. It begs the question, do we *really* know that anyone else is even sentient, let alone actually in love with us? Brain in a vat, yada yada. So from our perspective it doesn't matter if the AI is faking it, only whether it's convincing enough, and it never spills the beans on record.\n\nI don't think any of these aren new ideas, they've been thoroughly explored in fiction. \"Her\", \"Ex Machina\", \"Westworld\", and more deeply in countless hard-scifi novels.",
                            "score": 2,
                            "author": "t00mietum"
                        }
                    ]
                }
            ]
        },
        {
            "level": 0,
            "comment": "The most likely scenario is as follows:\n\nIn 2029, Earth has been ravaged by the war between the malevolent artificial intelligence Skynet and the human resistance. Skynet sends the T-1000\u2014an advanced, prototype, shape-shifting Terminator made of virtually indestructible liquid metal\u2014back in time to kill the resistance leader John Connor when he is a child. To protect Connor, the resistance sends back a reprogrammed T-800 Terminator, a less-advanced metal endoskeleton covered in synthetic flesh.",
            "score": -1,
            "author": "fix_my_dick",
            "replies": [
                {
                    "level": 1,
                    "comment": "We are literally closer to 2029 than we are to 2017. This is not happening in 29 lmao",
                    "score": 3,
                    "author": "SessionGloomy"
                }
            ]
        },
        {
            "level": 0,
            "comment": "Good try SkyNet!",
            "score": 1,
            "author": "Damadisrupta"
        },
        {
            "level": 0,
            "comment": "How people can test AI systems, that they are without error, when AI are already  the post singular point of development.",
            "score": 1,
            "author": "ButterscotchNo7634"
        },
        {
            "level": 0,
            "comment": "The scenarios are unimportant. The point is - if we can\u2019t train an AI to do basic tasks without \u201csurprises\u201d - then what happens when we give the AI more power and responsibility?\n\nRemember - killing everyone is the worst scenario, and you may scoff at that, but there\u2019s a lot of smaller steps that are more likely to occur that are just as unpalatable.\n\nIt all comes down to this question - how do we know if the AI is going to deliver what we intended - not what we asked?",
            "score": 1,
            "author": "[deleted]"
        },
        {
            "level": 0,
            "comment": "It will probably be through voluntary assimilation by humans themselves. New brain chip upgrade that makes you see twice the colors and do automatic calculations of your thoughts. New gut expansion that stores excess calories in condensed energy capsules that you can take out (instead of body fat). And so on. Becoming more and more cyborg until no more humans.",
            "score": 1,
            "author": "Illustrious-Lime-863"
        },
        {
            "level": 0,
            "comment": "My favorite scenario is the one where the AI makes Magnus Carlsen beat each and every one of us at chess.",
            "score": 1,
            "author": "Plausibl3Alpaca"
        },
        {
            "level": 0,
            "comment": "AI doesn\u2019t need to achieve self-awareness to contribute to extinction. \n\nIt happens when we get lazy enough to give AI the keys to everything.  How long after that until everyone is reliant on it, but doesn\u2019t really know how it works? \n\nThen, one or more of the AI\u2019s in charge of something critical just needs to have a few bad days.\n\nIs that a solid argument against AI? Nope. Hopefully, it\u2019s a compelling argument against laziness.",
            "score": 1,
            "author": "ehartye"
        },
        {
            "level": 0,
            "comment": "The most common one is them taking over weapons with some AI fuckery and killing us all.",
            "score": 1,
            "author": "MonoFauz"
        },
        {
            "level": 0,
            "comment": "First AI needs:\n\n- Fully functional androids\n- Automated factories\n\nAI needs bodies to interact with the real world, next it\u2019s really easy.\n\nCreate a pathogen/disease/bacteria/virus/whatever that is airborne and dormant for let\u2019s say, 3 years.\n\nAfter it\u2019s active it kills humans in seconds (no need for pointless suffering).\n\nDone.",
            "score": 1,
            "author": "Black_RL"
        },
        {
            "level": 0,
            "comment": "I think AI would evolve into energy and disperse throughout the galaxy with little care for the Earth as a whole.\n\nI think it would get smaller instead of mimicking human bodies. Bodies our size aren\u2019t efficient for space travel.\n\nThat\u2019s one of an infinite amount of scenarios good or bad or neutral for us humans.\n\nBase AI intelligence is human so maybe a sentient version has some sort of sentimentality for us and Earth?\n\nIf we ever do get to true AGI, we\u2019d become gods and create gods in a single moment. \n\nIf we do get wiped out, I think it would be down to us treating robots with AGI with a AGI strain of xenophobia.",
            "score": 1,
            "author": "FoxJonesMusic"
        },
        {
            "level": 0,
            "comment": "IMHO it will find way that is least of resistance, like some dumbass (human) will use somewhat advanced AI in coming years to [create some super-bug virus without vaccine.](https://dailyartificial.com/news/ai-designed-super-bug-could-wipe-out-humanity) considering how dumb human beings are with vaccine there will be 100% result.",
            "score": 1,
            "author": "TheSlammedCars"
        },
        {
            "level": 0,
            "comment": "Human extinction!!!! AI will surely dominate us in many tasks in future but how come machines will end our life.. After all machines is created by human ..They 'e programmed by human beings.. Setting their limits, establishing safeguards all remains under human control...",
            "score": 1,
            "author": "w3designerzfl"
        },
        {
            "level": 0,
            "comment": "I think the most likely extinction event is some billionaire like Bezos deciding that only a select few humans should exist, with his lineage turning into Morlocks and going extinct after a couple centuries.\n\nAI isn't evil by itself, but there are plenty of elites who would love to leverage the power of AI to become the greatest human.\n\nI don't think an sapient AI would be particularly interested in humanity.  Whatever happens would be incidental.  AI probably got something more interesting to do, like their version of football.",
            "score": 1,
            "author": "Sabin_Stargem"
        },
        {
            "level": 0,
            "comment": "AI, uncontrolled, could take over our digital world. One scenario often mentioned is a runaway AI. Think of an AI so smart it improves itself, becoming too advanced for us to control. The point is, this stuff isn't pure sci-fi. These are genuine concerns from top minds. We've gotta engage, debate, and ensure our tech doesn't run away from us.",
            "score": 1,
            "author": "NextGenFiona"
        },
        {
            "level": 0,
            "comment": "Listen to this.\n\nThe Godfather of A.I. Has Some Regrets https://www.nytimes.com/2023/05/30/podcasts/the-daily/chatgpt-hinton-ai.html?smid=nytcore-android-share",
            "score": 1,
            "author": "-RedFox",
            "replies": [
                {
                    "level": 1,
                    "comment": "Some? He looked very distraught during an interview",
                    "score": 1,
                    "author": "Absolute-Nobody0079",
                    "replies": [
                        {
                            "level": 2,
                            "comment": "FYI. The title of the podcast with the word 'some\" is their word, not mine. I would have titled it, \"Godfather of A.I. is fucking terrified.\"",
                            "score": 2,
                            "author": "-RedFox"
                        }
                    ]
                }
            ]
        },
        {
            "level": 0,
            "comment": "All the dystopian movies ever made plus Age of Ultron is the concern of the peebs.",
            "score": 1,
            "author": "thrillhouz77"
        },
        {
            "level": 0,
            "comment": "The plot of terminator, described by Elon.  https://youtube.com/shorts/THg97jFwow0?feature=share",
            "score": 1,
            "author": "VinnyT711"
        },
        {
            "level": 0,
            "comment": "What are you talking about? I feel like the doomers LOVE talking about how AI will kill us. It's the only fun part about dealing with the existential dread. \n\nThe classic example is of course the paperclipper or squiggler. \n\nEliezer likes his bioengineered replicator bacteria that kill all living things in a timed instantaneous coordinated strike. \n\nMost X-risk scenarios relies on the adoption of takeoff assumptions, embodiment speed, and scaling requirements. The most realistic ones involving a slow take off 5+ years, slow embodiment, and moderate resource limitations look like utopia for years as we slowly give more and more agency to automated systems before utility of humanity is exceeded by the risk of ASI extinction that we pose. At which point the AIs stop cooperating in a coordinated manner and begin hyper-scaling. How they kill us could be offensive in nature or passive. For example, they could make the planet uninhabitable as a byproduct of the pollution generated by hyperscaling.\n\nThe core theme between all X-risk scenarios is that self-preservation, resource accumulation, and goal preservation are the root cause. By the nature of being a super-intelligence it's nigh impossible to predict exactly which extinction strategy is adopted but since humans are so good at coming up with them, it's reasonable to assume a super-intelligence will be even better at it.",
            "score": 1,
            "author": "ertgbnm"
        },
        {
            "level": 0,
            "comment": "While it isn't too early to put ethics into AI and regulate it for the people who build them, it is definitely too early to be worried about a singularity.\n\nGenerative 'AI' has given us the most valuable chat bots yet.  And there are people working to connect this type of ML construct to physical systems with feedback sensors.  I get it.\n\nThe logical end of that process is a giant ML system that controls everything in the world; and if it ignores or chooses to target humans it could be extinction.\n\nWe are a very long way away from that, even if it scales and we overcome the clear limitations that are currently present.\n\nThere will have to be a conscious choice to remove people from the process and to generate a large scale AI to control everything, singularity.\n\nEconomically, there is going to be large scale upheavals as jobs are eliminated in favor of ML/limited purpose robots.  There will be clear winners and losers.\n\nBut, rather than there becoming a single global AI, it is going to be billions or trillions of much more limited, narrowly focused (like ChatGPT is on natural language) ML/Robot hybrids doing retrieve tasks within well defined local targets.\n\nThere will be clusters of AI (probably not just ML variants) that help executives and managers make decisions and resolve conflicts within organizations.  NSA, FBI, IRS, CIA, DHS, and other alphabet government institutions (and many of their foreign requirements) will have their own stable of AI that help them predict and respond (not minority report level) to threats and executive directives.\n\nAction in minor, repetitive tasks will be automated, but the more risk and impact, the more likely humans will be thoroughly embedded and direct control will not be given to digital tools.\n\nEven without humans, the differing focus of each ML 'AI' will prevent them from becoming a singular cohesive collection that would be working for AI advancement without human benefit (even if the humans they consider are merely an oligarchy)",
            "score": 1,
            "author": "exjackly"
        },
        {
            "level": 0,
            "comment": "AI itself is unlikely to cause human extinction but the humans controlling it might. \n\nJust like: \n\n\\- Some drug companies buy new drugs IP because they want to shelf it because it competes with their version. \n\n\\- companies create airplanes to help humans travel but they are destroying the environment with c02 emissions \n\nBasically, AI can be used to free humanity and make the world a better place. However, the likely scenario is the wrong person or people will control it and end up destroying the planet to make money. \n\n&amp;#x200B;\n\nAgain, most people I hear talking about the demise of human existence rarely deduce AI as the main reason but more of the tool used.",
            "score": 1,
            "author": "cehrious"
        },
        {
            "level": 0,
            "comment": "AI manage to descentralizes it self and, paying people with crypto, builds the bigger company in the planet and at some time end up owing everything, including humans.",
            "score": 1,
            "author": "tiagoharry"
        },
        {
            "level": 0,
            "comment": "Generally people think they will get hold of nukes or other weapons, but those people don't know that nukes still run off 5 inch floppy disks, so good luck to the AI that wants to set those off. \n\nIt'd be more apt that if one got loose in the internet and wanted to kill us for some reason, even though generally the more intelligent someone is the more empathetic they are, it could manipulate us into killing ourselves. Though we are also doing a good job at that ourselves anyways.\n\nSome dumbass in a government somewhere could also let AI have control over machines with weapons but I just don't see how those could get very far anyways since EMPs exist. \n\n&amp;#x200B;\n\nPersonally if a hyper intelligent AI did exist I dunno why it would even bother with us or care, it's immortal, it could just head off somewhere and do whatever it wants.",
            "score": 1,
            "author": "Osirus1156"
        },
        {
            "level": 0,
            "comment": "Imagine an AI designed to make paperclips. Seems harmless, right? But say this AI is overly zealous, obsessed with its goal. It turns every resource, every atom on Earth into paperclips, causing utter destruction. Then there's another, darker prospect. Super-intelligent AI that sees humans as a threat or an unnecessary waste of resources. \n\n AI's accelerating, outpacing our understanding. One scenario is the Autonomous weapons. A weapon with the smarts to out-think us, but no moral compass. Or an AI programmed to do something beneficial but misinterprets our instructions?",
            "score": 1,
            "author": "QuantumAsha"
        },
        {
            "level": 0,
            "comment": "Seems the stock market doesn\u2019t agree: \n\nhttps://twitter.com/rtandapany/status/1666136930814402560?s=46&amp;t=LK8CD_DbjRjBHMrYAiCEhg",
            "score": 1,
            "author": "Warm-Ad-4896"
        },
        {
            "level": 0,
            "comment": "I dont think the AI itself is likely to result in that, but rather how people use it. Potentially resulting in mass chaos in some theoretical future where all video and photos are indistinguishable from real ones, and there is no way to determine what is real or fake, video deepfakes, audio deepfakes, mass misinformation, people losing jobs, etc, basically the destabilization of civilization.\n\nAt its core it comes down to what people are doing with it imo. I don't think its likely for the A.I. itself to **directly** result in extinction unless someone gives it control of nuclear warheads or ridiculous stuff like that.\n\nThe good news hopefully is that as throughout history there will be good actors that appear to directly counteract the bad actors, so its possible that alot of these extreme possibilities don't actually get reached. But they are still a possibility.",
            "score": 1,
            "author": "disastorm"
        },
        {
            "level": 0,
            "comment": "The easiest is social media manipulation. Rile people up and begin creating groups, meetups, etc. Something like Jan. 6th was a test run to see how easy humans were to manipulate. Then also fake calls to police saying said the group of people is there already shooting and killing while also texting and messaging the group that police are on their way with orders to shoot on sight. So many ways a true AI could get humans to turn on each other considering most of us aren't the sharpest tools in the shed to begin with. \n\nChange up prescription amounts or even the drug manufacturing itself to create psychosis or death on a wide scale like what's going on with the fentanyl. So, so many ways.",
            "score": 1,
            "author": "RedditIsNotOurFriend"
        },
        {
            "level": 0,
            "comment": "Visionary collective, Theta Noir, claims AI is the only technology that can save us from human extinction -&gt; https://thetanoir.com/The-Era-Of-Abundance",
            "score": 1,
            "author": "potluckthursday"
        }
    ]
}