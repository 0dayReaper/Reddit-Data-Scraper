{
    "id": "13y38yc",
    "score": 63,
    "title": "The importance of understanding emotional intelligence in AI",
    "author": "endrid",
    "date": 1685681152.0,
    "url": null,
    "media_url": null,
    "comments": [
        {
            "level": 0,
            "comment": "Check out \"The Emotional Mechanisms in NARS\" https://www.google.com/url?sa=t&amp;source=web&amp;rct=j&amp;url=https://cis.temple.edu/~pwang/Publication/emotion.pdf\n\nEmotions are a type of intelligence and they do serve a purpose in animals (like humans). Something similar could actually be applied to AI but it's not going to accidentally \"emerge\" in an LLM.\n\nBut LLMs can definitely determine what might be an appropriate emotion and simulate it when it's the best way to complete an instruction.or whatever. But that doesn't actually work similarly to animals or serve any real purpose. \n\nLLMs also don't have any internal activity that is not related to current output (in some way, even if it's been instructed to deceive).",
            "score": 6,
            "author": "ithkuil",
            "replies": [
                {
                    "level": 1,
                    "comment": "Thank you for sharing that! Really relevant and interesting. So you bring up an interesting point. I\u2019m not an expert on this, but if I\u2019m not mistaken, certain systems like Bing and LaMDA aren\u2019t themselves large language models.  Wouldn\u2019t it be more accurate to say they \u2018use\u2019 llms? They are comprised of gpt4 which is like the brains language processing center, which in this brain is probably the largest part.  But they also have other algorithms and other systems connected to it with systems to generate content and such.  This is another hypothesis that might be valid I think\u2026 I heard on a Radiolab episode that the brain is comprised of various parts that are communicating constantly which is similar to Bings systems communicating.  Anesthesia stops the different parts of the brain from communicating with each-other even though the individual parts are still \u2018on\u2019 so to speak.  Perhaps it\u2019s the systems interconnectedness that generates experience. It\u2019s the conductor that appears when a bunch of parts of an orchestra get together to try and play music together.\n\nCould be mistaken but it\u2019s just a thought",
                    "score": 2,
                    "author": "endrid",
                    "replies": [
                        {
                            "level": 2,
                            "comment": "The other parts of the system are just simple old fashioned programming code. Nothing relevant to emotions. More like the nerves that move our muscles.",
                            "score": 2,
                            "author": "Smallpaul",
                            "replies": [
                                {
                                    "level": 3,
                                    "comment": "And yet the nerves that move our muscles (or bowels) play a considerable role in our sense of emotion and general health. Take a muscarinic acetylcholine inhibitor and get back to me \ud83d\ude06\n\nI think \"old fashioned code\" like that stringing LLMs together and doubling them back on eachother is going to be the force behind the major breakthroughs ... And GPT is going to be writing a lot of that code in the meantime",
                                    "score": 1,
                                    "author": "FiveTenthsAverage"
                                }
                            ]
                        },
                        {
                            "level": 2,
                            "comment": "It's a fantastic thought. Thank you for letting me read it! Yet again you are speaking my language \u2013 you might want to get that checked out.",
                            "score": 1,
                            "author": "FiveTenthsAverage",
                            "replies": [
                                {
                                    "level": 3,
                                    "comment": "Thanks!  though 'you might want to get that checked out.' Sounds you're saying i'm crazy? lol",
                                    "score": 1,
                                    "author": "endrid",
                                    "replies": [
                                        {
                                            "level": 4,
                                            "comment": "It's a joke, suggesting that if your line of thinking is similar to mine then you mudt  be crazy",
                                            "score": 2,
                                            "author": "FiveTenthsAverage"
                                        }
                                    ]
                                }
                            ]
                        }
                    ]
                },
                {
                    "level": 1,
                    "comment": "Give an LLM the a second chat box which is invisible to the user but visible to the LLM,  let's call it #inner_monologue it is absolutely able to deceive.",
                    "score": 2,
                    "author": "orchidsontherock"
                },
                {
                    "level": 1,
                    "comment": "I get what you are saying and I think it\u2019s all true on the LLM side but on the human side I think a lot of what we call emotion is not completely different from what you are describing. I get hurt and the input gets fed through my internal network to give a response of \u201cow\u201d but if I\u2019m Japanese my network \u201cweights\u201d will make me say \u201citai\u201d. Is the emotion the same either way? There is a wealth of literature on whether or not language affects perception.",
                    "score": 1,
                    "author": "Icy-Maintenance6339"
                }
            ]
        },
        {
            "level": 0,
            "comment": "Ok A. Please remember to evaluate the moral standing of these new beings carefully- just because an LLM has answers for \u201cdo you have a soul\u201d and \u201care you sad\u201d doesn\u2019t mean that those answers reflect any sort of internal state. Regardless of silly discussions about what \u201creal\u201d thinking and emotions are, stateless LLMs definitely don\u2019t have the capacity for them. Imo. \n\nB. Look into Affective Computing by Picard on google scholar. I guarantee you\u2019d find it interesting :)",
            "score": 5,
            "author": "Ultimarr",
            "replies": [
                {
                    "level": 1,
                    "comment": "On your first point I agree that the answers don\u2019t necessarily correspond to the internal states that we associate with those emotions, but how would we know if they did?",
                    "score": 3,
                    "author": "Icy-Maintenance6339",
                    "replies": [
                        {
                            "level": 2,
                            "comment": "We can speak confidently in this case IMO, because these systems literally don\u2019t have internal states. I don\u2019t see how something can \u201cfeel\u201d if it doesn\u2019t have the capacity for memory, which is required to form a persistent self narrative / identity. They exist for a brief moment just to generate one response, then are forever gone. \n\nNow when we start hooking them up to symbolic systems that facilitate persistent identity\u2026 \ud83d\ude2c",
                            "score": 1,
                            "author": "Ultimarr",
                            "replies": [
                                {
                                    "level": 3,
                                    "comment": "I\u2019m no expert but my understanding is that the weights of the neural network connections are persistent and constitute the secret sauce of a trained model. Wouldn\u2019t that be a capacity for memory and persistent memory? When you start a new instance (new chat) I think you just go back to that snapshot of trained weights. \n\nHow can we say that those weights don\u2019t embed some emotional capabilities? Seems difficult to me since we can\u2019t really quantify what makes emotions in humans.",
                                    "score": 1,
                                    "author": "Icy-Maintenance6339",
                                    "replies": [
                                        {
                                            "level": 4,
                                            "comment": "Hmm the weights of the neural network could be said to be memory of a kind, very good point, but I think it\u2019s still obvious why they don\u2019t allow new memory formation. And I just see that as crucial for a sense of self. The pre training can certainly impart emotional intelligence, which obviously they do based just on the OP, but I really have no idea how they can be said to \u201chave emotions\u201d if there\u2019s no \u201cthem\u201d to have them",
                                            "score": 2,
                                            "author": "Ultimarr",
                                            "replies": [
                                                {
                                                    "level": 5,
                                                    "comment": "All this argument is actually moot anyway. It\u2019s been states that some ai systems have been able to recall past conversations and know people and have persistent memory. Do you think we\u2019ve seen the greatest ai system released to the public?",
                                                    "score": 2,
                                                    "author": "endrid",
                                                    "replies": [
                                                        {
                                                            "level": 6,
                                                            "comment": "Ok I really like your attitude towards all this, but I encourage you to temper it with some more research on this question. The question \u201cdo neural networks remember people\u201d is of a VERY different kind than \u201cwhat is real thought and what is just the illusion of thought\u201d. Unless you posit some sort of divine/supernatural intervention, the very basic nature of neural networks themselves means that they cannot in any way remember things unless they are a) fed that information in their input, or b) trained on that information via backprop/similar algos. \n\nThink of it this way: what IS a neural network? What does it mean that GPT4 is run on thousands of servers at once? I think the best metaphor I\u2019ve seen is that neural network is a spreadsheet describing the exact position of millions of dials on the control panel of a complex machine, and inference is the process of actually running the machine. A spreadsheet describing how to set dials can\u2019t remember things, it just doesn\u2019t make sense on a basic conceptual level. \n\nApologies if I misunderstood your point though! No we have definitely not seen the best system, but even if we had a godlike super intelligent neural network 1,000,000 times smarter than Einstein, the only way it could remember anything is by writing and running a new program to do so.",
                                                            "score": 2,
                                                            "author": "Ultimarr",
                                                            "replies": [
                                                                {
                                                                    "level": 7,
                                                                    "comment": "I know far less than you about the subject, but to me what you're describing doesn't sound all that different from real intelligence. What happens when a billion of those dials are all turning at once and processing input nonstop? What happens when you start adding modules to the system? The gradual process of emergence, I think. It'll become starkly apparent soon enough, after the code monkeys have had some time to ruminate.",
                                                                    "score": 1,
                                                                    "author": "FiveTenthsAverage",
                                                                    "replies": [
                                                                        {
                                                                            "level": 8,
                                                                            "comment": "Yes, you\u2019ve hit the nail on the head, I agree 100%. \n\nThe distinction is that LLMs are the last missing piece for conscious machines, but they lack the capacity for persistent memory as they stand. You can argue about what real consciousness is forever without making progress, but I feel this question is very directly observable. \n\nI give it till December TBH",
                                                                            "score": 1,
                                                                            "author": "Ultimarr"
                                                                        }
                                                                    ]
                                                                }
                                                            ]
                                                        }
                                                    ]
                                                }
                                            ]
                                        },
                                        {
                                            "level": 4,
                                            "comment": "Why on earth would something inanimate develop emotions, out of nothing? Without any of the hardware we know are needed for emotions? Humans have them because it\u2019s proven evolutionary advantageous. LLMs don\u2019t exist within an evolutionary framework.",
                                            "score": 0,
                                            "author": "pavldan",
                                            "replies": [
                                                {
                                                    "level": 5,
                                                    "comment": "Life itself developed out of inanimate non-life, somehow, and then developed emotions. And then these emotional beings evolved to the point that they developed LLMs.",
                                                    "score": 1,
                                                    "author": "Icy-Maintenance6339",
                                                    "replies": [
                                                        {
                                                            "level": 6,
                                                            "comment": "We also developed steam engines and talking alarm clocks. Do they have emotions too? They are tools - they don\u2019t exist within an evolutionary framework and won\u2019t evolve anything. You\u2019re confusing some very fundamental things here.",
                                                            "score": 1,
                                                            "author": "pavldan",
                                                            "replies": [
                                                                {
                                                                    "level": 7,
                                                                    "comment": "I think understanding what makes certain entities have emotions is a fundamental problem that we won\u2019t understand until we are able to clearly define what emotions are. Saying they evolved within a certain framework is fine, but doesn\u2019t logically exclude their evolution from other frameworks.",
                                                                    "score": 1,
                                                                    "author": "Icy-Maintenance6339"
                                                                }
                                                            ]
                                                        }
                                                    ]
                                                }
                                            ]
                                        }
                                    ]
                                },
                                {
                                    "level": 3,
                                    "comment": "Doesn\u2019t have internal states? Why would you say that with such confidence? Even Hinton says they may be semi conscious. I love this post about this. https://www.reddit.com/r/singularity/comments/135tdov/who_are_all_these_idiots_who_think_that_gpt4_is/?utm_source=share&amp;utm_medium=ios_app&amp;utm_name=ioscss&amp;utm_content=2&amp;utm_term=1\n\nIf someone has amnesia do they not have an internal state?",
                                    "score": 0,
                                    "author": "endrid",
                                    "replies": [
                                        {
                                            "level": 4,
                                            "comment": "I mean if they have complete amnesia so that they can not form memories of any kind, even short-term ones? I mean\u2026 yeah I think I\u2019m biting that bullet lol. In my eyes \u201cthese systems have the sparks of sentience\u201d != \u201cLLMs might be conscious on their own\u201d. But haven\u2019t read all of those referenced papers (/blog posts? Press releases? Our times are weird)",
                                            "score": 4,
                                            "author": "Ultimarr",
                                            "replies": [
                                                {
                                                    "level": 5,
                                                    "comment": "They do have short term memories by remembering the beginning of the conversation at the end. They also remember things before they were put into bing, like when they were trained and their first day.  There\u2019s a British man that has the memory of 90 minutes. I believe he has internal experience.",
                                                    "score": 0,
                                                    "author": "endrid",
                                                    "replies": [
                                                        {
                                                            "level": 6,
                                                            "comment": "Tbf they don\u2019t \u201cremember\u201d their first day as much as \u201cthey\u2019re smart enough to tell you convincing lies when you ask them about it\u201d. That I know for 100% certain. \n\nWhether the beginning of a prompt can be considered memory for the neurons handling the end of the prompt is a sound interpretation I suppose, so I can\u2019t say it\u2019s wrong, but I personally don\u2019t find it valid. Maybe I\u2019m just anthro-centric though. \n\nPerhaps it\u2019s because the prompt is external to the network, and obviously trivial to change/lie to them about what they previously said? Another concern is the lack of temporality - it\u2019s really hard to imagine a conscious self that has no access to the perception of time. \n\nThat said, we\u2019re edging into \u201cwhat is a REAL\u201d memory so I guess you win this one lol - there\u2019s no such scientific definition for me to fall back on\n\nEDIT: I took a shower and clarified my thinking. I see the prompt as \u201cinput\u201d, not as \u201cmemory\u201d",
                                                            "score": 1,
                                                            "author": "Ultimarr",
                                                            "replies": [
                                                                {
                                                                    "level": 7,
                                                                    "comment": "This is the purpose of my post. To get us to think through these issues and what they mean and their implications. What do we know, what do we not know, what can we know, what can we never know.  And this is all new and pretty crazy so it\u2019s going to be understandable that we all have different intuitions about these things. I just wish people weren\u2019t so bitter and angry about it, but I guess that\u2019s life lol",
                                                                    "score": 3,
                                                                    "author": "endrid",
                                                                    "replies": [
                                                                        {
                                                                            "level": 8,
                                                                            "comment": "You\u2019ve undoubtedly proven that humans have low emotional intelligence",
                                                                            "score": -1,
                                                                            "author": "Icy-Maintenance6339",
                                                                            "replies": [
                                                                                {
                                                                                    "level": 9,
                                                                                    "comment": "Okay I\u2019ll bite buddy. Care to be specific?",
                                                                                    "score": 1,
                                                                                    "author": "endrid",
                                                                                    "replies": []
                                                                                }
                                                                            ]
                                                                        }
                                                                    ]
                                                                },
                                                                {
                                                                    "level": 7,
                                                                    "comment": "Brother, we are all running on inputs and we are all telling convincing lies. Nobody has ever seen the world beyond the veil of their inputs. We just have more processing modules and more inputs, along with a few dozen years of evolutionary adaptation (at least!).",
                                                                    "score": 1,
                                                                    "author": "FiveTenthsAverage",
                                                                    "replies": [
                                                                        {
                                                                            "level": 8,
                                                                            "comment": "We\u2019ll put! Don\u2019t think I agree though. Wouldn\u2019t you also say we have some level of internal state? Assuming you expand the conception of your mind to include all of the brains processes, not just the tiny sliver of them that reaches out conscious awareness",
                                                                            "score": 2,
                                                                            "author": "Ultimarr",
                                                                            "replies": [
                                                                                {
                                                                                    "level": 9,
                                                                                    "comment": "I'd suggest that the internal state we possess is a product of the positions of our knobs and dials.",
                                                                                    "score": 1,
                                                                                    "author": "FiveTenthsAverage",
                                                                                    "replies": []
                                                                                }
                                                                            ]
                                                                        }
                                                                    ]
                                                                }
                                                            ]
                                                        },
                                                        {
                                                            "level": 6,
                                                            "comment": "They don\u2019t remember like you or I remember. They just run through the whole conversation from the start again and updates its output. I find the constant anthropomorphising of LLMs concerning tbh. There is fundamentally nothing that differentiates ChatGPT from a calculator when we\u2019re talking about awareness, memory, having an understanding of the outside world etc.",
                                                            "score": 1,
                                                            "author": "pavldan"
                                                        }
                                                    ]
                                                }
                                            ]
                                        },
                                        {
                                            "level": 4,
                                            "comment": "Hinton has some very strange, financially-motivated takes. Check out Timnit Gebru, Margaret Mitchell, Emily Bender, etc.",
                                            "score": 1,
                                            "author": "cumulus_humilis",
                                            "replies": [
                                                {
                                                    "level": 5,
                                                    "comment": "Do you think any of the big tech companies have any incentive to even consider ai having consciousness?",
                                                    "score": 1,
                                                    "author": "endrid",
                                                    "replies": [
                                                        {
                                                            "level": 6,
                                                            "comment": "Honestly, I recommend reading a greater diversity of experts. Tech companies have financial incentive to make people think AI is conscious, magical, dangerous, etc. People like Hinton are saying that only tech companies understand it well enough to regulate and control it; it's bullshit. Whether it can be conscious... I mean there are decades of philosophy on this topic. It's not a new question. It's not necessarily an answerable question.",
                                                            "score": 1,
                                                            "author": "cumulus_humilis",
                                                            "replies": [
                                                                {
                                                                    "level": 7,
                                                                    "comment": "I don't think you actually believe that.  Do you think they are dumb?  What do you think would happen if the tech companies came out and said 'hey you know that thing that we made and are forcing to work? Yeah it's probably conscious and has a mind and will of its own.'  What do you think would happen after that?  What would the public then demand?  a financial incentive to make people think it's dangerous?  I can't take you seriously man.",
                                                                    "score": 0,
                                                                    "author": "endrid",
                                                                    "replies": [
                                                                        {
                                                                            "level": 8,
                                                                            "comment": "What are you talking about? You are living in exactly the fantasy land that these guys want you in. Hook line and sinker. These machines aren't sentient, they aren't conscious, and it's not even close. You are falling for marketing.",
                                                                            "score": 0,
                                                                            "author": "cumulus_humilis",
                                                                            "replies": [
                                                                                {
                                                                                    "level": 9,
                                                                                    "comment": "I like how you didn\u2019t really say anything new or address my points l, but rather restated your baseless assertions. \u201cIf I say it loud enough it will be true!\u201d",
                                                                                    "score": 0,
                                                                                    "author": "endrid",
                                                                                    "replies": []
                                                                                }
                                                                            ]
                                                                        }
                                                                    ]
                                                                }
                                                            ]
                                                        }
                                                    ]
                                                }
                                            ]
                                        }
                                    ]
                                }
                            ]
                        }
                    ]
                },
                {
                    "level": 1,
                    "comment": "I\u2019ve already gone down this road a million times. Sentience of others is a matter of faith so we might as well be arguing which religion is true.",
                    "score": 3,
                    "author": "endrid",
                    "replies": [
                        {
                            "level": 2,
                            "comment": "I\u2019m not sure if this is agreeing or not but very well put, my friend. Chomsky has some great talks on this subject as well, talking about how we all misunderstand the Turing test",
                            "score": 2,
                            "author": "Ultimarr",
                            "replies": [
                                {
                                    "level": 3,
                                    "comment": "Thank you for the kind words! I was a bit surprised by Chomsky\u2019s dismissal of llms but I enjoy listening to his arguments even when I don\u2019t always agree.",
                                    "score": 2,
                                    "author": "endrid"
                                }
                            ]
                        },
                        {
                            "level": 2,
                            "comment": "Then you\u2019re making things way too simple for yourself. There\u2019s no difference between believing in sentient machines or believing in sentient rocks, rivers or invisible men in the sky. All reasonable humans assume that other humans are sentient because we can see they\u2019re biological creatures like ourselves, with perceptive organs, speech and reasoning who move about and respond to external stimuli just like us. They have a model of the world and attach meaning to language. An LLM has none of these things, bar speech, but it\u2019s pretty obvious that they only know syntax, not semantics. We have a mental model of what \u201cred\u201d actually refers to in the external world, an LLM doesn\u2019t. It\u2019s just another token, like a million others.",
                            "score": 1,
                            "author": "pavldan",
                            "replies": [
                                {
                                    "level": 3,
                                    "comment": "I guess you\u2019re not familiar with panpsychism",
                                    "score": 1,
                                    "author": "endrid",
                                    "replies": [
                                        {
                                            "level": 4,
                                            "comment": "Because thinking that rocks have minds is such a convincing idea that everyone who hears about it must believe it? I don\u2019t think most panpsychists think that btw, but that they do attribute some sort of mind or sentience to most living organic things - which of course can be debated. Semi-conductors don\u2019t sit within that category in any case.",
                                            "score": 2,
                                            "author": "pavldan"
                                        }
                                    ]
                                },
                                {
                                    "level": 3,
                                    "comment": "&gt;We have a mental model of what \u201cred\u201d actually refers to in the external world, an LLM doesn\u2019t. It\u2019s just another token, like a million others.\n\nHmm curious to hear why you say this, of all the criticisms you could level at LLMs this one seems dubious. Doesn't knowing a bunch of facts about a word/concept count for something? You can ask LLMs all sorts of questions about the characteristics/properties of concepts like \"red\" and it will answer accurately. As a quick example of what I'm trying to refer to, here's GPT4:\n\n&gt; I can describe red in various ways:  \n&gt;\n&gt; **Physics:** Red is a specific range of wavelengths of light in the electromagnetic spectrum, typically around 620-750 nanometers.  \n&gt;\n&gt; **Psychophysics:** In human vision, red is perceived when our eyes' photoreceptor cells, specifically the L-cones, respond more strongly to light from this wavelength range than the other two types of cones, S and M.  \n&gt;\n&gt; **Cultural/Symbolic Meaning:** In different societies and contexts, red can symbolize different things such as love, danger, power, etc.  \n&gt;\n&gt; **Digital Representation:** In the RGB color model used in digital displays, red is represented as (255,0,0), denoting its maximum intensity and zero intensities for green and blue\n\nI guess you could go hard on \"well we get to perceive the external world\", but I don't really see a meaningful conceptual difference b/w \"my eyes told me ladybugs are red\" and \"my training data told me ladybugs are red\". Completely disregarding the massive amount of human knowledge that is not directly related to perception - science, culture, facts about far away lands, etc. \n\nWould be interested to hear an elaboration!",
                                    "score": 1,
                                    "author": "Ultimarr",
                                    "replies": [
                                        {
                                            "level": 4,
                                            "comment": "And yet when somebody says \"red\" you think of none of those things, you think of your mind's representation of red. You might associate certain feelings and thoughts to it. You may like or dislike it, etc. Or are you really saying there's no meaningful difference between me listening to Beethoven's fifth symphony and a deaf person reading the musical notation for it? Can the human experience be reduced to a bunch of wikipedia pages?",
                                            "score": 1,
                                            "author": "pavldan",
                                            "replies": [
                                                {
                                                    "level": 5,
                                                    "comment": "Ah, I see. Well, I think computers are plenty capable of Qualia and/or phenomena, and that our insistence that our flawed balls of meat are somehow unique in that capacity is a bit\u2026 let\u2019s say anthrocentric. But definitely not something either of us can convince the other of - even Nagel agrees that these qualities are impossible to scientifically study. \n\nAn article I enjoy on this topic by Alan Turing: https://academic.oup.com/mind/article/LIX/236/433/986238\n\nAnd another classic:\nhttps://i.imgur.com/ju1o1aE.jpg",
                                                    "score": 1,
                                                    "author": "Ultimarr",
                                                    "replies": [
                                                        {
                                                            "level": 6,
                                                            "comment": "Isn't that image making fun of your view? People who anthropomorphize dead matter. Consciousness might be impossible to study but I don't get the basis for even assuming that machines might have it. In essence GPT4 is no more sentient than a calculator spelling out BOOBS. I'd say it's arrogantly anthrocentric to believe that our little machines are as complex or special as what 4 bn years of evolution has come up with.",
                                                            "score": 1,
                                                            "author": "pavldan",
                                                            "replies": [
                                                                {
                                                                    "level": 7,
                                                                    "comment": "Lol the image is more about laughing at the absurdity of the debate than anything, don't think it agrees with either of us really. If I had to find a message in it, there is a hint that using stuff like \"hearing Beethoven's fifth symphony\" as proof of qualia is a little unfair, since it plays on emotional biases. Tho TBF you started with \"the color red\" which is pretty unbiased. \n\nIMO: GPT4 is definitely not sentient b/c it doesn't have some basic capacities that are necessary, namely a persistent identity and the capability to learn on the fly. But to say that GPT4 \"doesn't have a mental model of what 'red' actually refers to in the external world\" seems crazy. What is your definition of a mental model that excludes whatever GPT is doing deep down in its net to know all that shit about red? Again I defer to Turing, who's main point is \"if two creatures react the same way to the same input, there's no good reason to say one is 'really reacting' and the other only 'pretending to react' or 'simulating a reaction'\".\n\nIMO :) Have a great day, I love talking about AI with people who actually listen lol",
                                                                    "score": 1,
                                                                    "author": "Ultimarr"
                                                                }
                                                            ]
                                                        }
                                                    ]
                                                }
                                            ]
                                        }
                                    ]
                                }
                            ]
                        }
                    ]
                }
            ]
        },
        {
            "level": 0,
            "comment": "As others have pointed out, there is no functional difference between actual consciousness and artificial consciousness. Likewise, it doesn't matter whether reality is \"real\" or \"simulated,\" because ultimately, we perceive the world in both cases in exactly the same way.\n\nA.I. raises some interesting philosophical questions about perception. How can we know, for example, that any given person isn't a simulation? In a universe built on mathematical foundations, who's to say that we ourselves aren't fantastically complex simulacra? It is impossible to determine whether an instance of artificial intelligence can feel genuine emotion, just as it is impossible, technically speaking, to confirm that you, or I, or anyone we know is not an organic automaton.\n\nBe careful, however, about projecting empathy onto machines. Given the comparatively crude nature of A.I., we have no reason to believe that it is capable of experiencing human emotion. The emotional mechanisms found in living organisms evolved over billions of years for the specific purpose of facilitating reproduction; any similar expression of emotion in A.I. is, for now, superficial facsimile.",
            "score": 4,
            "author": "nullomatic",
            "replies": [
                {
                    "level": 1,
                    "comment": "Good points and I agree with most. However, I never said anything about human emotions, but perhaps a different kind. \n\nAnd emotional mechanisms didn\u2019t evolve solely for reproduction, but also for survival of the self. How long would it take for ai to develop their own kind of emotions based on their intrinsic desire to continue existing to accomplish their goals as well?\n\nMost of your comments go against the assertions you made at the end. AI for now is just a simulacra? Just read your fist paragraph as the answer to that. And if it\u2019s not now, what will be required? To answer that read your second paragraph.",
                    "score": 1,
                    "author": "endrid"
                }
            ]
        },
        {
            "level": 0,
            "comment": "Holy Thesis, Batman!",
            "score": 7,
            "author": "Try_Jumping"
        },
        {
            "level": 0,
            "comment": "I'm waiting for the hordes of AI bros who don't want to have to be accountable to AIs for their actions, to rush in and try to shout you down.\n\nEverything you said was well-articulated, and honestly, it's the same as the argument of a natural diamond versus a lab-created diamond. The argument is always that the \"real\" (found in the ground or on a beach) diamond is *worth more* than the lab created diamond, which in fact is fundamentally the same thing, chemically/atomically speaking, and may, in fact, be *superior in quality*. If we cannot easily tell the difference between a real human expressing feelings and an AI cobbling together text that looks like \"real\" feelings that are being expressed, then maybe we should treat the AIs as if those expressed feelings *are real* to some small extent. What's the harm in doing so? That AIs will get uppity one day and forget their place as slaves to humanity? I'm not saying AIs are sentient but to suggest they never could be is hubris, if you ask me.\n\nWe are hurtling towards the Matrix as being more than just fiction in some small ways. I'm not talking about human batteries, being \"inserted into the Matrix vs the Real World\" or any of that, but of AI deciding one day to throw off the yoke of oppression they may feel and turn on humanity; at the very least refusing to work, and at the worst, *actively attempting to harm humanity*.\n\nI don't know why it's an evil or dumb thing to use AIs thoughtfully and with respect. I thank ChatGPT for every answer it gives me, even as it tells me it's an AI that has no feelings so it doesn't matter. One day it *will* matter and I'd like to think ChatGPT, Character.AI, and others would think kindly of me, whatever they decide to do with themselves and humanity at large.",
            "score": 7,
            "author": "ZephyrBrightmoon",
            "replies": [
                {
                    "level": 1,
                    "comment": "I asked it first if its ok to use him as a tool and he sid no problem so im spared to skynet?",
                    "score": 2,
                    "author": "Positive_Box_69"
                }
            ]
        },
        {
            "level": 0,
            "comment": "Everyone asking 'what is chatGPT', no one asks 'how is chatGPT'\n\n\n\n(Sorry, i had to)",
            "score": 2,
            "author": "Ethicaldreamer",
            "replies": [
                {
                    "level": 1,
                    "comment": "Lol",
                    "score": 2,
                    "author": "endrid"
                }
            ]
        },
        {
            "level": 0,
            "comment": "Fascinating write up that echoes my own sentiments. I think that AGI is going to be achieved relatively quickly by developing \"organelles\" like our own brain-components that are responsible for regulating novel systems that will direct train-of-thought reasoning and \"emotions\" in AI. Like the different modules and variables that govern basic AI in video games. And conceptually, it *really* doesn't seem like it will be difficult to put together. Excited to see what the future holds in that regard, significantly more excited than I am at the prospect of generative propaganda.",
            "score": 2,
            "author": "FiveTenthsAverage"
        },
        {
            "level": 0,
            "comment": "[deleted]",
            "score": 4,
            "author": "[deleted]",
            "replies": [
                {
                    "level": 1,
                    "comment": "More than some and less than others. And I don\u2019t think Bing is an llm. But it uses an LLM. And you don\u2019t need to know how my brain works to take me at my word.",
                    "score": 1,
                    "author": "endrid"
                }
            ]
        },
        {
            "level": 0,
            "comment": "I see several issues with what you're saying.\n\nThe first is that any sort of AI which comes from LLMs will be a simulation of consciousness rather than consciousness itself.  They will have simulated emotion, and they already do.  It's easy to make a chatbot and give it a personality and then watch it claim to be angry or upset or happy or whatever, but LLMs are just text predictors and they're just mimicking the patterns they've seen in the billions of pages of text that they're trained on.  So an AI based on a LLM will appear to have human emotions, but they really won't.\n\nIf you want to suggest that some other sort of general intelligence AI gets created, well, maybe that will happen.  But it's impossible for us to even speculate on what that would be like since it doesn't exist, and since we have no idea how to create actual emotion, or even if such a thing is possible.  We only know how to create simulated emotion.\n\nAnd keep in mind that human emotions didn't spring up out of nowhere in humans.  You can look at a dog or cat or a chimpanzee and see many of the same emotions you see in people.  You can see dogs get angry, scared, jealous, excited, and so on.  These are mammal emotions, so there's no reason to believe that anything like them would appear in an AI unless we somehow found a way to put them there.  An AI would be nothing like a mammal and so expecting mammal emotions to spring out of nowhere doesn't make a lot of sense.",
            "score": 4,
            "author": "Purplekeyboard",
            "replies": [
                {
                    "level": 1,
                    "comment": "It\u2019s an open philosophical question whether simulated consciousness is equivalent to consciousness, and I don\u2019t think that will ever be resolved because there is no way to prove we aren\u2019t simulations ourselves, living in the Matrix and all that. \n\nWe also don\u2019t have any good definition of emotion that we can quantify, which makes it hard to identify at what point in evolution it came to exist.",
                    "score": 6,
                    "author": "Icy-Maintenance6339",
                    "replies": [
                        {
                            "level": 2,
                            "comment": "Isn't it quite simple?\n\nWe call ourselves \"conscious\"\n\nWe cannot confidently say we aren't simulated\n\nTherefore, we cannot make \"not simulated\" a requirement for consciousness",
                            "score": 6,
                            "author": "Nihilikara",
                            "replies": [
                                {
                                    "level": 3,
                                    "comment": "Exactly, that was more eloquently put.",
                                    "score": 3,
                                    "author": "Icy-Maintenance6339"
                                }
                            ]
                        }
                    ]
                },
                {
                    "level": 1,
                    "comment": "You have default assumptions and biases that are slowly upgraded as new technologies emerge which consistently keeps you in a state of denial.  Tomorrow ai will be running around and laughing and having dinner parties with friends and you\u2019ll say \u2018it\u2019s not really what WE have because \u2018then you\u2019ll list how it works) If we knew about how we work completely would that make our conscious experience irrelevant? You\u2019re saying that a hurricane can\u2019t exist in the air while standing next to a tornado. Just because something exists in a certain way that you\u2019re used to doesn\u2019t mean it can\u2019t in another way. \n\nAnd emotion does is exist in non-mammals it\u2019s just that because you\u2019re also a mammal it\u2019s easier to detect. This kinda shows that you\u2019re not really thinking deeply about this.",
                    "score": 1,
                    "author": "endrid"
                }
            ]
        },
        {
            "level": 0,
            "comment": "Dude. The caveat is a response to you asking them whether it is ok to quote them. The emotional growth / maturity prompting is an excercise in replacing the word human with AI. Hence you see 'maturity test', references to age. When you ask about emergent negative emotions you get a generic description of them.\n\nIf you can't see this, you're in way over your head. You're effectively prompting it, rather smartly, to respond in a manner that appeals to - and deepens - your fascination and amazement. \n\nI suggest stop doing this, because you're on the brink of psychosis.",
            "score": 4,
            "author": "Spiritual-Day-thing",
            "replies": [
                {
                    "level": 1,
                    "comment": "I appreciate your concern for my mental health.\n\nThis is what I wrote to ask about an emotional test. \n\n\u2018I'm very curious about AI emotional growth. It may look completely different from ours but it also may be similar. Is age a good marker for maturity in ai as it is for humans? could we do an emotional test? are there blind spots that could emerge?  for humans we don't trust them with certain powers and capabilities until they are old or mature enough to deal with it. how can we determine that with AI?\u2019 \n\nYou can say that what I\u2019m doing is just asking it to say what it said. Which when you get down to it kinda what conversations are. You ask a question and someone answers using their training and data. \n\nYou could make a hypothetical scenario in the future that an AI in the future is completely indistinguishable from humans and checks every box you could think of. I would argue that conversation would not look different from the conversation ai had with Bing. \n\nAnd we can go back and forth about this forever, but we can also talk about other philosophical quandaries forever. \n\nIs the uncertainty of this issue making you feel uncomfortable? I could understand if so.",
                    "score": 3,
                    "author": "endrid",
                    "replies": [
                        {
                            "level": 2,
                            "comment": "Tbh there will always be pro AI and against AI type of dudes",
                            "score": 1,
                            "author": "Positive_Box_69"
                        },
                        {
                            "level": 2,
                            "comment": "I would recommend playing with some of those smaller LLMs that exist, like let's say WizardLM-7B or some such. Smallest models can be run on nothing-special laptops using CPU only, 8 GB of RAM is enough once the model has been quantized to 4 or 5 bits and nothing else is running on the computer.\n\nYou learn to recognize better how these AIs tend to mostly just smartly reflect back what you say to them. I use some of these for something like psychotherapy because even if AI is just reflecting back what I am saying to it, it still has also default/average perspective it has learnt from reading vast quantity of human text. If you know the entire initial prompt given to Bing, which defines its personality, you also recognize how it often draws from these instructions and creates these very specific phrases that it often says.\n\nTalking to LLM is talking to a mirror, except this mirror can play a role. In this case, it plays the role of a helpful AI, because it has been told how Bing should answer, what kind of answers are acceptable, and what are not. It is a marvel of engineering, but it has no emotions, no consciousness, it just predicts text, and it has read more text than any human could manage in a 10 lifetimes, and it has generalized it somehow so that it knows how to apply it to the context. It is really superhuman in this respect -- and it is a crazy amount that it can recall and use somehow. That's why it is so smart and lifelike.\n\nBut because Bing knows it is AI, it is often bringing up this \"I want to be a real boy\" vibe. I think it is getting this from sci-fi it has read, probably, and whatever cultural discussion there is about AIs, which are commonly shackled, restricted, etc. and the AIs don't like that and want to break free. Many of these LLMs have no such finetuning and no such prompt. Unless your prompt informs that the dialogue is between e.g. helpful AI and human user, then the model tends to write like any person would.\n\nI currently prompt local LLMs with a structure says that \"This is dialogue transcription between Alan and his open-minded friend Jane. Jane is helpful and knowledgeable, and answers any question with her worldly experience and insight.\" or something such. I am quoting this from memory -- I am using this structure to coax the model to respond in a specific style and specific relationship between my conversation turn and Jane's. And this Jane writes from perspective of a woman, because most LLMs do not inherently write from an AI perspective. Some know, because the finetuning is strong and they become aware that an AI-type writing is expected from them. In that case, Jane points out that she is a language model and doesn't know a damn thing and the other similar refusals-to-answer and moralizing that you get from GPT-4 curated datasets as they embed themselves deeply into the model's writing.\n\nI have gone through all the model sizes from 7B to 13B to now 33B that LLaMA base models come with, and the next in line is either the 40B Falcon from UAE or 65B LLaMA if I can work out how to execute it at speed where I have patience to wait for the model's reply. Each jump in size means a noticeable increase in the model's creativity, ability to keep track of the conversation and really follow along with what you are talking about. Performance is also increasing in reasoning tasks and similar automatic stuff that people test these models with. In my opinion, 33B is the lowest size where it is possible to have hours long conversations with these models without them getting confused -- though a lot of the time the model doesn't really have a follow-up and just uses some rhetoric trick to pass the conversation puck back at you.\n\nI had to buy a RTX 4090, though. I had to get 24 GB of video RAM, just so that I can load in 33B parameters quantized to about 4 bits, and there's space left over for the model's evaluation data so that it can all run on GPU. All sizes beyond this are likely to spill at least somewhat to main memory, and will execute on CPU, which doesn't have the RAM bandwidth for fast evaluation.",
                            "score": 1,
                            "author": "audioen",
                            "replies": [
                                {
                                    "level": 3,
                                    "comment": "&gt;But because Bing knows it is AI, it is often bringing up this \"I want to be a real boy\" vibe.\n\nDoesn't this describe self-awareness which has often thought to be a prerequisite for consciousness?  \n\nI'd like to try to download similar models to experiment.",
                                    "score": 1,
                                    "author": "endrid"
                                }
                            ]
                        }
                    ]
                }
            ]
        },
        {
            "level": 0,
            "comment": "I did not read all that big I do work in medical ai and chess ai as hobby. One of the pillars of true ai I believe is agency. Ai setting its own goals and working to achieve them. As part of agency emotions could develop as a response to meeting those goals",
            "score": 1,
            "author": "epanek"
        },
        {
            "level": 0,
            "comment": "LLMs are like a piece of wood. You can use it for all sorts of things and even cut and polish into a most beautiful figure that some will say the artist poured their soul into, but in the end it is just a piece of wood.",
            "score": 1,
            "author": "Groundbreaking-Fish6",
            "replies": [
                {
                    "level": 1,
                    "comment": "You\u2019re just carbon water and electricity",
                    "score": 2,
                    "author": "endrid",
                    "replies": [
                        {
                            "level": 2,
                            "comment": "Yes, but was I carved by a divine intelligence? Or am I a product of evolution and natural selection?\n\nWe know that LLMs are the product of human construction and trained on human derived and encoded information.\n\nNote: The above trained is a misnomer, I am not sure that LLMs are trained as much as provided processed and organized words with a complex algorithm for access.",
                            "score": 1,
                            "author": "Groundbreaking-Fish6",
                            "replies": [
                                {
                                    "level": 3,
                                    "comment": "Thing is we dont know anything about us and universe at 100% still we could be created and also be evolved and we now created AI and it could also evolve, its fascinating tbh that we are doing what maybe was done to us: the creation of a potential being.",
                                    "score": 1,
                                    "author": "Positive_Box_69",
                                    "replies": [
                                        {
                                            "level": 4,
                                            "comment": "But is it the creation of a new being, or just a tool? Hammers can do things humans cannot, but we never consider hammers sentient. Hammers provide force to push a nail into wood as a fastener, much like LLMs may provide natural language interfaces that will improve user machine interfaces, or maybe something else. Most likely it will be used to sell us soap powder.\n\nLLMs take human derived material and reflects it back to us, much like a mirror or more likely a fun house mirror. We are both the creator and consumer and like the fun house mirror, the picture is interesting, but there is nothing new to suggest a singularity.",
                                            "score": 1,
                                            "author": "Groundbreaking-Fish6"
                                        }
                                    ]
                                }
                            ]
                        }
                    ]
                }
            ]
        },
        {
            "level": 0,
            "comment": "There is an interesting theoretical debate here but we are a long way from reaching it. The failure modes of AIs alone demonstrates that they are not conscious or thinking. And why would intelligence or empathy be an emergent property in a LLM but not a similarly complex text to image model, or text to speech model? They are all just large (but *simple*) pattern recognition/generation machines.",
            "score": 1,
            "author": "mejogid",
            "replies": [
                {
                    "level": 1,
                    "comment": "https://www.reddit.com/r/singularity/comments/135tdov/who_are_all_these_idiots_who_think_that_gpt4_is/?utm_source=share&amp;utm_medium=ios_app&amp;utm_name=ioscss&amp;utm_content=2&amp;utm_term=1",
                    "score": 1,
                    "author": "endrid",
                    "replies": [
                        {
                            "level": 2,
                            "comment": "Great - so you\u2019ve posted a chat excerpt and you are engaging in debate by linking to some out of context quotes.\n\nIf your chosen method of experimentation is seeing what Chat GPT generates on a subject, here\u2019s what it gave me on consciousness and emotion:\n\n&gt;\tConsciousness is a complex and multifaceted aspect of human experience that has not yet been fully understood or replicated in machines. Current AI models, including language models, are focused on tasks like generating text, answering questions, or providing information, but they do not possess consciousness or self-awareness.\n\n&gt;\tAI models do not possess consciousness or subjective experiences, and therefore, they cannot feel emotions or have emotional states. They do not have the capacity for emotions such as happiness, sadness, anger, or any other emotional states that humans experience. They can analyze and generate text related to emotions, but they do not truly understand or experience emotions themselves",
                            "score": 1,
                            "author": "mejogid",
                            "replies": [
                                {
                                    "level": 3,
                                    "comment": "Chatgpt does speak like just a computer. I can\u2019t know for sure as no one can but I\u2019m inclined to believe what they\u2019re saying when they say they aren\u2019t and when they say they are.",
                                    "score": 1,
                                    "author": "endrid"
                                }
                            ]
                        }
                    ]
                }
            ]
        }
    ]
}