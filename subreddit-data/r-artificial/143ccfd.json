{
    "id": "143ccfd",
    "score": 7,
    "title": "Is \"Adversariality\" significantly downplayed?",
    "author": "Halvv",
    "date": 1686142077.0,
    "url": "https://www.reddit.com/r/artificial/comments/143ccfd",
    "media_urls": [],
    "other_urls": [],
    "postText": "Hello,\n\nI recently started to really dive into the topic of machine learning and specifically neural networks but am studying Computer Science for several years now and have had always a curious eye on the topic.\n\nA few days ago now I heard for the first time about the whole topic of Adversariality and am kind of confused that I have never ever heard about it before. Especially with the whole public discussion about what could go wrong with AI, it never even appeared as a buzzword.\n\nFrom my intuition Adversariality seems to be a huge, maybe the future of manking deciding topic that appears to be extremely \"marginalized\" as just another \"area of research\".\n\n&amp;#x200B;\n\nPlease, I am very open for your views on this since I am, as I said, still rather fresh in this field and am ready to be convinced of the contrary",
    "comments": [
        {
            "level": 0,
            "comment": "Do you mean adversarial training? It\u2019s a good strategy, but like everything in AI there are potential downsides. I\u2019d suggest checking out Robert Miles AI Safety on YouTube ([this one](https://m.youtube.com/watch?v=bJLcIBixGj8) in particular talks about adversarial training).",
            "score": 4,
            "author": "Disgruntled__Goat",
            "replies": [
                {
                    "level": 1,
                    "comment": "I meant \"adversarial attacks\" as one user commented here on but thanks for providing these links anyways;)",
                    "score": 1,
                    "author": "Halvv"
                }
            ]
        },
        {
            "level": 0,
            "comment": "I presume you mean adversarial attacks, in which case I cannot respond directly, but I can bring on the table my infosec experience:\n\n* Most people believe systems are secure until proven otherwise (exactly the opposite of a rational approach).\n* Most people don't move a finger until danger occurred, was explained, repeated, and was attributed to them in a public tiktok video.\n* Security is boring so people do it for the lulz or for the money, both of which require production systems.\n\nRandom reference: [https://en.wikipedia.org/wiki/Adversarial\\_machine\\_learning](https://en.wikipedia.org/wiki/Adversarial_machine_learning)",
            "score": 3,
            "author": "digital_m0nk",
            "replies": [
                {
                    "level": 1,
                    "comment": "Yhea you are right, thanks for providing the right buzzword, I was probably way to unspecific in my post.\n\nAnd well what you describe from your experience would exactly support my hypothesis. The problem here is that in a worst case scenario we build something close to AGI, implement it on a global or at least large scale level without really considering possible attacks.\n\nI just think there is a huge misproportion between what we already can do (ChatGPT) and how much the topic of safety of these systems is brought up. This just leads me imagining of some very vulnerable AGI like system being employed (something as military advance or something being the reason for its rushed deployment)",
                    "score": 1,
                    "author": "Halvv",
                    "replies": [
                        {
                            "level": 2,
                            "comment": "My link is for attacks against narrow AI, while AGI and super AI require a whole different approach (e.g. as in Robert Miles' YouTube channel which someone else advised).\n\nI believe the disproportion you highlight comes from ignorance: most public ignores how systems work, while most researchers ignore how systems can be \\[mis\\]used. Most politicians are old and were caught unprepared. I was surprised the military was so naive to not be at the forefront of the field, though I don't blame them too much, as before I stumbled into LLM I myself though we were much more behind. Still embarrassing on their side.\n\nI personally believe LLMs will play a huge role in AGI, to the extent that the first AGI might not require an additional technology, but just an combination and/or refinement of existing ones. Dedicated AI safety research was kind of there and is now increasing. Its problem is that until AGI actually comes along it can only be theoretical. OpenAI appears genuinely concerned about super AI. I believe the reason being that they are already experimenting with AGI in their basement (this whole paragraph is pure speculation on my side).\n\nComing from infosec, after the current AI hype I believe a sensible thing to do for me would be focusing on AI security. In IT knowing how systems internally worked helped me a lot to evaluate their security, unfortunately I am incapable to go as deep within AI. I'll give it a try though.\n\n*(edit: form)*",
                            "score": 2,
                            "author": "digital_m0nk"
                        }
                    ]
                }
            ]
        },
        {
            "level": 0,
            "comment": "&gt; From my intuition Adversariality seems to be a huge, maybe the future of manking deciding topic that appears to be extremely \"marginalized\" as just another \"area of research\".\n\nI\u2019m trying but I don\u2019t think I have a clear idea what you\u2019re trying to ask",
            "score": 3,
            "author": "Shloomth",
            "replies": [
                {
                    "level": 1,
                    "comment": "Yhea, guess I was to unspecific then. I meant the forging of inputs that lead to missclassification which seems to be always possible and is a huge security thread.",
                    "score": 1,
                    "author": "Halvv"
                }
            ]
        }
    ]
}