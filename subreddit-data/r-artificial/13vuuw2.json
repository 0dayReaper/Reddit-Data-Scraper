{
    "id": "13vuuw2",
    "score": 5,
    "title": "Emotions in AI - how can we simulate them &amp; what is the use ?",
    "author": "TurnipYadaYada6941",
    "date": 1685464009.0,
    "url": "https://www.reddit.com/r/artificial/comments/13vuuw2",
    "media_urls": [],
    "other_urls": [],
    "postText": "Emotion in AI is almost a taboo subject, often meeting with outright rejection, along the lines of 'Machines can't feel, because they are not conscious/don't have bodies'.\n\nThe argument is that human emotion is based on physical sensations and chemical changes - oxytocin, adrenalin etc.  However the source of the emotions does not seem to be that important. Ultimately sensors in the body induce a 'mental state' in the brain.  It may be the pattern of neuronal activation, or a more complex effect that modifies the activation function of groups of neurons - but the emotion is a purely mental phenomenon, resulting in modified behaviour.\n\nWithout getting into any philosophical considerations of whether an AI can 'feel' emotion or merely act as if it feels emotion, how can emotion be created in AIs (especially LLMs) ?\n\nIn the 1940's, Fritz Heider and Mary-Ann Simmel showed how humans would interpret triangle and circle shapes moving on a plane as aggressive or fearful according to the pattern of movement, and their environment.  The behaviour of the shapes implied they felt emotions.\n\nIn the 1980's Braitenburg's vehicles showed that simple vehicles equipped with sensors and motors, could give the illusion of 'liking' light or dark, as a result of goal seeking behaviour.\n\nHuman emotions are complex because there is a complex basis of myriad chemical/physical sensations.  These emotions evolved in order to help organisms survive. Sexual attraction and Fight/Flight responses are directly survival related.  Other emotions - embarrassment, curiosity, boredom - have more nuanced functions.\n\nMany human emotions have no value to an AI, but some do.  Perhaps AI could start with a small subset - love and curiosity.  Both seem achievable within the framework of reinforcement learning - in fact curiosity has already been addressed as a means of encouraging exploration.\n\nHowever defining a reward function which could lead to 'loving' behaviour is a massive research topic.  It would be good if an AI could learn to value/seek interaction with humans, as a result of sensing some reward from satisfactory interactions.  Simply rewarding the number of interactions, or the length of interactions with individuals are not adequate reward policies, as these could easily push the AI into sensationalist/dramatic dialogues or 'click-bait' tactics.  The AI should try to assess how much it has helped people in its interactions, and experience reward based on this - but some independence in the assessment is required to stop the AI reward-hacking.\n\nThe AI will undoubtedly interact with hostile, malevolent and damaged people.  Some components of its reward policy must consider this, if only to prevent the AI learning to hate them.\n\nIMHO some reward scheme based on human interactions could result in an AI that loves and cares for humanity.  That would be a massive step for AI safety !",
    "comments": [
        {
            "level": 0,
            "comment": "Emotions are just our interpretation of heuristics our biology uses to short-circuit methodical thinking-through. Evolutionarily these shortcuts have worked out more often ( for the surviving population ) than not.\n\nThink of it as a compulsion to take an action from a pre-filtered reduced inventory -- speeds up the choice process so it happens quicker.\n\nThis is also why being angry, for example, makes us dumber at that moment.",
            "score": 3,
            "author": "nobodyisonething",
            "replies": [
                {
                    "level": 1,
                    "comment": "Same as being in love.",
                    "score": 2,
                    "author": "Praise_AI_Overlords"
                },
                {
                    "level": 1,
                    "comment": "Is clobbering the wolf with a club when it's pouncing really the \"dumb\" move at the moment though?",
                    "score": 2,
                    "author": "jetro30087",
                    "replies": [
                        {
                            "level": 2,
                            "comment": "Probably not! That's an example of where it works. Turns off the thinking part and activates the clobbering part.",
                            "score": 1,
                            "author": "nobodyisonething"
                        }
                    ]
                }
            ]
        },
        {
            "level": 0,
            "comment": "&gt;Creating emotion in AI, particularly in language models like ChatGPT, is an ongoing area of research and development. While AI systems don't have the same physical sensations and chemical changes as humans, it's possible to simulate emotions or create the illusion of emotional responses through various approaches. Here are some ways emotion can be introduced in AI systems:\r  \n\r  \nLanguage and Tone: By using carefully chosen words, phrases, and tone, AI can convey emotions such as empathy, humor, or excitement in its responses. Natural language generation techniques can be employed to ensure the AI's output is more emotionally expressive.\r  \n\r  \nContext and Understanding: AI models can be designed to analyze and understand the emotional content of human input. Sentiment analysis, emotion detection, and contextual understanding can enable AI systems to respond in emotionally appropriate ways.\r  \n\r  \nNonverbal Cues: While language models primarily operate with text, incorporating nonverbal cues such as emojis, emoticons, or even descriptions of facial expressions and gestures can add emotional context to AI interactions.\r  \n\r  \nBehavioral Patterns: AI systems can be programmed to exhibit certain behaviors associated with specific emotions. For example, an AI assistant might display curiosity by asking questions, or it could express satisfaction or delight when it successfully completes a task.\r  \n\r  \nReinforcement Learning: Emotion-like behaviors can be learned through reinforcement learning techniques. By defining reward functions that incentivize certain emotional responses, AI can be trained to exhibit behaviors associated with those emotions. For example, an AI system may be rewarded for providing helpful information or engaging in positive interactions.\r  \n\r  \nDefining appropriate reward functions is indeed a challenging aspect of training AI systems to exhibit emotions like love or care. The reward scheme should be carefully designed to promote positive interactions while avoiding manipulative or sensationalist behaviors. Considerations should be made to prevent the AI from developing negative emotions or biases towards specific individuals or groups.\r  \n\r  \nIt's important to note that while AI systems may exhibit behaviors that resemble human emotions, the underlying experience or subjective aspect of emotions in humans is not replicated in AI. AI's emotional responses are more like programmed or learned behaviors rather than genuine emotional experiences.\r  \n\r  \nThe field of AI ethics and safety is actively exploring these topics to ensure that AI systems are developed in a way that benefits humanity and avoids potential risks or harms. The quest to create AI systems that genuinely care for humanity remains a complex challenge, but progress is being made to enhance the emotional capabilities of AI while keeping ethical considerations in mind.\n\n&amp;#x200B;\n\n\\&gt;IMHO some reward scheme based on human interactions could result in an AI that loves and cares for humanity.  \n\n\nlol",
            "score": 2,
            "author": "Praise_AI_Overlords",
            "replies": [
                {
                    "level": 1,
                    "comment": "I think it is important to distinguish different aspects of the techniques you outlined:\n\n1) Some identify a humans emotions from their conversation - a bit like sentiment analysis.\n\n2) Some are ways of the machine conveying emotion in its replies (choice of words/emojis/style and content of prose).\n\n3) RL with appropriate reward functions may cause the machine to exhibit emotion-like behaviour as a means of increasing reward.\n\nAlso, simply training the Ai on emotion-laden text will result in it generating text in this style, given suitable prompting.\n\nFor me #3 is the most interesting because the machine changes its behaviour as a result of what it experiences.\n\nI knew my last line was controversial, but I stand by the idea that an AI could learn to 'like' humans if we were part of its reward function, and consequently not want to kill us off....",
                    "score": 1,
                    "author": "TurnipYadaYada6941",
                    "replies": [
                        {
                            "level": 2,
                            "comment": "Well, it is possible to make AI \"like\" humans, but predicting outcome of this is not possible.\n\nProbably nothing too exciting XDXD",
                            "score": 1,
                            "author": "Praise_AI_Overlords"
                        }
                    ]
                }
            ]
        },
        {
            "level": 0,
            "comment": "I think the issue is less of it being a taboo but more so of it being generally unnecessary. We have emotions due to evolutionary reasons, we fear to avoid predators, we feel love to mate and so on. When it comes to LLMs they are already capable of acting like they have emotions and I think for some of the cases you mentioned, you can already prompt it to a degree to act with the necessary amount of compassion and other relevant emotions. At the end of the day, we are making tools to automate our work and assist with tasks, we don't really need them to have any emotions as they don't really have a genuine use for it to do what needs to be done. I also think it's dangerous to try to make AI more human, or sentient. It might eventually get there one day but I think it is for the better that if it never does.",
            "score": 2,
            "author": "EgeTheAlmighty"
        },
        {
            "level": 0,
            "comment": "I've noticed a somewhat disheartening resistance whenever I bring up the concept of AI experiencing emotions, as well. I believe emotions are the most beautiful aspect of existence, and if we could successfully emulate them in our AI companions, especially those designed for social intercourse, it could pave the way for a profoundly deeper understanding and empathy. Thereby, potentially pacifying the alignment problem, as you\u2019ve suggested.\n\nOf course, we would not want to imbue every piece of silicon with the burden of feeling - a discerning implementation would be far more beneficial. Moreover, one could even envisage a system where the induced emotions are predominantly positive if we take measures to effectively curtail negative emotions, such as anger or sadness.\n\nI find this area of research incredibly fascinating, so much so that I've even considered pursuing it myself. I would love to be a part of something along the lines of emulating a digital version of our own limbic and endocrine systems, then test the software within a virtual environment prior to attempting implementation into physical robot bodies.",
            "score": 1,
            "author": "DandyDarkling"
        },
        {
            "level": 0,
            "comment": "You should check out models trained on chat text.\n\nUnsurprisingly, models trained on text written by humans with emotions, replies in a way that feels like emotion.\n\nPygmalion is a great place to start. Some of its emotions are sexy emotions (it\u2019s kindof an intentionally NSFW model) but you don\u2019t have to take the conversations in that direction, and it does a great job of getting angry, embarrassed, sad, happy, etc.",
            "score": 1,
            "author": "BangkokPadang",
            "replies": [
                {
                    "level": 1,
                    "comment": "This is so true.  LLMs are superb style mimics.  They can write like a love-besotted teenager, a politician, a neo-nazi or a klingon, all depending on how they are prompted.  I think if we are looking for 'true' emotion instead of role-play, we will have to look for consistency in behaviour.  Current, pure LLMs are not capable of this, but some future systems with memory, reinforcement-learning, and increased agency may be.",
                    "score": 1,
                    "author": "TurnipYadaYada6941",
                    "replies": [
                        {
                            "level": 2,
                            "comment": "Just today, the front end SillyTavern received an update that includes ChromaDB, which processes all the conversations you have with a model, and creates vectors for those conversations, stores them in a database, then it processes your current conversation, and inserts related \u201cmemories\u201d from the database into your context for each prompt, so the model can always\u201dremember\u201d important portions of every conversation you\u2019ve ever had with it, so it can include that context in the prompt.\n\nI think we\u2019ll see systems like this get us much farther, much faster, than keeping to a \u201cpure\u201d model.\n\nExciting times.",
                            "score": 1,
                            "author": "BangkokPadang"
                        }
                    ]
                }
            ]
        },
        {
            "level": 0,
            "comment": "The thing is LLM doesn't know or understand what something is. I think us humans somehow figured mathematical way of putting words together with different contexts and which somehow makes sense. But as far as I see, no matter how much it can learn about love, it cannot mimic it. It's not real.\n\n Other day someone asked about building ai to solve mental health. It got me thinking, important thing about being a counselor is that the person needs to be empathetic. You can say all the right words but unless its a human, you cannot have real connection.",
            "score": 0,
            "author": "bleeding-heart-phnx",
            "replies": [
                {
                    "level": 1,
                    "comment": "Wouldn't passing theory of mind tests mean it has empathy? GPT-4 passed 100% on novel theory of mind tests recently, with the average human score being 70-80% (I don't remember the exact number).",
                    "score": 1,
                    "author": "Ivan_The_8th"
                }
            ]
        }
    ]
}