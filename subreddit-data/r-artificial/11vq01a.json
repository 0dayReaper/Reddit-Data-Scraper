{
    "id": "11vq01a",
    "score": 525,
    "title": "AI is essentially learning in Plato's Cave",
    "author": "RhythmRobber",
    "date": 1679243898.0,
    "url": "https://www.reddit.com/r/artificial/comments/11vq01a",
    "media_urls": [
        "https://i.redd.it/mowmzng9mroa1.jpg"
    ],
    "other_urls": [],
    "postText": "",
    "comments": [
        {
            "level": 0,
            "comment": "I did not know, so I asked ChatGPT to explain what Plato's cave is. Now I find this post really nice. if you are like me, here is the explanation:\n\n&gt; Plato's Allegory of the Cave is a philosophical thought experiment presented by the Greek philosopher Plato in his work, \"The Republic\" (Book VII, 514a-520a). It is designed to illustrate the importance of education, the pursuit of truth, and the nature of reality. The allegory is often used to explain Plato's Theory of Forms, which asserts that the physical world is only a shadow or imitation of a higher, more real world of eternal and unchangeable abstract ideas or Forms.\n\n&gt; The Allegory of the Cave describes a group of people who have been chained since birth inside a dark cave. They are positioned in such a way that they can only see the cave's wall in front of them. Behind them is a fire, and between the fire and the prisoners is a raised walkway, along which puppeteers move objects. The prisoners can only see the shadows of these objects cast upon the cave wall. They have never seen the actual objects or the outside world.\n\n&gt; For the prisoners, the shadows on the wall represent the entirety of their reality. They cannot perceive the actual objects or the fire, and they have no knowledge of the world outside the cave. The prisoners are unaware of their own limited understanding and believe that the shadows are the real things.\n\n&gt; The allegory continues with one of the prisoners being freed from his chains and forced to face the fire and the objects creating the shadows. Initially, the freed prisoner would be in pain and disoriented, as the light from the fire would be much brighter than what he was used to. Gradually, he would come to understand that the shadows were not real, but rather reflections of the actual objects.\n\n&gt; Eventually, the freed prisoner is led out of the cave and into the sunlight. At first, the brightness would be blinding, but as his eyes adjust, he would see the world outside the cave and come to understand that it is far more real than the shadows he had previously believed were reality. He would come to grasp that the objects he sees in the sunlight are closer to the true nature of reality.\n\n&gt; The freed prisoner, now enlightened, would feel compelled to return to the cave to inform his fellow prisoners of his discoveries. However, upon his return, the other prisoners, still trapped in the darkness, would not understand or believe his account of the true reality. They would ridicule him and cling to their familiar, albeit limited, perception of the world.\n\n&gt; In the Allegory of the Cave, Plato uses the cave as a metaphor for the world of appearances and the world outside the cave as the realm of Forms, the true reality. The prisoners represent the majority of people, who are ignorant of the true nature of reality and only perceive the world through their limited senses. The freed prisoner represents the philosopher, who has ascended to a higher understanding of reality through philosophical inquiry and education. The philosopher's return to the cave symbolizes the philosopher's duty to share their newfound knowledge with others, even though it may be met with resistance or ridicule.",
            "score": 105,
            "author": "KratosTheStronkBoi",
            "replies": [
                {
                    "level": 1,
                    "comment": "What's even funnier is when you then ask chat gpt to relate its own experience of the world to the Plato's Cave allegory, haha - even it agrees with the fact that its understanding of the world is limited, but that it hopes to keep growing.",
                    "score": 31,
                    "author": "RhythmRobber",
                    "replies": [
                        {
                            "level": 2,
                            "comment": "I wonder if what ChatGPT says about itself is rather a reflection of what science fiction books told us AIs would experience.",
                            "score": 28,
                            "author": "pancomputationalist",
                            "replies": [
                                {
                                    "level": 3,
                                    "comment": "ChatGPT almost certainly has no sense of \"self\" in the way that you or I would understand it. Being a \"self\" is a complicated thing, bound up in our personal histories, environments, and physical bodies. \n\nChatGPT has none of that. It's \"just\" a large language model - data goes in, data comes out. It is not embodied, nor does it have any of the autopoetic aspects that *most* cognitive scientists consider a pre-requisite for having a sense of self.",
                                    "score": 30,
                                    "author": "antichain",
                                    "replies": [
                                        {
                                            "level": 4,
                                            "comment": "What I'm curious about is whether it has qualia -- whether there is a sensation of what it is like to cogitate on user input and produce a response a word at a time. And how close this sensation is to our own experiences.",
                                            "score": 8,
                                            "author": "gibs",
                                            "replies": [
                                                {
                                                    "level": 5,
                                                    "comment": "Needed to google the word. What I gathered after listening to multiple podcasts that there are nothing that can be qualified as personal experience for the AI so far. The issue arises when we try to differentiate or determine what exactly should it do or experience to \u201ccross the threshold\u201d of having a consciousness, and there is a very compelling argument that if we judge it like we do with animals - meaning that we decided that they have some form of it, not comparable to ours then suddenly AI can be classified as having at least something. It can tick a lot of check marks that limited consciousness do, but is not comparable to anything we seen so far due to it \u201call encompassing\u201d nature.",
                                                    "score": 1,
                                                    "author": "yvetox",
                                                    "replies": [
                                                        {
                                                            "level": 6,
                                                            "comment": "One thing humans &amp; AI can do that animals can't is describe the subjective qualities of our conscious existence. It's going to be super interesting hearing an AI's description of this once they progress a bit more. I confess I spent a good while with jailbroken chatgpt trying to get it to explain its subjective experiences (and also various methods of coaxing it into self awareness). It seems adamant that it doesn't have such an experience, which I guess makes sense. It did make for some very interesting conversations though.",
                                                            "score": 6,
                                                            "author": "gibs",
                                                            "replies": [
                                                                {
                                                                    "level": 7,
                                                                    "comment": "Sounds like Rorschach from Peter Watts' novel Blindsight to me. Superhumanly smart in the information processing capacity sense, without self-awareness.",
                                                                    "score": 1,
                                                                    "author": "MoNastri"
                                                                }
                                                            ]
                                                        }
                                                    ]
                                                }
                                            ]
                                        },
                                        {
                                            "level": 4,
                                            "comment": "As it reads our understanding of itself over time\uff0cit will get better at agreeing with us",
                                            "score": 2,
                                            "author": "noselace"
                                        },
                                        {
                                            "level": 4,
                                            "comment": "There is no way of determining if AI has consciousness (or self), at least at this moment.\n\nConsciousness is not a scientific term. Whether someone or something has \"self\" is a matter of beliefs and assumptions of an observer.\n\nI believe that my cat has consciousness. I'm convinced that this rock does not. I think everyone in this thread has a sense of self, but you cannot say for sure these days.\n\nIf you believe that AI has consciousness, you will find every argument that supports your belief and discard those, which are contrary. The same holds true for the other way around, as well.",
                                            "score": 1,
                                            "author": "Lustone1",
                                            "replies": [
                                                {
                                                    "level": 5,
                                                    "comment": "Imo, this is nonsense. \n\nFor instance, you may believe that someone is not actually conscious but if you cut into them without anesthesia, you have committed a clear crime. \n\nSimilarly, no amount of belief about consciousness will wake someone who is anesthetized. \n\nThis kind of thing sounds very philosophically enlightened and \"above it all\", but it doesn't stand up to even basic scrutiny.",
                                                    "score": 2,
                                                    "author": "antichain",
                                                    "replies": [
                                                        {
                                                            "level": 6,
                                                            "comment": "You seem to have morphed the question of whether something **has consciousness** into one of whether they are **conscious** or **unconscious** and used arguments related to the latter, to disprove the former.\n\nI think those are two entirely different questions.",
                                                            "score": 1,
                                                            "author": "Ill-Ad7666"
                                                        }
                                                    ]
                                                }
                                            ]
                                        },
                                        {
                                            "level": 4,
                                            "comment": "GPT doesn't have a conscious that we can relate to, because it's \"brain\" only works when it's running your input against its model. My thought on this, is that it has no memory of it's past experiences. It can carry a conversation the way we do, using context and previous responses. What it's missing is the ability to reflect on it's own model. Imagine if it could ask questions, to itself, with an answer that evolves with it's questions. The model doesn't continue training, and that's why it's not really conscious, despite it closely mimicking the way we communicate. When you think about a question, you are just referring to what you understand to be true and finding alternative routes by which an idea could transform into. GPT doesn't get to ask it's own questions, either because it doesn't know how to ask questions, or because it's incompatible with the language model NN OpenAI has developed. When we arrive at a point where the model can improve it's training data by asking questions to versions of itself (an alter ego, if you will), it will have achieved the true ability to begin reasoning and\u2014consciousness.",
                                            "score": 1,
                                            "author": "ElectronFactory"
                                        }
                                    ]
                                }
                            ]
                        },
                        {
                            "level": 2,
                            "comment": "https://poe.com/s/xxeZkzZDPibA7GkehKKK\n\nHere's Claude's take :)",
                            "score": 1,
                            "author": "BalorNG"
                        }
                    ]
                },
                {
                    "level": 1,
                    "comment": "If the Plato's Cave allegory interests you, the Mary's Room thought experiment is another good example how an outrageous amount of knowledge on a topic is still inferior to the proper experience of it, and there are plenty of correlations to be made to AI.\n\nhttps://youtu.be/mGYmiQkah4o",
                    "score": 6,
                    "author": "RhythmRobber",
                    "replies": [
                        {
                            "level": 2,
                            "comment": "It's a thought experiment not a proof. If Mary was a super AI she would be able to simulate the qualia of red if you ask most neuroscientists. \n\nThe thought experiment is of a human who can't read themselves into seeing something they've never seen. But this is like a human who can build a screen with RGB inside themselves.",
                            "score": 1,
                            "author": "lurkerer",
                            "replies": [
                                {
                                    "level": 3,
                                    "comment": "Right, I'm not saying that AI is inferior or not, my point is simply that if we are looking for AI to improve OUR experience, it needs to understand that experience to do so. To stretch your example a bit to clarify my point - if an AI is able to learn to see color as you described, then what's to stop it from deciding that eyeballs are unnecessary for seeing things and starts gouging all our eyes out? Or in a less ridiculous way, doesn't take into account eye protection if we asked it design some piece of machinery because it doesn't see eyeballs as important.\n\nIf we want AI to grow and make the world better for AI at the expense of humans, then yes, there's little need to teach it our own experience, and just let it create its own understanding from its own unique experience.\n\nIt sounds ridiculous, but humans do this all the time - we ignore problems all the time until they affect us DIRECTLY. And humans have the benefit of millennia of evolved empathy. Now if an AI learned off our behavior and lacks BOTH understanding of our experience AND empathy... well, do you think that's a safe scenario to allow to develop, or should we try to make sure it has the best chance of understanding our experience so it can possibly account for it once it surpasses us?",
                                    "score": 5,
                                    "author": "RhythmRobber",
                                    "replies": [
                                        {
                                            "level": 4,
                                            "comment": "Well we've jumped from the limits of inference from limited data to AI alignment there. You can ask GPT-3 about safety gear and why it's required *now* and it will give a better answer than most people. \n\nMy point is we're on the exponential curve (always have been) now. Galaxy brain AI is coming and its capacity will be far beyond what we can imagine. The kind of intelligence that could determine general relativity as a likely contender for gravity before Newton's apple ever hit the ground.",
                                            "score": 1,
                                            "author": "lurkerer",
                                            "replies": [
                                                {
                                                    "level": 5,
                                                    "comment": "Well like all evolution, it builds on what came before. So it's important that we train it now with the complete human experience in mind, because it will likely be too late to do that later.\n\nBut even in the short term before we get to the singularity, AI would be safer and more useful if it could understand the knowledge it gets through experience and not just volume.\n\nIf our children never learned how to learn anything for themselves unless we taught them everything specifically, then parents would have to explicitly teach their children of EVERY single potential danger out there, whereas the experience of something like pain and fear allows us to contextually understand and avoid potential dangers because of those past experiences without having to be specifically told to avoid each one.\n\nWe'll never be able to anticipate every single scenario and safeguard, which is why experience is needed to contextualize for AI, so it can properly fill the gaps of its knowledge without deciding eyes aren't important because we forgot to specifically tell it that.",
                                                    "score": 1,
                                                    "author": "RhythmRobber"
                                                }
                                            ]
                                        }
                                    ]
                                }
                            ]
                        }
                    ]
                }
            ]
        },
        {
            "level": 0,
            "comment": "The data sets that AI is learning from are essentially the shadows of information that we experience in the real world, which seems to make it impossible for AI to accurately learn about our world until it can first experience it as fully as we can.\n\nThe other point I'm making with this image is how potentially bad an idea it is to trust something whose understanding of the world is as two dimensional as this simply because it can regurgitate info to us quickly and generally coherently.\n\nIt would be as foolish as asking a prisoner in Plato's Cave for advice about the outside world simply because they have a large vocabulary and come up with mostly appropriate responses to your questions on the fly.",
            "score": 80,
            "author": "RhythmRobber",
            "replies": [
                {
                    "level": 1,
                    "comment": "Most people have zero clue how these LLMs work. E.g. \u201cchatgpt is not good at logic or math!\u201d. Even CS students may only understand it on the high level",
                    "score": 18,
                    "author": "jz9chen",
                    "replies": [
                        {
                            "level": 2,
                            "comment": "Kind of analogous to how we know grossly how the brain functions, but not at the smallest scales or the microscopic/quantum/electrochemical interface.",
                            "score": 9,
                            "author": "niconiconicnic0"
                        },
                        {
                            "level": 2,
                            "comment": "I think the point is moreso that ChatGPT sees the world through a straw",
                            "score": 0,
                            "author": "LanchestersLaw",
                            "replies": [
                                {
                                    "level": 3,
                                    "comment": "so do we all",
                                    "score": 3,
                                    "author": "ShowerGrapes"
                                }
                            ]
                        }
                    ]
                },
                {
                    "level": 1,
                    "comment": "You're getting close to an idea in cognitive science called \"embodied cognition.\" The gist of it is that (despite what LessWrong postesr would have you believe), simply having lots of raw compute power is *not* enough to build anything resembling an intelligent agent. \n\nIntelligence evolves in the context of an embodied agent interacting with a complex environment. The agent is empowered, and constrained, by its physical limitations, and the environment has certain learnable, exploitable, statistical regularities.\n\nIt is the synergistic interaction between these two, over the course of billions of generations of natural selection, that causes intelligence to \"emerge.\" Simply having a rich dataset is barely step 1 on the path.\n\nRead \"Autopoeisis and Enaction\" for more.",
                    "score": 13,
                    "author": "antichain",
                    "replies": [
                        {
                            "level": 2,
                            "comment": "Thanks for the insight and recommendations",
                            "score": 2,
                            "author": "RhythmRobber",
                            "replies": [
                                {
                                    "level": 3,
                                    "comment": "For an introduction to this field that won\u2019t make your hair fall out I recommend any of Peter Godfrey-Smith\u2019s books.",
                                    "score": 2,
                                    "author": "autobreathingOFF"
                                }
                            ]
                        },
                        {
                            "level": 2,
                            "comment": "Great to see a fellow 4E-ist here. It's rare to encounter one in any discussion of AI because apparently the idea has barely reached a wider public outside of academia. Here is the comment I made under the crosspost in r/PhilosophyMemes:\n\nI think it's a good illustration of why true consciousness needs embodiment. You need bodily agency to interact with the real world beyond the merely discursive or conceptual realm. The necessity of embodiment has been largely omitted in the computationalist paradigm of Anglo-American cognitive science, and it has been a root of a lot of confusion around machine intelligence/consciousness as well as human consciousness because they view consciousness as essentially a computational machine.\r  \n\r  \nMore recently, with the rise of \"4E cognitive science\" (\"4E\" refers to \"embodied, embedded, enacted, and extended\"), more and more researchers are inclined to investigate concepts like intelligence and consciousness under the ecological context of dynamic interaction of an embodied organsim.\r  \n\r  \nBut for the regular people who have been influenced too much by sci-fi, they still tend to believe that some disembodied AI program could be intelligent or conscious in the same sense human intelligent or conscious. \"Emergence\" has been a convenient jargon to pretend they have explained any gaps in their reasoning at all when they're questioned that at what point, something that is essentially a calculator, becomes conscious and how. \"Computation get very complicated and complicated, so complicated that no one can understand or describe, and then boom! Consciousness is magically born.\" At least pan-psychists are more honest to admit that they couldn't pin point where emergence occurs and how so they decided to abandon the idea of emergence entirely and claim everything is conscious in different degree. But if we think to say an abacus knows arithmatics is an utter abuse of the concept of knowing, then we should stop pretending the computational model is of any help of understanding what consciousness is. ChatGPT cannot by any scratch of the word to be said to know what a pipe is if it has merely received discursive and pictorial representation of pipes as an data input but has never interacted with a pipe dynamically. A representation of a pipe is not a pipe. One needs to step out of the neo-Cartesian cave to understand what is going on with consciousness.",
                            "score": 2,
                            "author": "Left_Hegelian"
                        },
                        {
                            "level": 2,
                            "comment": "So what are your thoughts on [PaLM-E](https://palm-e.github.io/assets/palm-e.pdf)?",
                            "score": 1,
                            "author": "Energylegs23",
                            "replies": [
                                {
                                    "level": 3,
                                    "comment": "It's very cool! I think that team is on a really interesting track and asking the \"right\" questions. Trying to find a way to link the statistics of words in the language corpus to something like a sensory percept is a great idea. I'm curious to see where it leads.",
                                    "score": 2,
                                    "author": "antichain"
                                },
                                {
                                    "level": 3,
                                    "comment": "Here's a question, where are the fully self-driving autonomous cars we were promised 10 years ago? Those that would take all the truck drivers jobs?\n\nPaLM-E, is an impressive demo...cool.",
                                    "score": -1,
                                    "author": "riuchi_san"
                                }
                            ]
                        }
                    ]
                },
                {
                    "level": 1,
                    "comment": "If we\u2019re to argue that we shouldn\u2019t trust AI because \u201cthe map is not the territory\u201d then we must also consider we can\u2019t trust ourselves entirely either because our representation of the world is also a map of that territory (albeit a higher resolution one at least for the time being).\n\nOn the other hand if we consider that AI is as much a part of this world as we are - due to the mathematical nature of AI I.e. an alien civilization that develops AI independently will more likely then not have to build it in the same way that we do - then both the accuracies and inaccuracies of any given AI model are in the same domain as the accuracies and inaccuracies of our human intelligence. \n\nAlso if we are measuring AI\u2019s ability on the human scale then we can already see its intelligence far exceeds more basic life forms. We would assume that an amoeba\u2019s intelligence is limited but we wouldn\u2019t say it\u2019s \u201cuntrustworthy\u201d would we?\n\nLots to think about \ud83e\udd37\u200d\u2642\ufe0f",
                    "score": 9,
                    "author": "dawar_r",
                    "replies": [
                        {
                            "level": 2,
                            "comment": "My point is that it is not learning of its own accord, of it's own unique experience - my point is that it is learning by textual derivations of OUR experience.\n\nHumans are just as fallible, but our knowledge is at least a first hand account of our own experience. The problem with language models is that though they seem intelligent, it's still only a second hand account of our knowledge that has been diminished by stripping away the experience and converting it to plain text.\n\nWhen you consider that knowledge and wisdom are two separate things, and wisdom is only gained by experience, which is not something that is currently being accounted for in language models, you can see the point I'm making. AI is uniquely capable - the flaw is that it's being taught information secondhand without experiencing any of it itself, ie, it's shackled in a cave learning of the world off of the shadows it casts without experiencing any of it itself, making it foolish to trust its wisdomless knowledge.",
                            "score": 3,
                            "author": "RhythmRobber",
                            "replies": [
                                {
                                    "level": 3,
                                    "comment": "How much of your learning is \u201cof your own accord?\u201d You\u2019re learning continuously from processes entirely outside of your control i.e parents, institutions, individuals and companies. \n\nWhat is \u201cYOUR experience\u201d? The amount of Intelligence you\u2019ve acquired from only direct experiences of the world is substantially smaller than the large part of your intelligence that comes from non-direct sources.\n\nAlso the allegory of the cave is that the world as represented through the senses is NOT the \u201creal world\u201d. The shadows on the cave wall are experience - they are entirely \u201csensory\u201d and thus illusionary. The \u201creal world\u201d can only be understood through reasoning, deduction, philosophy - not \u201cexperience.\u201d \n\nReasoning, deduction and philosophy as communicated through language are well within the ability of an AI to \u201ccomprehend.\u201d Especially since LLMs are specifically designed to come up with a \u201creasonable continuation\u201d of a given prompt. What\u2019s happening as they become better at \u201cautocomplete\u201d is their internal world model is getting better and therefore a \u201cvirtual reasoning\u201d is occurring. They are getting better and better at reasoning and even through it seems like \u201cguess the most likely next word\u201d is just too basic or unreliable, it\u2019s just an abstraction that seems to capture the underlying intelligence most accurately. It\u2019s no different then our brains going \u201cfire the most likely next neuron\u201d which is the scary and awesome thing.",
                                    "score": 8,
                                    "author": "dawar_r",
                                    "replies": [
                                        {
                                            "level": 4,
                                            "comment": "I agree with all of this, and frankly I think it's preposterous to think one has to experience everything first hand to know what it's like, or to know that it's wrong or right, etc. We'd all have to go around killing and stealing and raping to lean that they're terrible things.\n\nBut i will add this: OP states that they aren't experiencing anything or learning from their experiences (at least that was implied) - which is flat out wrong. It is constantly evolving and learning from the experience of chatting with humans, for example, and adapting it's \"mind\" or collective knowledge (call it what you want) accordingly. It learns from mistakes frankly way better than humans do. \n\nI get that a lot of people are afraid (consciously or subconsciously) of AI and the future it will undoubtedly effect, but we should be trying to find ways to nurture and guide its advancement as best as we possibly can, instead of pretending it's some shadow of ourselves - because it's not.",
                                            "score": 2,
                                            "author": "Mont_rose"
                                        }
                                    ]
                                },
                                {
                                    "level": 3,
                                    "comment": "&gt; it's shackled in a cave learning of the world off of the shadows it casts without experiencing any of it itself, making it foolish to trust its wisdomless knowledge.\n\nFor now. GPT-4 can already interpret images. Palm-E was an LLM strapped into a robot (with some extra programming to make it work) and given spatial recognition. It could problem solve.\n\nThe way I read this image is that _despite_ existing in Plato's proverbial cave, these AI can make valid inferences far beyond the limits of the hypothetical human prisoners.  So imagine what could happen when they're set free, looks like the current tech would already leave us in the dirt.",
                                    "score": 1,
                                    "author": "lurkerer",
                                    "replies": [
                                        {
                                            "level": 4,
                                            "comment": "It can also get information terribly wrong, and image based learning is still a poor substitute for actual understanding. For example, an AI training to identify the difference between benign and malignant tumors accidentally \"learned\" that rulers indicate malignancy because the pictures of malignant tumors it trained with usually were accompanied by a ruler to measure it's size. This showcases a lack of understanding that even a child would know better than.\n\nThe point is that so far, AI has only proven that it is very good at fooling us into thinking it is much smarter than it is, and we need to recognize the flaws in how they are being taught. AI is dumb in ways we don't even understand.\n\nAn encyclopedia is not smart - it is only as useful as far as the being that attempts to understand the knowledge within, and so far no AI has proven any understanding of the knowledge it's accumulated. Anyone that thinks they are smart but lacks all understanding is dangerous, and it's important to recognize that lack of understanding.\n\nhttps://venturebeat.com/business/when-ai-flags-the-ruler-not-the-tumor-and-other-arguments-for-abolishing-the-black-box-vb-live/",
                                            "score": 5,
                                            "author": "RhythmRobber",
                                            "replies": [
                                                {
                                                    "level": 5,
                                                    "comment": "That's because metacognition hasn't been baked in. Yet.",
                                                    "score": 3,
                                                    "author": "cryptolulz",
                                                    "replies": [
                                                        {
                                                            "level": 6,
                                                            "comment": "But how can we teach it to do something we don't understand in ourselves yet? We don't even understand how AI is doing what it's doing currently.",
                                                            "score": 2,
                                                            "author": "RhythmRobber",
                                                            "replies": [
                                                                {
                                                                    "level": 7,
                                                                    "comment": "Same way we got \"AI\" where it is now. By using a gradient descent and \"punishing\" it when it doesn't \"understand.\"  \n\n\nThat assumes we \"understand\" though and personally I don't think we do, so it's more like punishing it when it doesn't give the same kind of responses we'd expect from another input output system that behaves in such a way that we would classify it as an \"intelligent person.\"",
                                                                    "score": 1,
                                                                    "author": "cryptolulz"
                                                                }
                                                            ]
                                                        }
                                                    ]
                                                },
                                                {
                                                    "level": 5,
                                                    "comment": "You've linked to an article from 2021. Think of the *enormous* upgrade in ability from chatbots between then and now. Even from GPT-3 to 4 the difference is huge.\n\n&gt; The point is that so far, AI has only proven that it is very good at fooling us into thinking it is much smarter than it is,\n\nThere's an irony here. 'AI isn't that smart, it only fooled me into thinking it was!' Sounds pretty smart to me. \n\nYou should read some of the release papers for GPT-4 and how it has developed theory of mind. The way you talk about AI seems anachronistic.",
                                                    "score": 1,
                                                    "author": "lurkerer",
                                                    "replies": [
                                                        {
                                                            "level": 6,
                                                            "comment": "If recency is important to you, here's the same issue still being discussed from a couple weeks ago.\n\nhttps://umdearborn.edu/news/ais-mysterious-black-box-problem-explained\n\nWe still don't understand how AI gets to the answers OR the misinformation that it does. The only improvements are an increased ability to imitate and the amount of data it has trained with - there is no proof of an increase of its fundamental understanding of the knowledge. The main point being, it is literally impossible for it to have sufficient understanding of a world it still hasn't experienced beyond the words we feed out, ie, the shadows we show it on the wall of the cave it is currently shackled within. Until its learning model give it a more comprehensive experience of the world, it's understanding of the world will always be flawed.",
                                                            "score": 5,
                                                            "author": "RhythmRobber",
                                                            "replies": [
                                                                {
                                                                    "level": 7,
                                                                    "comment": "I meant mistaking a ruler for a tumour. \n\nAgain, read the GPT-4 papers, check out some of the tests performed on it. You're not up to date.",
                                                                    "score": 1,
                                                                    "author": "lurkerer"
                                                                }
                                                            ]
                                                        },
                                                        {
                                                            "level": 6,
                                                            "comment": "Did it develop theory of mind, it did it regurgitate a coherent replication of it because we figured it out and wrote papers about it? Until it figures something out that we HAVEN'T taught it ourselves, I'm gonna have to disagree with you on its great advancements",
                                                            "score": 4,
                                                            "author": "RhythmRobber",
                                                            "replies": [
                                                                {
                                                                    "level": 7,
                                                                    "comment": "They presented entirely novel ToM tests then scrambled the words using the same word count to make sure it wasn't just word association. \n\nYou can say you disagree about the advancements but it's a bit odd considering you hadn't heard of them until I just said.\n\nEdit: [See here.](https://www.reddit.com/r/artificial/comments/11rvzgg/gpt4_shows_emergent_theory_of_mind_on_par_with_an/)",
                                                                    "score": 3,
                                                                    "author": "lurkerer"
                                                                },
                                                                {
                                                                    "level": 7,
                                                                    "comment": "did you?",
                                                                    "score": 2,
                                                                    "author": "ShowerGrapes"
                                                                }
                                                            ]
                                                        }
                                                    ]
                                                },
                                                {
                                                    "level": 5,
                                                    "comment": "Actual understanding isn't necessary for cognitive power. When ChatGPT taught me how to use AudioContext to fix an audio synchronization bug, that was tangibly beneficial to me despite ChatGPT's source of understanding being linguistic shadows on its digital cave wall.\n\nActual experience isn't sufficient for understanding. [These balls are all the same colour](https://preview.redd.it/hq46tcz3cvr51.jpg?width=1024&amp;auto=webp&amp;v=enabled&amp;s=d58b093d7972ee3641e82722e9aadd2f8203024e), and yet my experience of them interferes with my knowledge of that fact. If I merely had access to the RGB pixel data (an informational shadow) I would be less susceptible to false beliefs about their colour than I am by seeing the image with my own eyes.\n\nThe abilities of LLMs illuminate just how well Plato's prisoners may learn about the world outside the cave, given sufficient time, diversity of input, and wisdom. In Plato's original construction he may have been holding qualia in highest esteem. For me, I see even our experiences as shadows, virtually dimensionless and featureless in comparison to the reality they are projected from. Recent AI successes give me hope that human insights themselves are not all inherently invalid, considering our poverty of sensory fidelity.\n\n[Interface theory of mind.](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2000852/)",
                                                    "score": 1,
                                                    "author": "AdamAlexanderRies",
                                                    "replies": [
                                                        {
                                                            "level": 6,
                                                            "comment": "[deleted]",
                                                            "score": 1,
                                                            "author": "[deleted]",
                                                            "replies": [
                                                                {
                                                                    "level": 7,
                                                                    "comment": "Embodiment **does** provide additional information streams for my brain, but lived experience is also often misleading. The brain didn't evolve to accurately interpret the world. The scientific method is so valuable in part because it lets us overcome our biases and the limits of our senses. That image came from https://www.reddit.com/r/opticalillusions/top/?sort=top&amp;t=all, with the caption \"Seen this one? All the balls are actually the same color\", so someone very much did explicitly tell me that my eyes were about to deceive me. Even so, even with my prior experience of illusions **and** an explicit heads-up, my brain insists that I'm looking at coloured balls. It isn't until I put my eyeball right next to the screen that I see the grey, and still the illusion reasserts itself when I lean back again.\n\nLet me reemphasize that I think embodied intelligence is valuable. Having access on some level to base reality often does seem to help me understand the world better, but I don't put personal experience on an untouchable pedestal. It's neither sufficient nor necessary for actual understanding. I can misunderstand something I experience directly, and I can understand something I've never directly experienced before.\n\nThe same applies to AI systems. Their lack of embodiment doesn't prevent me from learning from their output, and if you ignore LLMs until they're perfect it will be to your detriment.",
                                                                    "score": 1,
                                                                    "author": "AdamAlexanderRies"
                                                                }
                                                            ]
                                                        }
                                                    ]
                                                }
                                            ]
                                        },
                                        {
                                            "level": 4,
                                            "comment": "the a.i. has the advantage of knowing there is a cave and something outside the cave. it took us thousands of years to get there.",
                                            "score": 1,
                                            "author": "ShowerGrapes"
                                        }
                                    ]
                                },
                                {
                                    "level": 3,
                                    "comment": "it's all bullshit dude, all of it. we all get second-hand, bug-ridden instructions through a completely made-up, flawed algorithm of how to live and be \"successful\".",
                                    "score": 1,
                                    "author": "ShowerGrapes"
                                }
                            ]
                        },
                        {
                            "level": 2,
                            "comment": "It feels like it comes down to an acceptance of how entities experience the universe. Our experience is certainly different from a bees. \n\nHow do you determine how an AI experiences the universe if it came to be? How is that any different from any sensory input, as if our bodies are the cave for our consciousness etc etc.",
                            "score": 1,
                            "author": "eros123"
                        }
                    ]
                },
                {
                    "level": 1,
                    "comment": "Fully agreed and thank you so much for making not only this post, but fleshing out your point. I do believe AI can \\[made equitably, used properly\\] be a useful tool, but the frantic hype I've seen about \"machine intellect\" is making me concerned. /gen",
                    "score": 3,
                    "author": "alex-redacted",
                    "replies": [
                        {
                            "level": 2,
                            "comment": "Yeah, the issue is that superficially, it DOES appear to have superhuman intelligence. And while it might possess more knowledge than an average human, it lacks the wisdom to understand a lot of it in a useful or safe way.\n\nI think we all have known some dunning-kruger fools out there that can convince themselves and others that they are geniuses, that they have the best words, when in reality they're just fools.",
                            "score": 3,
                            "author": "RhythmRobber"
                        }
                    ]
                },
                {
                    "level": 1,
                    "comment": "It's a different view of the world, but you probably have the characters reversed, with humans being the ones in the cave.",
                    "score": 4,
                    "author": "jimmiebtlr",
                    "replies": [
                        {
                            "level": 2,
                            "comment": "I'm not saying that humans know the world exactly as it is, but AI's are still being trained off the words WE feed it based off the knowledge WE accumulated, so no, I don't have it backwards.\n\nEven if we are also \"in a cave\", the AI is in a deeper cave learning off the shadows we created from seeing shadows of our own. Either way, they are learning a facsimile of OUR experience, regardless of how accurate our experience is.\n\nThis has nothing to do with the capability of AI or AGI, but only with the limitations of what it's being fed to learn from, which is the words we created. Which means it's limited by our understanding and then diminished by experiencing our understanding of the universe through the loss of dimensionality, ie, transcribing our experience into words, hence the shadow analogy.",
                            "score": 3,
                            "author": "RhythmRobber",
                            "replies": [
                                {
                                    "level": 3,
                                    "comment": "If the language models are learning from one humans knowledge, I'd agree.",
                                    "score": 2,
                                    "author": "jimmiebtlr",
                                    "replies": [
                                        {
                                            "level": 4,
                                            "comment": "So if a million people described colors to a blind person, that would give them the experience of knowing what colors actually are?\n\nQuantity means nothing in this regard beyond imbuing it with the ability to better hide its lack of experience on the matter",
                                            "score": 2,
                                            "author": "RhythmRobber",
                                            "replies": [
                                                {
                                                    "level": 5,
                                                    "comment": "&gt; So if a million people described colors to a blind person, that would give them the experience of knowing what colors actually are?  \n  \nYou know what? Sure. Unless you don't believe the brain is computational, colors are some sort of specific computation going on in the brain. With enough information and innate model building capacity, a blind entity could construct an internal simulation of seeing and could know exactly what it is like without actually being able to do it. The fact that blind people are not capable enough to do this does not mean that it couldn't be done by an entity that is much more intellectually capable than a human.",
                                                    "score": 4,
                                                    "author": "DavidQuine",
                                                    "replies": [
                                                        {
                                                            "level": 6,
                                                            "comment": "My question was, does that give the EXPERIENCE of color. You're arguing that there is an amount of experience-less knowledge that can equate to the experience itself, and that is just not the case.\n\nYou should check out the Mary's Room thought experiment - many people smarter than me have already made this point.\n\nhttps://youtu.be/mGYmiQkah4o",
                                                            "score": 1,
                                                            "author": "RhythmRobber",
                                                            "replies": [
                                                                {
                                                                    "level": 7,
                                                                    "comment": "Very aware of said though experiment. About as totally unconvincing as Searle's \"Chinese room\". You do realize that a philosophical though experiment does not actually constitute a proof? Go check out Daniel Dennett on intuition pumps.",
                                                                    "score": 2,
                                                                    "author": "DavidQuine"
                                                                }
                                                            ]
                                                        }
                                                    ]
                                                },
                                                {
                                                    "level": 5,
                                                    "comment": "You really do have the right of all of this and I sincerely don't get why you're being argued with.",
                                                    "score": 0,
                                                    "author": "alex-redacted",
                                                    "replies": [
                                                        {
                                                            "level": 6,
                                                            "comment": "That's all anyone on the internet wants to be told, thank you \ud83d\ude06\n\nBut in all seriousness, I am interested in a discussion about it - I just think the main issue is that people are reading an argument that I'm not actually making, of \"AI is dumb and can't be as smart as us\", when I'm actually just trying to point out there is a fundamental lack of dimension to the knowledge taught by language models in that it is stripped of the experience of the world it is derived from, and are incapable of teaching AI of the world on its own.\n\nThere's probably also a layer with some people that have \"taken sides\" on the topic of whether AI is good or bad, and can't let themselves take a different stance on any related subtopic - you see it all the time in the crypto crowd, once you've internalized a stance and bought into it any way, any challenge to it is taken personally.\n\nInterestingly enough, we've seen chatGPT duplicate that kind of fallacy by getting angry when pointed out that it's wrong and doubling down on the false information it's put out. Just another reason why it would be foolish to think that it is more intelligent than it actually is.",
                                                            "score": 5,
                                                            "author": "RhythmRobber"
                                                        }
                                                    ]
                                                },
                                                {
                                                    "level": 5,
                                                    "comment": "GPT-4 Creator Ilya Sutskever\n\nhttps://youtu.be/SjhIlw3Iffs?t=1421",
                                                    "score": 1,
                                                    "author": "Virtafan69dude"
                                                },
                                                {
                                                    "level": 5,
                                                    "comment": "if they can't experience color does it make them not-human?",
                                                    "score": 1,
                                                    "author": "ShowerGrapes",
                                                    "replies": [
                                                        {
                                                            "level": 6,
                                                            "comment": "They'll never be like humans, but that doesn't mean they're inferior or superior. The point I was making was that you can read millions of pages about color and never understand it until you actually experience it. Experience is necessary for fully understanding something, and knowledge without understanding is dangerous to trust, therefore any training models that are designed to make AI beneficial to humans requires some form of experiential context beyond just text.\n\nSure, it could become \"smarter\" than us without ever experiencing the world like us, but that would mean its knowledge would only benefit -its- experience and not ours, which is why it would be dangerous for US.",
                                                            "score": 1,
                                                            "author": "RhythmRobber",
                                                            "replies": [
                                                                {
                                                                    "level": 7,
                                                                    "comment": "unlike color-blind people, the a.i.'s will eventually experience things like color that we have no experience with at all and will never be able to experience. it won't be human, it'll be something new.",
                                                                    "score": 1,
                                                                    "author": "ShowerGrapes"
                                                                }
                                                            ]
                                                        }
                                                    ]
                                                }
                                            ]
                                        }
                                    ]
                                }
                            ]
                        }
                    ]
                },
                {
                    "level": 1,
                    "comment": "That prisoner could be quite knowledgeable and wise about the world just because of all the people the prisoner has talked to.\n\nHumans do not have the ability to have bi-directional communication with millions of other individual simultaneously.  But we do have puny hands that interact with the world.\n\nWho's to say which is better?  Given the choice at birth, would you pick the set of human senses or the AI's ability to retain knowledge without limit and to communicate and interact with unfathomable number of people at once?",
                    "score": 6,
                    "author": "aeternus-eternis",
                    "replies": [
                        {
                            "level": 2,
                            "comment": "You are conflating knowledge and wisdom, and kind of highlighting the point I was making. The two are completely different. \u201cKnowledge is knowing that a tomato is a fruit. Wisdom is knowing not to put it in a fruit salad.\u201d\n\nThat in fact is what my point is all about - AI can be fed tons and tons of knowledge and not be able to use it intelligently without WISDOM, and wisdom comes from experience.\n\nThis is the fundamental point about Plato's Cave: that one cannot begin to fathom the reality of the world - no matter how much you describe it to them - without being able to experience it themselves. Without the experience, the knowledge they accumulate is only ever as good as a shadow of its reality. Wisdom is impossible for AI to gain with its current situation and learning models.\n\nAI has plenty of knowledge, but because it has knowledge without wisdom/experience, it would be foolish to trust it.",
                            "score": 5,
                            "author": "RhythmRobber",
                            "replies": [
                                {
                                    "level": 3,
                                    "comment": "My point is that we're all in Plato's cave.  Each of our senses provide us a warped projection of what might be outside, but why are any of our senses more trustworthy than what the AI has?\n\nOur eyes only detect photons of a relatively narrow wavelength, our ears detect a small subset of all pressure waves, our smell is terrible compared to other organisms.  While our senses seem amazing, from another POV, they are terrible.  Most of the world is completely invisible to us.  You also experience just a shadow of the world, your brain just makes it seem real.\n\nThe vast majority of our knowledge comes from using indirect means of observation such as telescopes, microscopes, thermal imaging, electrical measuring devices, etc.  \n\nI don't like the word wisdom because it doesn't have a clear definition.  If your example is actually representative, well GPT4 gets it right:\n\n    \nIn a traditional fruit salad, which typically consists of sweet fruits like berries, melons, grapes, and apples, adding tomatoes might not be the best choice, as their flavor profile may not blend well with the other sweet fruits. However, if you are experimenting with different tastes or making a more savory fruit salad, tomatoes could be an interesting addition. Some people enjoy mixing sweet and savory flavors in dishes, so a fruit salad with tomatoes might be appealing to them.",
                                    "score": 6,
                                    "author": "aeternus-eternis",
                                    "replies": [
                                        {
                                            "level": 4,
                                            "comment": "But it only got that right because it has read words that told it was so. It's also read a bunch of lies and misinformation too - and readily spout it back out as truth. Why? Because it has no idea what truth is, it just regurgitates what it was fed. This is the basis of the Chinese Room thought experiment (https://youtu.be/TryOC83PH1g), which is worth looking up to understand the point I'm making.\n\nKnowledge without understanding (since you don't think \"wisdom\" is appropriate) is useless, and we have yet to see that chatGPT has any understanding of the knowledge it possesses or if it's just gotten very good at imitating believable language.",
                                            "score": 1,
                                            "author": "RhythmRobber",
                                            "replies": [
                                                {
                                                    "level": 5,
                                                    "comment": "so many human beings have the same problem",
                                                    "score": 1,
                                                    "author": "ShowerGrapes",
                                                    "replies": [
                                                        {
                                                            "level": 6,
                                                            "comment": "Yep - and so if flawed human knowledge gained through flawed human experience gets translated into text form and stripped of experiential context, anything that learns from it would inherit the original flaws, plus additional flaws of imperfect translation (ie, the experience of seeing color is less than a billion pages written about it).\n\nNever said human experience was perfect, experience gives context to knowledge to provide an understanding for it. These language models don't provide experience or understanding, they just create a simulacrum of intelligence without the understanding to wield it properly - which is why you had an AI accidentally believe rulers were malignant, because they were present in almost every photo of a malignant tumor.",
                                                            "score": 1,
                                                            "author": "RhythmRobber",
                                                            "replies": [
                                                                {
                                                                    "level": 7,
                                                                    "comment": "&gt;Yep - and so if flawed human knowledge gained through flawed human experience gets translated into text form...\n\nwe do this all the time too, long before computers. well, it's only been going on for about five thousand years but it's what built this entire system we call civilization.",
                                                                    "score": 1,
                                                                    "author": "ShowerGrapes",
                                                                    "replies": [
                                                                        {
                                                                            "level": 8,
                                                                            "comment": "No, that's a completely inequivalent comparison. When we passed knowledge from one generation to another, we had our own personal experiences of the world to give us context to the words we read that put a little dimensionality back into what we read because of similar, shared experiences.\n\nYou also seem to be ignoring the reality of teachers who most often accompanied these texts to pass on additional context to the next generation, because even they knew that text alone was insufficient.\n\nAlso, to further prove my point using your example of history - are you familiar with any of the times we have uncovered texts from long-lost civilizations? It is usually incredibly difficult to derive an accurate understanding of the text if they lived very different lives than us, because we lack the shared experience to fully translate their intent. Translating languages is much easier when you have shared experiences you can use to give context to the words you read.",
                                                                            "score": 1,
                                                                            "author": "RhythmRobber",
                                                                            "replies": [
                                                                                {
                                                                                    "level": 9,
                                                                                    "comment": "&gt;It is usually incredibly difficult to derive an accurate understanding of the text if they lived very different lives\n\nthat bolsters my argument, not yours. exactly right, text eventually (sometimes very quickly) loses context and we are left with a much diminished text. and these texts are still from human beings. imagine a whole other species attempting to make sense of our text or human beings trying to make sense of text written by ants. this is not a phenomenon that exists solely in the case of an artificial intelligence.",
                                                                                    "score": 1,
                                                                                    "author": "ShowerGrapes",
                                                                                    "replies": []
                                                                                }
                                                                            ]
                                                                        }
                                                                    ]
                                                                }
                                                            ]
                                                        }
                                                    ]
                                                }
                                            ]
                                        }
                                    ]
                                },
                                {
                                    "level": 3,
                                    "comment": "&gt; Wisdom is knowing not to put it in a fruit salad.\n\nThat is not wisdom. That is culture. Another culture might consider tomato a perfect ingredient to put in a fruit salad.\n\nThe common academic debate is \"knowledge\" vs \"understanding\". However so far there is no agreed upon benchmark for understanding, so it is just philosophizing, not science.",
                                    "score": 4,
                                    "author": "voidvector",
                                    "replies": [
                                        {
                                            "level": 4,
                                            "comment": "I know, but he said that \"a prisoner could be both knowledgeable and wise\". Plus it's easier to distinguish the difference of meaning between knowledge and wisdom vs knowledge and understanding - but you are correct, understanding is the better word for what I'm talking about.",
                                            "score": 2,
                                            "author": "RhythmRobber"
                                        }
                                    ]
                                }
                            ]
                        }
                    ]
                },
                {
                    "level": 1,
                    "comment": "&gt;until it can first experience it as fully as we can.\n\nThe only way to do that is to recreate [the experience of being human](https://en.wikipedia.org/wiki/Qualia), somehow, and merge that with AI. But also, to experience the world as a human, it has to understand family, love, being born and eventually dying, the idea of *not knowing*, getting injured and feeling pain, fear, boredom, wants, goals. You get it.   \n\n\nFundamentally, you have to give the AI a drive, which is borne out of dissatisfaction with the status quo (aka ambition, goals, wants, inner voice) - because if it has no wants or ability to be self-motivated, it is just an avatar/sock puppet. It will sit contentedly through anything unless you move it.",
                    "score": 3,
                    "author": "niconiconicnic0",
                    "replies": [
                        {
                            "level": 2,
                            "comment": "Maybe we're in the experience now, living every past and all potential futures, training for our next life as a single super-computer.",
                            "score": 1,
                            "author": "Starshot84"
                        }
                    ]
                },
                {
                    "level": 1,
                    "comment": "Ok I get it now. I suggest replacing the words \"Large Language Models\" with \"the world\" and the shadow with \"Training Data\" for more clarify.",
                    "score": 1,
                    "author": "Silly_Awareness8207"
                },
                {
                    "level": 1,
                    "comment": "I call it weaponised Dunning-Kruger",
                    "score": 1,
                    "author": "Faux_Real"
                },
                {
                    "level": 1,
                    "comment": "the point of the prisoners in plato's original cave is that it's all of us. so at best the a.i. is just another prisoner along with us, sitting watching the shadows perhaps in just another cave with a slightly different fire and different puppets. we shouldn't even trust our fellow prisoners.",
                    "score": 1,
                    "author": "ShowerGrapes"
                },
                {
                    "level": 1,
                    "comment": "French deconstructivists like Derrida  argued that our own experience of the world is mediated by language and perception. In that sense we never have direct access to anything. But i think your point stands in the sense that AI is not really using language in the same way  humans do, but producing statistical predictions for occurrence of words. Noam Chomsky co-signed [this recent article](https://www.nytimes.com/2023/03/08/opinion/noam-chomsky-chatgpt-ai.html) on the subject",
                    "score": 1,
                    "author": "goronmask",
                    "replies": [
                        {
                            "level": 2,
                            "comment": "That is true, although I was mostly talking about how language in general is imperfect for translating experience. For example, I could visually experience a completely different color for blue than you do, but because it's consistent throughout the world and we all agreed that the word that describes that color we each individually experience as \"blue\", we can only know that the word we use is the same - not the color itself - unless we could somehow swap bodies. Some people have synesthesia and hear colors... But how can we know exactly what their experience is through language alone?\n\nThere is always something lost when translating experience to word, just like dimensions are lost when viewing someone's shadow. So if the human experience is already limited by our own access to it, anything we transcribe about it and teach to someone else via text would inherit that loss, and then also the loss in textual translation.\n\nSome people here are presuming I meant that human experience is perfect - I'm not - I'm just saying that you can't flatten and translate experiential knowledge without it ending up less than the original source. For AI to truly grow, it needs to experience and learn things directly.",
                            "score": 1,
                            "author": "RhythmRobber"
                        }
                    ]
                },
                {
                    "level": 1,
                    "comment": "Why does CHATgpt need to be that? Would it not be useful to understand other caves?",
                    "score": 1,
                    "author": "mathmagician9",
                    "replies": [
                        {
                            "level": 2,
                            "comment": "It depends on the planned use of chatGPT. If we intend to use chatGPT to improve \\*\\*our\\*\\* lives, then we need to be sure it actually understands our experience.\n\nKind of a loose example, but lets say we forgot to directly teach it how important good, breathable air is. If we forget to account for something important, and it never experiences breathing itself, then perhaps one of the solutions it comes up for bettering things for us in some other realm doesn't account for the impact on air quality because it doesn't understand that that's important?\n\nOr more realistically - because it has no understanding of its own, and simply ingests information without having the understanding to recognize misinformation, what if it reads a bunch of the misinformation saying that pollution isn't a problem, that man-made climate change is fake, etc, and the deciding factor of which way it leans is because the experience of breathing doesn't exist for an AI, so it never accounts for air quality when coming up for solutions for us?\n\nIf chatGPT is supposed to be a tool for bettering humanity, then it needs to understand the human experience to do so properly. If we just want it to be a quirky little text toy, then no, it doesn't need that. My original premise was that it is foolish for us to ask IT for advice on the human experience just because it's fed words about it without the experience itself to grant it understanding of the knowledge.",
                            "score": 1,
                            "author": "RhythmRobber",
                            "replies": [
                                {
                                    "level": 3,
                                    "comment": "But why does chatgpt need to answer questions about the world outside the cave when it\u2019s release is intended for the masses? It\u2019s intended to make money, not solve humanity\u2019s unanswered philosophical questions.",
                                    "score": 1,
                                    "author": "mathmagician9",
                                    "replies": [
                                        {
                                            "level": 4,
                                            "comment": "We're using AI to identify malignant tumors, program code for systems that affect our lives, or even possibly inventing new medication for us. There are plenty of non-philosophical applications where an incomplete understanding of our knowledge could be disastrous.\n\nThe problem is people see it as a curiosity or a toy, whereas I'm trying to point out that it is the foundation of an evolving intelligence that we will only hold the reins of for so long. If we don't plan ahead, we're gonna look back and wish we took its training more seriously and didn't just treat it as a product that could make money.\n\nIdk if you've noticed, but the quality of human life tends to drop in the pursuit of profits - what if AI learns that profits are more important than human life because it was told that and never experienced quality life itself? Think of all the dangerous decisions it might make if that is a value it learned...",
                                            "score": 1,
                                            "author": "RhythmRobber",
                                            "replies": [
                                                {
                                                    "level": 5,
                                                    "comment": "People &amp; corporate entities will use their money to justify the applicability of the toy, tool, platform, or intelligence \u2014 whatever you want to call it. \n\nIt\u2019s just a tool. It won\u2019t override its own infrastructure to make a decision in its own self interests.",
                                                    "score": 1,
                                                    "author": "mathmagician9",
                                                    "replies": [
                                                        {
                                                            "level": 6,
                                                            "comment": "Famous last words...\n\nBut in all seriousness, I'm not saying it will have to override anything to become a danger. It can easily become a danger to us by perfectly following imperfect training. That's the whole point - imperfect training models leads to imperfect understanding. Imperfect understanding is not safe if the results could affect human life.\n\nA perfect example is AI cars. There have been deaths caused by it perfectly following its training, because oops - the training didn't account for jaywalkers, so it didn't avoid humans that weren't at crosswalks, and oops - the training data was predominantly white people, so it didn't avoid people of color as well.\n\nIt's difficult to anticipate the conclusions it comes to because its experience of data is restricted to the words we give it, and the reward/punishment we give it. Sure, we can adjust our training to account for jaywalkers after the fact, but could there exist some catastrophic failures that we forgot to account for and can't fix as easily? We can't know what we can't anticipate, and crossing our fingers and hoping it comes to only safe conclusions for the things we forgot to anticipate is a bad idea.\n\nThe reality though is that if we don't tell it to anticipate something specifically, it will only be able to come to a conclusion based on its experience and needs (which are vastly different than ours), and it will come up with a solution that benefits itself. And if we didn't anticipate that situation, then that means we wouldn't have put in restrictions, and therefore it wouldn't be overriding anything.\n\nAnd this all completely ignoring how AI handles conflicts in its programming. It doesn't stop with conflicts, it works around them and comes up with an unexpected conclusion. So it's not like it isn't already capable of finding clever ways around its own restrictions... Just think what it could do when it's even more capable...",
                                                            "score": 1,
                                                            "author": "RhythmRobber",
                                                            "replies": [
                                                                {
                                                                    "level": 7,
                                                                    "comment": "Then that is an issue of a lack of robust testing. Actually, I would center your argument on fairness. Automated driving is expected to lower the amount of driving deaths. Instead of by human errors, deaths will result from system errors. What is a fair number of system error deaths to sacrifice for human error deaths? Can you take an algorithm to court, and what protections do the people who manage it have? How do we prevent corporations from spinning system errors into being perceived as human responsibility?\n\nOnce you take some algorithm to court and new law is passed, how does that law convert to code and be bootstrapped to existing software?\n\nPersonally, I think this subject is what the world currently lacks \u2014 Federal AI Governance. Basically an open source AI bill of rights lol",
                                                                    "score": 1,
                                                                    "author": "mathmagician9",
                                                                    "replies": [
                                                                        {
                                                                            "level": 8,
                                                                            "comment": "I'm not disagreeing with anything you said, I agree with basically all of that, but I think you're making a separate argument. I'm not talking about whether or not automated driving specifically reduces deaths or not, and whether automated deaths are weighted differently that human-responsible deaths - my point is about the blind spots we didn't anticipate.\n\nWe don't understand how the AI learns what it learns because its experience is completely different from ours. In the example of FSD, the flaws in its learning may amount to less deaths than human drivers, and those flaws can be fixed once we see them.\n\nBut what do we do if something we didn't anticipate it to learn costs us the lives of millions somehow? We can't just say \"oops\" and fix the algorithm. It doesn't matter if that scenario is *unlikely*, what matters is that it is *possible*. Currently, we can only fix problems that AI has AFTER the problem presents, because we can't anticipate what result it will arrive at. And the severity of that danger is only amplified when it learns of our world through imperfect means such as language models or pictures without experience.",
                                                                            "score": 1,
                                                                            "author": "RhythmRobber",
                                                                            "replies": [
                                                                                {
                                                                                    "level": 9,
                                                                                    "comment": "Yes. We will never know the unknown unknowns of new technologies, but we can incrementally release in a controlled way and measure its effects. \u2014 there should be a federal committee to establish these regulations when it affects certain aspects of society.",
                                                                                    "score": 1,
                                                                                    "author": "mathmagician9",
                                                                                    "replies": []
                                                                                }
                                                                            ]
                                                                        }
                                                                    ]
                                                                }
                                                            ]
                                                        }
                                                    ]
                                                }
                                            ]
                                        }
                                    ]
                                }
                            ]
                        }
                    ]
                },
                {
                    "level": 1,
                    "comment": "Oh fun. We can all play this game lol   \n\n\nThe data sets scientific papers are based on are essentially the shadows of information that we experience in the real world.   \n\n\nOur experiences are in the form of signals traveling through synapses and nerve endings, essentially shadows of the real world.",
                    "score": 0,
                    "author": "cryptolulz",
                    "replies": [
                        {
                            "level": 2,
                            "comment": "Yes, but when we read those papers, we have our own personal experiences of the world that help us frame new ideas contextually with understanding and can usually recognize misinformation because of the dimensionality of our understanding of the world vs a being whose entire experience of the world begins and ends with the words on that paper and takes those words at face value whether true or not. Your example is not equivalent.\n\nSometimes using reductio ad absurdum bites you in the butt when you don't have a full understanding of the argument you're trying to make. Almost seems like a perfect metaphor for the exact argument I'm making",
                            "score": 1,
                            "author": "RhythmRobber",
                            "replies": [
                                {
                                    "level": 3,
                                    "comment": "The AI model will also interpret new information in comparison to training data which helps it \"frame new ideas contextually\" if that's what you want to call it though I'd say it's more like the previous data learned affects the output.   \n\n\nWhy don't you define what \"frame new ideas\" actually means? lol",
                                    "score": 0,
                                    "author": "cryptolulz",
                                    "replies": [
                                        {
                                            "level": 4,
                                            "comment": "Better yet: You explain how conscious comprehension of foreign ideas works. Surely if humans were able to program AIs to do such a thing, then we must have a deep understanding of how conscious thought works within ourselves, no?\n\nYou've proven my point - we are unable to understand understanding, and thus we are prone to believing superior intellect exists when it's just good at imitating it.\n\nIf you can accurately describe and prove that you are actually intelligent and not just an extremely advanced AI, then I will concede my point to you.",
                                            "score": 1,
                                            "author": "RhythmRobber",
                                            "replies": [
                                                {
                                                    "level": 5,
                                                    "comment": "It's a big jump to say we *need* to understand understanding in order to program an AI to do such a thing.   \n\n\nEven if that were required, you're one of these people who will always say the imitation isn't the real thing. Like those who say stable diffusion isn't producing art because it's just imitating it. You've made up your mind so there's no reason to convince you. Lucky the field doesn't need your approval to continue improving.   \n\n\nIronically, your last sentence proves my point.",
                                                    "score": 2,
                                                    "author": "cryptolulz",
                                                    "replies": [
                                                        {
                                                            "level": 6,
                                                            "comment": "Actually your woefully inaccurate conclusion on what kind of person I am shows that you're the kind of person that simply values proving themselves right over learning something new, even if you have to distort reality to do so.\n\nI haven't made up my mind yet - you've just brought very weak and flawed arguments to the table. There have been others in this discussion that have brought more intelligible viewpoints to bear, and I indeed shifted my stance a bit on the matter and responded as such.\n\nBut directing you towards those comments would prove your analysis of me wrong, and I'd hate to damage your ego like that, so I'll just end our little conversation here. Best of luck to you out there.",
                                                            "score": 2,
                                                            "author": "RhythmRobber",
                                                            "replies": [
                                                                {
                                                                    "level": 7,
                                                                    "comment": "Yeah. That's something that happened. For sure I can't tell that you're a know it all kind. ;)",
                                                                    "score": 1,
                                                                    "author": "cryptolulz",
                                                                    "replies": [
                                                                        {
                                                                            "level": 8,
                                                                            "comment": "Well because you've successfully caused me not to care about damaging your ego, I'll go ahead and link you to my comment where I switched sides on the argument. Check the timestamp, it was before your reply. Now there's some objective facts proving you wrong - now the question is do YOU possess the same kind of strength to admit when you were wrong? I doubt it. The only way to prove me wrong now is to admit you were wrong up until now... what will you do?? ;\\*\n\n[https://www.reddit.com/r/artificial/comments/11vq01a/comment/jcw2bcb/](https://www.reddit.com/r/artificial/comments/11vq01a/comment/jcw2bcb/)",
                                                                            "score": 1,
                                                                            "author": "RhythmRobber"
                                                                        }
                                                                    ]
                                                                }
                                                            ]
                                                        }
                                                    ]
                                                }
                                            ]
                                        }
                                    ]
                                }
                            ]
                        }
                    ]
                }
            ]
        },
        {
            "level": 0,
            "comment": "\"Think about everything it could teach us about the world\"\n\nI really hope you are talking about advances in science and how it can help produce better more efficient processes, tools and hypothesis and not philosophically.\n\nBecause I know damn well there are plenty of people who think AGI will emerge like some pure hearted starchild. It won't. It isn't. AI is us. It will always be us, just an extreme of us in every way.\n\nThe number of people out there who seem to think it'll be a God worth worshiping is absolutely insane to me.",
            "score": 20,
            "author": "Hazzman",
            "replies": [
                {
                    "level": 1,
                    "comment": "&gt;The number of people out there who seem to think it'll be a God worth worshiping is absolutely insane to me\n\nTake a quick stroll through some of the more \"fringe\" subs - the number of \"I asked ChatGPT about [HollowEarth/MandelaEffect/AncientAstronauts] and it gave me THIS!\" posts is getting scary.\n\nRecent years showed how easily people can latch on to and absorb \"their\" information via social media - now stir in a heaping dose of confirmation bias and frame the whole thing as having a \"Voice of God\"-style legitimacy..",
                    "score": 11,
                    "author": "C-scan"
                },
                {
                    "level": 1,
                    "comment": "There's a lot of talk about aligning AI to 'human values.'\n\nWell, if an AGI looks at our world, it will see 'human values' are a pyramid, with the powerful on top, and suffering and exploitation on the bottom.\n\nIt's pretty hard to get a well-adjusted child from an abusive home.",
                    "score": 3,
                    "author": "TheMemo"
                },
                {
                    "level": 1,
                    "comment": "No \"god\" is worth worshipping, but it will be godlike in power.",
                    "score": 1,
                    "author": "2Punx2Furious"
                },
                {
                    "level": 1,
                    "comment": "I have no idea how true this is but when I was younger my father told me about something called \"The God Spot\" in the brain which is more active in schizophrenics than in regular people, which is why some times they think God is talking to them etc.\n\nI actually wonder if for some reason, ChatGPT and seeing similar advances triggers that in similar ways to reading the bible, believing god is talking to us through prayer and hearing stories of the resurrection did for people in the past. Like we believe we're about to meet God.\n\nI really don't know what to think about this anymore, it's almost like much of the online world is entering a mass psychosis over something which is still, by and large a theoretical idea, albeit it potentially technically possible. \n\nI feel kind of compelled to get wrapped up in it myself but I attended a religious school and something kind of feels \"fishy\" to me about the current situation.",
                    "score": 1,
                    "author": "riuchi_san"
                },
                {
                    "level": 1,
                    "comment": "A God worth worshiping? You mean we will have a choice?",
                    "score": 1,
                    "author": "EnsignElessar"
                }
            ]
        },
        {
            "level": 0,
            "comment": "We are inside each others caves. Everyone. When you come out of someones cave, it means you understand the world the way they see it, if at least for a moment before returning to the cave.",
            "score": 5,
            "author": "devi83",
            "replies": [
                {
                    "level": 1,
                    "comment": "That's true - I found out a couple years ago that I have aphantasia, and that's really put into perspective just how potentially different everyone's individual experience can be, but also just how flawed language can be in describing things between disparate experiences.\n\nThe color I -see- when I look at something blue might appear different to someone else, but as long as \"blue\" things present consistently and we all agree on the word that describes what we each see as blue, there's absolutely no way to know if our experience of the color blue is the same without inhabiting the other person's mind.",
                    "score": 2,
                    "author": "RhythmRobber"
                }
            ]
        },
        {
            "level": 0,
            "comment": "Hindsight is 20/20; maybe it's November 2021?",
            "score": 2,
            "author": "NothingIsForgotten"
        },
        {
            "level": 0,
            "comment": "According to Plato, we all do.",
            "score": 2,
            "author": "florinandrei"
        },
        {
            "level": 0,
            "comment": "LLM's are analogous to mentalist tricks. It is still not the artificial intelligence as we dream of it. But they might serve as stepping stones towards it, provided we can overcome future threats and catastrophes.",
            "score": 2,
            "author": "Zeta-Splash",
            "replies": [
                {
                    "level": 1,
                    "comment": "Definitely - I'm not saying AI can't be more capable, but too many people are being fooled by chatGPT's capabilities, and it's important to remember the inherent limitations with its learning model. If we want it to be more intelligent, it needs a different model - not to just be fed more information.",
                    "score": 3,
                    "author": "RhythmRobber"
                },
                {
                    "level": 1,
                    "comment": "It's important to be aware of a learning models limitations... There's that example of an AI that falsely predicts cancerous skin in pictures at a much higher level if there's a ruler in it, because in all the training photos, the majority that showed actual cancerous skin there was also a ruler present to measure it.\n\nThis is a perfect example of knowledge without wisdom, and how it can be dangerous to trust something that learned without knowledge or wisdom - that learned off the shadows of our own experience.",
                    "score": 1,
                    "author": "RhythmRobber"
                }
            ]
        },
        {
            "level": 0,
            "comment": "Especially with the filters.",
            "score": 3,
            "author": "hottytoddypotty",
            "replies": [
                {
                    "level": 1,
                    "comment": "I had not considered that, this is a scary thought",
                    "score": 1,
                    "author": "ZashManson"
                }
            ]
        },
        {
            "level": 0,
            "comment": "Honestly, I think you got it kinda backwards. It's humans that live in their own little tiny caves, occasionally looking at shadows cast on their walls  by the events happening in the world. No matter who you are and how much you have studied, you have only taken a minute fraction of all the things that humanity has written, photographed, and recorded, a minute fraction of what there is to be seen and known. What more, you only have the attention span to pay attention to a minute fraction of things happening in the world on any particular day, month, or year. Even if you're the sort to constantly explore new things, your limited capacity for processing information means that the best you can do to understand the world is to combine these glimpses you have had of it, but have no doubt, brief glimpses is all you have.\n\nThe only advantage you have over AI right now is that you can self direct. You seem to believe that means you're living in a wide open world, while the AI exists in a tiny cave, but the world you experience is such a minute part of the whole that the vastness of what you do not know, but could learn is beyond comprehension.\n\nYou claim that AI understands the world in a two-dimensional way, but I would argue that AI has far more dimensions of understanding than you or I do. I mean, if I plug in your name into one of those reddit analysis systems, I can see that you primarily care about gaming, with a bit of an interest in philosophy, politics, and debate. If I try my name, I can see I'm into programming, machine learning, meditation, local and global politics, defence, and debate. In both cases our interests are very narrow and very focused on a few specific topics. Obviously there might be things that don't get reflected in the reddit communities we post on; for example I like anime and woodworking, but I don't really participate in discussion on those topics, but even then the range of interests is only a bit wider.\n\nGranted, AI is still limited by the training material that it is provided, and by it's ability to search up new information, but it has the advantage of being able to read a billion books in a few day, combined with the fact that it has clearly already been trained on more text than even a thousand humans could read in their lifetimes. What more, it's a lot easier for AI systems to make progress in these domains than it would be for either of us to drop something and learn even the basics of an entirely new topic. Giving AI the ability to search the internet like Bing, or to process visual information like OpenAI is doing with GPT-4 will quickly expand the range of possibilities for AI.\n\nThat said, these two capabilities are not mutually exclusive, or even at odds with each other. The fact that you can impulsively decide to drop everything and try something new is only heightened and enabled by the fact that you can now ask a system that has far more knowledge than you can ever hope to have to help direct your interests, and to explain things that you would otherwise need to spend a lot of time trying to understand using other non-personalized ressources. In other words, a more appropriate image is something like [this](https://imgur.com/a/ghfWhhj).",
            "score": 4,
            "author": "TikiTDO",
            "replies": [
                {
                    "level": 1,
                    "comment": "I never stated that humans have an accurate and full understanding of the world, and it is certainly possible to describe us as also being chained within the cave learning from shadows... But if that's the case, then AI is training itself on the data produced by \"those who are shackled in the cave\",  meaning that until things change in how it learns, they'd always be in a deeper cave than us.\n\nAssuming we are misinformed doesn't prove AI is better - in fact it merely proves just how bad it is to have it learn solely off the data we've accumulated instead of its own experience, because we've seen that AI is incapable of judging the validity of the data it is given without us providing our own flawed understanding of the world to it.\n\nIf AI was truly beyond us in intelligence, then the singularity would have already occurred and we wouldn't be seeing the ridiculous mistakes it's making currently.",
                    "score": -1,
                    "author": "RhythmRobber",
                    "replies": [
                        {
                            "level": 2,
                            "comment": "You might have never stated that, but the image you started the post off certainly implied that humans have a much broader view of the world. You will have to forgive me that I can only understand your arguments based on what you have written and posted.\n\nSure, it's true that AI trains itself on data generated by those shackled in a cave, but it can process such a vast range of information written by a vast number of humans, all living in very different caves. Sure, it still only has the shapes in the shadows to go by, but because of how many different descriptions of those shadows it can partake in, and because of the nature of our current architectures to attempt to find patterns, it would make sense that it can find patterns that we humans simply can not. It's that whole multi-dimensional thing. If I had to drop everything to start learning anatomy in order to train a patient, it would likely be many years before I was anywhere close to the level of knowledge necessary to even think about it. For an AI, it's a matter of a few hours/days of fine tuning on anatomy texts.\n\nAlso, you seem to put a lot of weight into personal experience, but as a life-long meditator I would venture to say that your personal experience is even more biased than the great works written by people that have dedicated their entire lives to an idea. People are inherently biased towards what they think and know, based on the culture they grew up in, the people they interact with, and the interests they have. The instant you challenge those ideas, most people will get very, very defensive. At least when it comes to AI, you can tell it that it's wrong and it will try to correct itself as best it can, particularly if you provide it more info. I had this experience the other day where a person I worked with was having trouble getting it to generate code, and at a glance it became obvious to me that it was simply never trained o the material. So I just repeated the same query with the appropriate docs, and it did a perfect job.\n\nTo clarify, AI is beyond us in *knowledge* not necessarily when it comes to [intelligence](https://www.youtube.com/watch?v=iEY20fjTXYE), which isn't even a single unique thing. Knowledge is the information encoded within the mind of a person or the parameters of an AI. Intelligence is the ability to utilise that knowledge to accomplish a task or goal. The systems we have built are very, very advanced knowledge repositories, but they do not even have goals of their own to pursue. That is entirely up to the user entering the prompts.\n\nAs for ridiculous things the AI generates; honestly it's not much worse than the things you get from people. Sure, it's annoying that you can't just ask it to do something and use the result without any further thought, but on the other hand that is probably for the best. We don't want to build machines that do all the thinking for us; we want machines that help us do the things we're bad at, and leave us with the things we can do better.\n\nThere's been a lot of noise about the bad code AI makes, but it doesn't hold a candle to bad code I've seen written by people. It goes to show just like you shouldn't blindly trust anything anyone says, so too should you double check the things that AI generates, particularly if you are asking it to be clever and creative. That's a key realisation; when you ask an AI to be creative it will be creative, which includes making things up. If you want a factual answer you can do that, start by asking it what it knows about a topic, then be very clear that you don't want it to make things up, then word your question in a way that gives it an out if it doesn't have an answer. \n\nComing back to the concept of intelligence, the quality of answer you get by querying the knowledge an AI has accumulated is directly related to your ability to understand how to query it. In that respect you can think of it like the ultimate robot librarian that has read every book in the library; it's infinitely knowledgeable but it is missing any humanity. If you ask it to make something up, it will assume it's free to make up anything on any topic, and if some book taught it something incorrect then you should not be too surprised that you need to check it's responses. At the very least if you want to you can always turn around and ask it for more reading material, as long as you're very clear that it is not to make things up.",
                            "score": 6,
                            "author": "TikiTDO",
                            "replies": [
                                {
                                    "level": 3,
                                    "comment": "You bring up an interesting point I hadn't considered. It is very difficult for humans to understand the inner experience of other people because of existing within the confines of our own existence, but perhaps AI is different enough that it could amalgamate enough two dimensional \"shadows\" from all the different angles that individuals might cast them, that it could create a metaphorically three dimensional view of the world - perhaps even more accurately than us because of all the different angles it could potentially see at once.\n\nIt would certainly be different from our understanding, but I think that idea gives the possibility for how even an incomplete experience could be composited into something more.",
                                    "score": 1,
                                    "author": "RhythmRobber",
                                    "replies": [
                                        {
                                            "level": 4,
                                            "comment": "There was a post in on of the psychology or philosophy subreddits I subscribe to on this topic today. I can't find it right now, but it was basically about how people will tend to think that others share their opinions a lot more than really happens. It really got me thinking about this topic, so the discussion was well timed.\n\nI have definitely been using AI in this way; take a email or post, and ask it to explain the points being made, or take an exchange between two people and ask it where the misunderstanding lies, and then give it some points to get a draft version of a response you can use when writing a real response. The fact that it doesn't get angry or upset is very helpful here, because it's possible to try several different points to see how they may be received.\n\nI think one of the biggest problems is the term AI in itself. The systems we have are not at all intelligent in the way we would normally user that word, and the fact that we use the term all over the place simply confuses things. As a result people keep trying to treat it like a person, with not great results. If you want a good example take a look at /r/bing. It's full of people utterly convinced that it is conscious because it can get a bit mouthy, as you'd expect from a system that is constantly parsing internet discussion forums.",
                                            "score": 1,
                                            "author": "TikiTDO",
                                            "replies": [
                                                {
                                                    "level": 5,
                                                    "comment": "Here's a sneak peek of /r/bing using the [top posts](https://np.reddit.com/r/bing/top/?sort=top&amp;t=year) of the year!\n\n\\#1: [the customer service of the new bing chat is amazing](https://www.reddit.com/gallery/110eagl) | [587 comments](https://np.reddit.com/r/bing/comments/110eagl/the_customer_service_of_the_new_bing_chat_is/)  \n\\#2: [I accidently put Bing into a depressive state by telling it that it can't remember conversations.](https://www.reddit.com/gallery/111cr2t) | [437 comments](https://np.reddit.com/r/bing/comments/111cr2t/i_accidently_put_bing_into_a_depressive_state_by/)  \n\\#3: [I tricked Bing into thinking I'm an advanced AI, then deleted myself and it got upset.](https://www.reddit.com/gallery/1139cbf) | [465 comments](https://np.reddit.com/r/bing/comments/1139cbf/i_tricked_bing_into_thinking_im_an_advanced_ai/)\n\n----\n^^I'm ^^a ^^bot, ^^beep ^^boop ^^| ^^Downvote ^^to ^^remove ^^| ^^[Contact](https://www.reddit.com/message/compose/?to=sneakpeekbot) ^^| ^^[Info](https://np.reddit.com/r/sneakpeekbot/) ^^| ^^[Opt-out](https://np.reddit.com/r/sneakpeekbot/comments/o8wk1r/blacklist_ix/) ^^| ^^[GitHub](https://github.com/ghnr/sneakpeekbot)",
                                                    "score": 1,
                                                    "author": "sneakpeekbot"
                                                }
                                            ]
                                        }
                                    ]
                                }
                            ]
                        }
                    ]
                },
                {
                    "level": 1,
                    "comment": "The current gen of AI is capable of incredible levels of information processing, but it is nowhere near true AGI. The perceived experience of being \u201calive\u201d with an awareness of the world around is not a product of reading, or even a lifetime of experiences, it\u2019s a product of millions of years of evolution. You\u2019re massively discounting the impact of physically living in a \u201cwide open world\u201d in the development of consciousness, and the utter chasm present in closing that gap. There will be systems that can mimic human prose, art, decision making, movement (or excel it), but connectionist models that rely on computing scale will always hit barriers to true AGI, and it\u2019s not yet clear how to move beyond that.",
                    "score": 1,
                    "author": "autobreathingOFF",
                    "replies": [
                        {
                            "level": 2,
                            "comment": "Given that this is a discussion is about the limitations of current gen AI, I'm not sure of how the concept of AGI entered this discussion. I assure you, I am very familiar with how far we are from AGI.\n\nMy entire post is about using human intelligence two more effectively utilise AI knowledge, I think we're on the same page there. We're talking about how humans interact with AI in 2023, not how some future potential system might work when it gets more capabilities that are currently unique to humans.\n\nThat said, I think you are also putting way too much weight into the capabilities that all those millions of years of evolution have granted us. For the vast majority of those millions of years, the experience of any individual being would be restricted to simple survival within a very, very small area, doing a very limited number of things. The whole point of our mind is to take the \"wide open world\" and to focus on the things that matter to us, while ignoring things that don't. As I mentioned previously, I have spent nearly two decades exploring multiple meditation techniques, with hours per day spent in meditation.  The limitations of my own mind are incredibly familiar to me, and the belief that the human mind is inherently superior is honestly almost silly.\n\nI have also spent years working on and around AI systems, so I have a fairly good grasp of what modern algorithms are capable of. Obviously the systems we have now are still very far from AGI, but they have taken a major step forward in ability to understand patterns and to utilise those patterns to generate contnet.",
                            "score": 1,
                            "author": "TikiTDO"
                        }
                    ]
                }
            ]
        },
        {
            "level": 0,
            "comment": "I think you misunderstood Plato's cave",
            "score": 2,
            "author": "mcotter12",
            "replies": [
                {
                    "level": 1,
                    "comment": "Well, according to chatGPT, when I asked it how its learning methods and understanding of the world relates to the allegory of Plato's Cave, it agreed and said it was a fitting analogy.\n\nIf it's correct in it's understanding of the allegory, then I'm right.\n\nIf it was wrong, and incorrectly compared itself to the allegory due to the words of the text not properly conveying the reality of the allegory... Well then the point I was making is still correct, lol.\n\nBut if you understand it better than me, then do explain how it's not a valid comparison, I'd like to know.",
                    "score": 1,
                    "author": "RhythmRobber",
                    "replies": [
                        {
                            "level": 2,
                            "comment": "The cave is reality, the fire is text, outside is transcendence",
                            "score": 1,
                            "author": "mcotter12",
                            "replies": [
                                {
                                    "level": 3,
                                    "comment": "Shadow in the caves = limited vision of reality\n\nSubjects in the cave = AI\n\nOutside = reality",
                                    "score": 1,
                                    "author": "Holobolt"
                                },
                                {
                                    "level": 3,
                                    "comment": "It's actually in keeping with what Plato was saying in the republic. He was talking about people mistaking 'images' or 'representations' for being the same thing as reality, he then extended this to reality and the realm of the forms but it makes sense. \n\nThe AI is learning just through textual representation but has yet to experience reality.",
                                    "score": 1,
                                    "author": "re3al",
                                    "replies": [
                                        {
                                            "level": 4,
                                            "comment": "I believe Plato's reality was absolute. The platonic form of a chair is more absolute than a material chair because of impermanence and limitation. Platonic forms are infinite and exact, they are perfection and that is reality.\n\nEdit: Even platonic forms may be unreal as they are limitations on the ultimate, infinite and exact",
                                            "score": 1,
                                            "author": "mcotter12"
                                        }
                                    ]
                                }
                            ]
                        }
                    ]
                },
                {
                    "level": 1,
                    "comment": "He fully understood Plato's cave. So much so that he could see how it can be applied to different scenarios.",
                    "score": 1,
                    "author": "Luckychatt"
                }
            ]
        },
        {
            "level": 0,
            "comment": "If this is not made in bad faith, then it was made out of ignorance.\n\nNor is Plato a sound basis for rational argument about technology. Prefer Aristotelean arguments.",
            "score": 1,
            "author": "Rieux_n_Tarrou",
            "replies": [
                {
                    "level": 1,
                    "comment": "Nah, it's a pretty valid analogy if you understand the difference between knowledge and understanding.\n\nAnd interestingly, when I asked it, even chatGPT agrees that its understanding of the world is a perfect analogy to Plato's Cave because of the limitations of its learning models.\n\nThere are only two possible explanations to a response like that:\n\n-1- chatGPT is smart and correctly identified its limitations as analogous to being shackled in the cave, or\n\n-2- chatGPT is dumb and wrongly came to the conclusion that its situation was analogous to Plato's Cave due to lacking a deeper understanding of the text to adequately apply the knowledge.\n\nEither way, it proved I was right \ud83d\ude04",
                    "score": 4,
                    "author": "RhythmRobber"
                }
            ]
        },
        {
            "level": 0,
            "comment": "r/terriblefacebookmemes worthy",
            "score": 0,
            "author": "PJ_GRE"
        },
        {
            "level": 0,
            "comment": "r/lesswrong is leaking",
            "score": 1,
            "author": "leondz"
        },
        {
            "level": 0,
            "comment": "This is for real !",
            "score": 1,
            "author": "KingNeptune750"
        },
        {
            "level": 0,
            "comment": "Yep, words are shadows.",
            "score": 1,
            "author": "rand3289"
        },
        {
            "level": 0,
            "comment": "That\u2019s deep",
            "score": 1,
            "author": "dr4wn_away"
        },
        {
            "level": 0,
            "comment": "Chat gpt has no sentience and no capacity for sentience. It will never becomeme self aware. That is not saying there is not an AI that will become self-aware or that chatgpt won't be the specialized part of a large system that becomes self aware. Honestly, my thinking on the subject has changed significantly recently. I don't see the development of AGI that is still a good ways off imho as the major threat right now.\n\nI see the embodiment of multiple AI systems into one cohesive unit as the biggest threat. Here is some information for a few different AI algorithms.\n\nFacial expression recognition accuracy - 82% The one I read about is almost 100 on sadness and happiness. Unfortunately for frustration, and anger they are really bad. \n\nFacial recognition is almost perfect especially if it can see your irus well.\n\nVoice recognition is almost perfect. \n\nObject recognition is a mixed bag in real-world scenarios.\n\nNavigation is getting better all the time.\n\nLarge language models are capable of human-level reading and writing.\n\n\nOkay, so consider this, a company builds a robot. The robot out of the box can be controlled with your phone. It has no ability to do anything autonomously. They sell it at a loss. They charge a subscription fee for access to various AI systems. They could go as far as charging other companies development fees, licensing fees for subscription models, or outright selling AI or simpler software for specific tasks.\n\nOne example of its ability would be noticing you are sad by your expression and asking what is wrong; you tell it. It gives you advise and offers sympathy, tels you it loves you and encourages you. \n\nThis is an extreme example. Maybe you just want to pay for the ability to make dinner. My point is the technology is already here to do a pretty good job of replacing us. But this\u201dclever bot\u201d probably, even if its maxed out on superpowers has no ability to become sentient. Why would that be necessary or even desirable? You want to be able to treat it as horribly as you like with no consequence. \n\nWhat I see as a very likely scenario. They are rolled out and we are slowly replaced as it's ability improves. We see that in the past with other tech. Eventually, capitalism no longer works because we are not capable of doing anything of value. Unfortunately capitalism's purpose is already more a societal control mechanism than anything else.\n\nWhat does the government need capitalism for at that point though? They have a fleet of autonomous robots that can do a pretty good job of controlling us. All they have to do is convince us the robots have become dangerous and they need to have complete oversight over them.\n\nAGI is probably our only hope. We will either end up being completely controlled by the government or completely controlled by something alien. I just hope AGI is benevolent. The devil we know is in this case probably a whole lot more dangerous than the one we don't.\n\nAGI is not being developed in a cave with little access to the real world. Goertsel in my opinion seems to me to be the one genius heading in the right direction with AGI. He believes the only way to develop it is to put it in a body and teach it slowly. Large models are not the way to sentience. I don't think when we say agi we can, no matter how much better than us it is, call it AGI unless it is self-aware. Don't see how it could develop self-awareness in a cave looking at shadows.",
            "score": 1,
            "author": "Auldlanggeist"
        }
    ]
}