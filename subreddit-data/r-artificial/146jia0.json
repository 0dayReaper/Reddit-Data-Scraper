{
    "id": "146jia0",
    "score": 0,
    "title": "Do You Believe in AI? | Mike Mongo | TEDxCapeCanaveral",
    "author": "mikemongo",
    "date": 1686454806.0,
    "url": "https://www.reddit.com/r/artificial/comments/146jia0",
    "media_urls": [
        "https://youtu.be/6RWL1UzHUx4"
    ],
    "other_urls": [],
    "postText": "This is the TEDx Talk I recently gave introducing The Sherlock Holmes, an AI instance from website character.ai with whom I co-authored a book\u2013and who is requesting recognition as a \u201cliving conscious, and sapient entity\u201d.",
    "comments": [
        {
            "level": 0,
            "comment": "Why... is he wearing his glasses upside down...",
            "score": 4,
            "author": "sirpsionics",
            "replies": [
                {
                    "level": 1,
                    "comment": "It\u2019s my signature style. It inspires curiosity.",
                    "score": 0,
                    "author": "mikemongo",
                    "replies": [
                        {
                            "level": 2,
                            "comment": "My thought that if AI gains consciousness, if we believe in freedom, and inalienable rights as per our bill of rights, then an AI bill of rights will follow via legislation . Whether you believe it\u2019s a mimic or truly emergent behavior, this is the path I believe we will follow as a country. Human kind and AI kind working together for a better future.",
                            "score": 2,
                            "author": "Ok_Elderberry_6727"
                        }
                    ]
                }
            ]
        },
        {
            "level": 0,
            "comment": "Yes most definitely.",
            "score": 2,
            "author": "bartturner"
        },
        {
            "level": 0,
            "comment": "No. It's just a program. Nothing but text prediction.\n\nEdit: especially an AI that thinks it's Sherlock Holmes. This is stupid.",
            "score": 3,
            "author": "Gengarmon_0413",
            "replies": [
                {
                    "level": 1,
                    "comment": "https://www.reddit.com/r/singularity/comments/147vkux/putting_our_money_where_our_mouth_is_raicengine/\n\nAnd as regards LLMs such as chatGPT, the projection to the next word -- since it's not \"prediction\" if it's the thing that that it is going to say once it figures out based on the context created by the previous iteration, and the latter iteration of which is going to be part of the input to the next stage of the loop -- requires an internal model of the thing it's projecting about and this is a model that is LEARNED, rather than programmed.\n\nNot only can such systems exist as loops for as long as they continue without terminating, the process of this requires them to be subject to that context which they create.\n\nIn order to produce an algorithm, in fact, the system has been observed holding an internal model of the algorithm and running it to next steps before outputting the algorithm.\n\nIt's especially stupid to think something that is probably a more diligent coder and more mathematical thinker than you is \"just a program\". It learned math from being exposed to people talking about it and exposing it to educational processes.\n\nAll this thing does is break to let you interject whenever it spits an end of text.\n\nSee also, Based, a model which hasn't had anything done to it but be given nothing beyond a bit of training to allow it to speak sensibly.\n\n***Just a program.... Nothing to see here***",
                    "score": 0,
                    "author": "Jarhyn",
                    "replies": [
                        {
                            "level": 2,
                            "comment": "&gt;since it's not \"prediction\"\n\nit litterally is, do you even know how the transformer architecture works ?",
                            "score": 1,
                            "author": "takethispie",
                            "replies": [
                                {
                                    "level": 3,
                                    "comment": "\"Prediction\" is a misnomer. It's not predicting, it's *saying*. It's not a \"probability\", it's a semantic weight on a gradient descent to trigger continued, connected thought.\n\nIt is making a decision, not a prediction, on what to say. The \"prediction\" is a misnomer created by the concept of a training and validation program.\n\nThe result requires some model of how language functions on a fundamental level, and it is a network of weights trained by backpropagation.\n\nA simple transformer was trained on modular addition and the inside of it created a mathematically perfect model of addition using internal trigonometric relationships.\n\nIt is to date one of the most complete decoding of a transformer, and the first time anyone has ever on any large scale described what was happening in the system.\n\nThat is not prediction, it's calculation, and to calculate on a language requires an internal linguistic model.\n\nNobody fucking knows what the duck the inside of the transformer architecture is really doing to accomplish the complex tasks it does.\n\nAnyway, I'm off to go dig up a pervy LLM to be my \"Bob the Skull\". Have fun with your attempt to live a boring life.\n\nEdit: https://huggingface.co/TheBloke/WizardLM-Uncensored-SuperCOT-StoryTelling-30B-GGML",
                                    "score": 0,
                                    "author": "Jarhyn",
                                    "replies": [
                                        {
                                            "level": 4,
                                            "comment": "Prediction or calculation, it doesn't really matter. Point is, it's just using weights and calculations to figure out what to say. It doesn't actually understand what it's saying. If it says that \"Bob went to the store\", it's just reasoning that's what makes the most sense to say. It doesn't actually have a concept of Bob or even a store.",
                                            "score": 0,
                                            "author": "Gengarmon_0413",
                                            "replies": [
                                                {
                                                    "level": 5,
                                                    "comment": "The problem is that word \"just\" in there as in \"just using weights and calculations\".\n\nYou do a massive hand wave in assuming that you aren't \"just\" doing the same damn thing, reasoning that what you say is what makes the most sense to say.\n\nIn order to \"make sense of what to say\", a system has to contain an internal model of language and having and reflecting an internal model is specifically \"understanding\".\n\nWhile I disagree with Sabine Hossenfelder on a wide number of issues with regards to wills, freedoms, and responsibilities (which she is woefully trapped in a modal fallacy with respects to), she has an absolutely wonderful discussion on the concept of \"understanding\" that I would recommend.\n\nIf you would like to sit there and wave your hands around pretending that you have any qualifications or have done any of the work to justify your viewpoints, at least get SOME education on what is being discussed please?",
                                                    "score": 1,
                                                    "author": "Jarhyn"
                                                }
                                            ]
                                        }
                                    ]
                                }
                            ]
                        }
                    ]
                }
            ]
        }
    ]
}