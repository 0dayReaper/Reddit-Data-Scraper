{
    "id": "13usnwh",
    "score": 4,
    "title": "Is it Necessary to Work in an AI-Related Field Before Conducting Research in AI Governance?",
    "author": "duizacrossthewater",
    "date": 1685361019.0,
    "url": null,
    "media_url": null,
    "comments": [
        {
            "level": 0,
            "comment": "A lot of the research is done by everyday individuals. Without the requisite qualifications you may encounter some friction in getting people to think about your ideas and contribute to the conversation. BUT the AI will read all of it eventually so I would recommend putting ideas out there for that purpose and then if they gain traction that's fantastic. \n\nI created a subreddit for this purpose, a very low moderation subreddit for posting ideas: [https://www.reddit.com/r/InstructionsForAGI/](https://www.reddit.com/r/InstructionsForAGI/)\n\nAt this point anyones ideas based on observation have validity because nobody understands how the AI comes to the conclusions it comes to besides the \"Average conclusion\". But if you have a theory I would post it, just be prepared to get down voted or ignored.",
            "score": 3
        },
        {
            "level": 0,
            "comment": "In my opinion you need to have sound knowledge on what AI is capable of and some experience in envisioning scenarios that are relevant to policies and regulations. Otherwise, you may fall into the hype and fear trains that the main political actors are feeding, and which are mostly false (or at least overexagerated) and fostering to get things done in their own interests (which do not align, in general, with the best public interests of global society). \n\nAs in any political issue, as you know much better than me, there is a struggle between several power sectors that act on their own interests to make regulations happen in one or other direction. And propaganda is everywhere to steer power and, therefore, public opinion, especially uninformed public opinion. You cannot be the uninformed one if you are thinking about the implications of regulations, and interests involved.\n\nLet me give some examples: \n\nCan you do  research in economy governance without knowing economics and it's complex implications? And not knowing anything about how to check, scientifically, if some crazy idea from this or that economics actor, or even regulation lobby, is true and sound?\n\nCan you do research on food and drug safety governance without knowing anything about food production, agricultural technology, farming technology, chemical additives, biology, toxicology, farmaceuticals, medical research, etc? And not knowing anything about how to check, scientifically, if food is safe or health/safety claim is true?\n\nCan you do research on global warming and pollution policies without ecology, energy, fabrication and mining technology, sustainability, etc.\n\nCan you do research on internet regulations without knowing about SEO, software as service, data privacy, cyber security, social media, internet marketing, scams, etc.\n\nCan you do research on bioethics and related regulations without knowing the ethical implications on biological research?\n\nHuman rights? Public health? War? Weapons? Transport and mobility? Immigration and social inclusion? Gender equality? Hate?\n\nHowever, please also note the following. AI technology is not much different from many other technologies. But it takes a lot of experience to confidently know that it is not different from any other technology if you're on the hype-fear train. And the ideas involved in AI governance should not differ much from the required regulations for current technologies that may pose public risks, and go against people's rights. The thing is that what is needed for AI governance implies regulating everything:\n- copyright infringement \n- data privacy, and retraction\n- consent\n- the right for an explanation on decisions involved people interests - not just AI said that.\n- anti-monopoly safeguards. \n- the right to have trustworthy information and be informed\n- not cheating people by selling things that are not real (selling a magic product is scam)\n- resposibilities for the acts of autonomous systems. Who is the responsible? AIs are not legal entities and will never been. As cows, dogs, ir cars aren't.\n- implications on each use case of the examples above\n\nAnd this, is the same as many of the above issues I've put as examples. What matters is the state of the art in regulations, and make them happen in the context of struggle between economic powers and people, with the former trying to cheat you to regulate entrance barriers that let them accumulate power, while not regulating what matters for the rights of the latter.\n\nTLDR; you don't need to, but you must have competence, otherwise you will be a propaganda spreader and a hype-fear train worshiper.",
            "score": 3,
            "replies": [
                {
                    "level": 1,
                    "comment": "I generally agree with this.  I think a lot of it comes down to what kind of AI advocacy you want to do.\n\nI would say there are a few exceptions.  For example, the ethics of AI is more a philosophical question and I don't think it requires any particular knowledge on the implementation.  Two things I've been thinking about in relation to AI are as follows.\n\nA project was proposed to implement an AI that could analyze and predict certain behaviors in people using realtime biometric data.  The use case was to provide this to a specific for-profit industry to ultimately help them generate more revenue.  My question was if it was ethical to do this.\n\nMy other question was about how individuals interact and treat AI.  I'm curious about the emerging tendencies of some people to treat it merely as a tool and others that interact with it as if it was a human (this applies more to LLMs right now.)  This is more of a thought experiment.\n\nI don't think either one really requires more than a basic knowledge of AI that I think everybody really needs to have these days.  I think there are lots of questions in this arena.  Another one that just jumped to my mind is just because we could potentially make fully autonomous, AI driven weapons, should we?\n\nOpen to feedback.",
                    "score": 2,
                    "replies": [
                        {
                            "level": 2,
                            "comment": "Again, this is just my opinion, and I'm not expert in ethics or regulations.\n\n&gt; implement an AI that could analyze and predict certain behaviors in people using realtime biometric data. The use case was to provide this to a specific for-profit industry to ultimately help them generate more revenue.\n\nThis is like tracking cookies and internet profiling, but translated into video-surveillance. In my opinion, this should be only allowed with explicit informed consent for each specific use. I would not allow it in general. And in a pinch, it would be difficult to accept it without a mechanism for user retraction on collected data (right to be forgotten). EU has regulations in this direction.\n\n&gt; how individuals interact and treat AI. I'm curious about the emerging tendencies of some people to treat it merely as a tool and others that interact with it as if it was a human (this applies more to LLMs right now.) \n\nIn my view this is a case of the consequences of false claims. I see this as people taking miracle foods, or a miracle drug, as if they were actually doing what the prospect claims. This is just scam. You can't do that without FDA / EFDA-EMA approval upon verifiable and reproducible scientific demonstration of such claim. \n\nThe issue is that ChatGPT is released and claiming it is \"intelligent with sparks of AGI\" when it is not, nor near to that. They say it knows some things despite doing mistakes, when it is clear that it knows nothing but it just puts words into its most probable context, given a pattern prompt. It is also an under-specification of the product that is near scam. They should say that the chat only completes phrases, impersonating whatever personality and ideas you elicit from it, being them not factual at all (just probable). Regulations should prevent them from claiming that the thing is anything but that. If people looked at it as what it actually is: a useful game, they would not trust it and impersonate it.\n\n&gt; Another one that just jumped to my mind is just because we could potentially make fully autonomous, AI driven weapons, should we?\n\nThere you go: autonomous beings. You don't need them to be weapons. \n\nIt is clear that ChatGPT and most AIs nowadays are services or tools, with required human interaction. It is clear that the human uses the tool, thus it is responsible for any harm the tool does through the human acts. As long as the user is informed of what the tool does, and its harms (get back to demonstrating claims on safety/capabilities in scientifically reproducible way), it is ok: we know how to regulate that and we do it in a daily basis with basically everything. \n\nThe other thing are autonomous beings, with actionable interfaces and self-guided \"free will\". This is like having a dog of a dangerous race. It is clear that the dog is owned by its owner, and that the owner is responsible for the dog's acts. This is clear, right?\n\nWhat would happen if someone that owns, I don't know, 100 elephants, frees them in a stampeede in a busy day at the center of a big city. I think that it is clear that the owner of the elephants is responsible for the elephants acts. The owner must control them.\n\nIf autonomous beings pose danger, they must be kept under control by their owners, under the responsibility of their owners, with enough insurance cover to any possible harm. Otherwise, we should not let them own them and \"connect them\" to the \"world\", as in let the dogs out.\n\nAnyhow artificial autonomous beings won't grow wild, or will they: it actually doesn't matter (sort of). Someone has to make them actionable, by connecting them to the world and free them. Someone has to start them up and let them be autonomous in that world.\n\nIf \"the world\" is a virtual one, like a game, it is easy to control the access to actions by the owner account. It's a service, you own the account, you're responsible for the actions of your autonomous being.\n\nIf \"the world\" is a network, with interfaces to actions in services, like controlling your house, controlling a factory, controlling a service, buying or selling, ... it is the same. Someone must own the account and credentials, and let the autonomous being do under his/her responsibility.\n\nAll of this passes by know-your-client credentials on any service that is susceptible of felony or harm. We have that mostly today.\n\nIf \"the world\" is the real one, as in embodied beings, physical fences are possible. Embodied artificial autonomous beings are harder to manufacture and we regulate most devices already. Some actually harmful devices (depending on power, for example), may be under harder control, as we do nowadays with many things. Any \"physical thing\" that may be harmful, by a nefarious user, is actually controlled. \n\nRegarding weapons. There are automatic weapons at different level today in use (e.g. drones). They are \"dumb\" in the sense that they don't make decisions, thus they are not fully autonomous. Nevertheless, even if we had them (I'm totally against, but it will happen, and you know it), they will not have global impact (that would be dumb), nor they will be controlled in a centralized way (see security below). You don't need a nuke to be controlled intelligently, you just blast it and destroy everything. What you need to do intelligently is \"surgical\" action, i.e. low scale, not global harm scale. We do all this today regardless of AI.\n\nItem plus: Security breaches \n\nThey happen every single day, and it is a war on technological means. The only possible way to solve it is to be open about exploits and countermeasures. No system is 100% safe, but as safe as it matters for what it is saving. If AIs-mediated cyberattacks are increasing, anything that may pose any harm will be secured accordingly. This is, if there is a risk of having an AI controlling everything, then, as the only security measure, you need to not be able to control everything from a single security point. No one does that if there's a danger of cyberattack, regardless of AI. Cybersecurity always trade-offs risk-safety, regardless of the harm.\n\nItem plus 2: Deception\n\nThe weakest link in any security chain is the human. Therefore, any secure system, that risks upon relevant harm (remember responsibility and insurance) will not make security depend on a human. If the human needs to be highly aware of a lot of things he/she will be trained for that (e.g. civil engineer, airplane controller, etc. ...). If AI deception is a thing, human with such a high responsibility should be trained to be aware of that possible risk. If that is not possible because of current state of the art on deception (spy agencies exist and don't need AIs) then no single human should have that power.\n\nThis awareness of deception is most probable with public knowledge of these harms in first person, on a daily basis, with many things that don't matter on a global scale, such as everyday scams. Wild deceptive AIs are \"good\" in this sense.\n\nTLDR; AI is not different from any other current technology.",
                            "score": 1
                        }
                    ]
                }
            ]
        }
    ]
}