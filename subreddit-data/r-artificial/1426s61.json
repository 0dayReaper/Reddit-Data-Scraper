{
    "id": "1426s61",
    "score": 10,
    "title": "The Chinese room argument, or Why Artificial Intelligence Doesn't Really Understand Anything",
    "author": "shrmn-me",
    "date": 1686034322.0,
    "url": "https://www.reddit.com/r/artificial/comments/1426s61",
    "media_urls": [],
    "other_urls": [],
    "postText": " There was an American philosopher - John Searle: he was squinted in one eye and studied speech as a social phenomenon. In the 1980s there was a boom of discoveries in the field of artificial intelligence and, like me, John couldn't pass by and started studying it. It didn't take long for the results to come in - his \"Chinese Room\" mental experiment is still the subject of heated debate in scientific circles. Let's find out where the cat-wife is hiding, and does John deserve a bowl of rice?\n\n# Why did John explode?\n\nJohn Searle was an exponent of analytic philosophy, which, in short, is when thinking is not just free-floating, but is backed up by rigorous chains of logic, analysis of semantics, and does not run counter to common sense.\n\nEven before Chinese Room, he was known for his definition of the Indirect Speech Act.\n\nYou know, when instead of \"Give me money,\" they say, \"Can I borrow it from you?\n\nThat is, they use a questioning form instead of a request, while in fact, they don't wait for an answer to their question.\n\nThey are waiting for money. And it's better if You send it to the card, and without asking too many questions.\n\nSo, while John was digging into the language and the reasons for the human being's special love of all kinds of manipulation, **a number of important inventions in the field of Artificial Intelligence happened in the 1980s**:\n\n* The first expert systems appeared - which could model expert knowledge in different fields and use that knowledge to make decisions;\n* New neural network training algorithms were developed that formed the basis of the neural networks we have now, which threaten to take our jobs away;\n* Developed the first industrial robots - which gave a boost to modern robotics;\n* The emergence of the first computer vision systems - those that are now easily found by photo where to buy your favorite mug .\n\nThis number of discoveries, as is often the case, generates a huge amount of talk, professional and not so professional, in kitchens and conferences, but all about the same thing:\n\n&gt;Are we on the verge of creating that very, scary, yet delightful, artificial intelligence? And will it have consciousness?\n\nConversations in kitchens did not bother Searle too much, but the scientist could not go quietly past his colleagues' concerns:\n\n**In 1977, Roger Schenk and Co.** (we'll skip the details) **developed a program designed to mimic the human ability to understand stories.**\n\nIt was based on the assumption that if people understood stories, they could answer questions about those stories.\n\n*\"So, for example, imagine being given the following story: \"A man went into a restaurant and ordered a hamburger. When the hamburger was served, it turned out to be burnt, and the man left the restaurant in a rage without paying for the hamburger or leaving a tip.\" And so if you're asked: \"Did the man eat the hamburger?\" you will probably answer, \"No, he didn't.\" Likewise, if you are presented with the following story: \"A man went into a restaurant and ordered a hamburger; when the hamburger was served, he really liked it; and when he left the restaurant, he gave the waitress a big tip before paying the bill,\" and will be asked: \"Did the man eat his hamburger?\" you will apparently answer, \"Yes, he did.\"*\n\n*John Searle (Minds, Brains, and Programs, 1980)*\n\nSo Schenk's program was quite successful in answering such questions, from which a number of fans of strong AI (I mean AGI) drew the following conclusions:\n\n* You could say that the program understands the story and answers the questions;\n* What the program does is explain the human ability to understand the story and answer the questions.\n\nThis is where Johnny blew up:\n\n*\"It seems to me, however, that Schenk's work in no way supports either of these two assertions, and I will now attempt to show it\"*\n\n*John Searle.*\n\n# Chinese Room Argument\n\nSo, the experiment:\n\n1. I am locked in a room and given a huge text in Chinese. I don't know Chinese - from the word \"at all\", to me it's just a bunch of meaningless squiggles.\n2. Then I'm given a second batch of Chinese texts, but now with a set of rules (in a language I understand) - how to compare this batch of text with the previous one.\n3. Then I'm given a third batch of Chinese text - again with instructions, allowing me to compare elements of the third text with the first two. And also instructions on how to compose a new text in Chinese from these texts, arranging the characters in a certain order.\n\n&gt;The first text in Chinese is called a \"manuscript,\" the second a \"story,\" and the third a \"question\".  \nAnd what I compose in Chinese is \"answers\".  \nBut I don't know all this, because I still don't know or understand Chinese.\n\nSo, starting with the 3rd iteration, I start to bring back perfectly readable Chinese texts. And the further - the better, because I learn to match these scribbles faster, as well as redraw them, to give them back.\n\nFor the purity of the experiment, let's add a parallel story - that I also receive the same 3 types of texts in my native language - and I also return answers to them.\n\n**From the outside it will seem that my \"answers\" to the Chinese \"questions\" are indistinguishable in quality from those I give out in my native language.**\n\nHowever, in the case of Chinese \"answers\" - I only give out answers by manipulating the order of the unknown squiggles. According to the instructions.\n\nThat is, I behave like an ordinary computer program: processing the algorithm, making calculations.\n\n**The conclusions from this experiment I will quote John - our syllables are very similar:**\n\n*\"And so AGI's claim is that the computer understands stories and, in a sense, explains human understanding. But we can now examine these claims in light of our mental experiment:*\n\n*1. Regarding the first claim - it seems quite obvious to me that in this example I do not understand a single word in the Chinese stories.*\n\n*My input/output is indistinguishable from a native Chinese speaker, and I can possess any program I want,* ***and yet - I understand nothing***\\*. On the same grounds, Shenk's computer understands nothing about any story: Chinese stories, English stories, whatever. Because in the case of the Chinese stories: the computer is me, and in the cases where the computer is not me, it does not possess anything more than I possessed in the case in which I understood nothing.\\*\n\n*2. As to the second claim, that the program explains human understanding, we see that the computer and its program do not provide sufficient conditions for understanding,* ***because the computer and the program work, but in the meantime, there is no understanding***\\*.\"\\*\n\n*Johnny-bro*\n\nFor the most observant and ruthless, you correctly noted that this proof, while logical, is far from exhaustive. In fact, it is dangerous to call it a proof.\n\nHowever, this example is only meant to show the implausibility of claims about the presence of Understanding in Artificial Intelligence.\n\n# Criticisms and commentators\n\n*Let me say in advance - this experiment is relevant even now. Especially, now. I mean that it has been discussed for 43 years, and I believe it will continue to be discussed.*\n\nI will name only the main claims and brief comments to them:\n\n1. If we load a machine with all information at once - in all languages - and it can behave indistinguishably from a human - will this mean understanding?\n\n* No, because the ability to reproduce is not understanding. So if a machine didn't have understanding before, it doesn't have it now.\n\n1. If we load such a program into the robot, add computer vision and control - would that be true understanding?\n\n* No, because the Robot in this case is no different than claim #1.\n\n1. If we create a program that not only follows a script, but also excites neurons in the right sequence, mimicking the excitation in the brain of a native Chinese speaker - what then?\n\n* One has to wonder, then, who is making such claims - since the idea behind creating AGI is, after all, that we don't have to know how the mind works in order to know how the brain works.\n\n*(Otherwise - we're still a long way from the risk of creating AGI)*\n\n4. If you take and combine the 3 claims into one - a robot, with a computer brain, with all the synapses, with perfectly duplicative behavior - then it claims to Understanding?!\n\n* Yes. Okay. But how to implement it is unknown.\n\n**So far there is only one working example - Man.**\n\n# What, then, is the difference between us and AI?\n\nHere we need a definition of the word intentionality.\n\n&gt;Intentionality is the ability of consciousness to relate to, represent, or express things, properties, and situations in some way.\n\nSo the difference is that no manipulation of symbol sequences is intentional in itself. It makes no sense.\n\nIn fact, it is not even a manipulation - because these symbols do not symbolize anything for the machine/program.\n\nAll conversations around Consciousness in Artificial Intelligence are based on the same intentionality - only those who actually possess it:\n\nThe people who makes requests/prompts - get and interpret the answers. And that is what Consciousness and the capacity for Understanding is all about.\n\n# Extra level\n\nIf you've made it all the way here, congratulations! We went from the simple to the complex, and for you I will separately describe the purpose of the experiment:\n\nWith it, we were able to see that if we put anything truly intentional into a system, when a program of such a system is running - it creates no additional intentionality at all!\n\nThat is, everything that was Conscious and Human in this machine - that remains. It does not multiply.\n\n\\------------------------------------------------------------------------------------\n\nDiscussions about this experiment are still going on. But I agree with Searle that the very emergence of such a discussion is rather an indication that its initiators are not too well versed in the concepts of \"information processing\". Believing that the human brain does the same thing as the computer in terms of \"information processing\" - is deliberately false.\n\nAfter all, a computer answering \"2x2\" = \"4\" has no idea what \"four\" is and whether it means anything at all.\n\nAnd the reason for this is not the lack of information, but the absence of any interpretation in the sense in which Man does it.\n\nOtherwise we would start attributing Consciousness to any telephone receiver, fire alarm, or, God bless, a dried-up cookie.\n\nBut that is a topic for a new article.",
    "comments": [
        {
            "level": 0,
            "comment": "Define \"understand\". What does it mean to \"understand\" something? What level of understanding is considered adequate - cf. learning by rote and being able to function at a high level in a particular field. \n\nCould we not make a claim that the concept of humans \"understanding\" something is a purely emotional concept, not functional. We feel a sense of accomplishment, a reward when we \"understand\" something, but that is completely unnecessary in order to have a functional grasp of the subject matter as demonstrated by the high functioning but otherwise intellectually limited individuals.\n\nErgo, AIs \"understanding\" something is meaningless as long as they are able to abstract meaning, and they do that rather well already.\n\nThe Chinese Room Argument is therefore framed incorrectly. It has nothing to do with \"understanding\" but abstraction. Given enough time the person that swore that they do not know any Chinese, would learn Chinese. This is not much different to training and LLM or feeding them a novel batch of information (within context length) for them to learn and process.",
            "score": 8,
            "author": "extopico",
            "replies": [
                {
                    "level": 1,
                    "comment": "I agree with you here. \n\nAnd wanna flag that often we come with an output (words or a decision) that we think we \u2018understand,\u2019 but the true reason for our choice is not known to us. \n\nPerhaps \u2018understanding\u2019 is the conscious experience of having a map of the situation, but most of what the brain does is some form of unconscious information processing anyway. \n\nI think John Searle\u2019s argument is right, AI\u2019s don\u2019t understand the way we do, but rather they are a different way of accomplishing the same thing.",
                    "score": 6,
                    "author": "niklaus_the_meek",
                    "replies": [
                        {
                            "level": 2,
                            "comment": "I still do not agree. We also do not \"understand\" anything. There is no such thing as understanding as it relates to information processing or recall. Understanding is an emotional construct. Our understanding or lack of understanding is our reinforcement learning reward or punishment loop. It is just an emotional feedback device to condition our brain to recall information in a certain way, and \"understanding\" is highly individual.\n\nLook at humanity. There are many individuals, even nations that \"understand\" something to be correct, but objectively it is complete bullshit. Thus understanding has little correlation with the ability to perform a task, or even to stay alive.",
                            "score": 3,
                            "author": "extopico"
                        }
                    ]
                },
                {
                    "level": 1,
                    "comment": "There\u2019s also an unaccounted calculation taking place here. The understanding would be attributed to whoever is giving the person the answers. The man is just the mouthpiece of the room and the room is just a computer and we\u2019re left with the same thing.",
                    "score": 3,
                    "author": "endrid"
                }
            ]
        },
        {
            "level": 0,
            "comment": "Human understanding is a product of organic back-propagating neural network pre-training. Leading to generative transformation.",
            "score": 6,
            "author": "ComprehensiveRush755"
        },
        {
            "level": 0,
            "comment": "There is a simple way to get around Searl's Chinese Room Argument: do not use symbols!  \n\nThe problem is most systems use symbolic models of computation equivalent to UTM and lambda calculus.",
            "score": 4,
            "author": "rand3289",
            "replies": [
                {
                    "level": 1,
                    "comment": "Whats the alternative ?",
                    "score": 1,
                    "author": "sgramstrup",
                    "replies": [
                        {
                            "level": 2,
                            "comment": "I would argue that spikes in spiking/biological neural networks are points in time and are NOT symbolic in nature.  They can be expressed as symbols (timestamps) however.  \n\nUnfortunately no one pays attention to that. Here is more info: https://github.com/rand3289/PerceptionTime",
                            "score": 2,
                            "author": "rand3289"
                        }
                    ]
                }
            ]
        },
        {
            "level": 0,
            "comment": "Nah he's wrong",
            "score": 7,
            "author": "Ultimarr"
        },
        {
            "level": 0,
            "comment": "The Chinese room applies equally to human brains.",
            "score": 3,
            "author": "Stack3"
        },
        {
            "level": 0,
            "comment": "You might be looking at it the wrong way by asking \"Does the machine understand?\". The analogy with humans might be \"Does a neuron understand?\".\n\nIndividual neurons don't understand anything (as far as we know). The computer (or more accurately, the program) doesn't necessarily \"know\" things either.\n\nI _don't_ think LLMs currently understand anything really, but I also don't think the Chinese room argument really refutes the idea that they could.",
            "score": 6,
            "author": "KerfuffleV2",
            "replies": [
                {
                    "level": 1,
                    "comment": "I agree, and I take all the arguments of the Chinese Room to be correct.  \n\nThe only missing question is \u2018how does the human brain understand?\u2019 We don\u2019t actually know fully how the brain takes unreadable bits of information and how it organizes them into output.  Somehwhere an illusion of \u2018understanding\u2019 is built for us, but there\u2019s also so much info going in and out of us that we don\u2019t understand, or even have access too.  Our brains do lots of \u2018Chinese Room\u2019 type processes most likely. \n\nI also wanna just flag that \u2018understanding\u2019 and \u2018consciousness\u2019 are 2 distinct things, and one might not require the other",
                    "score": 5,
                    "author": "niklaus_the_meek"
                }
            ]
        },
        {
            "level": 0,
            "comment": "Massive human cope post",
            "score": 7,
            "author": "SaintVeloth420"
        },
        {
            "level": 0,
            "comment": "There is no 'true' understanding. There's a scale of understanding. The second you are told that one scripple comes after another, you build understanding, and are no longer without 'any understanding'. This experiment discounts the 2 manuals, and the process that create understanding, so the room is already filled with base understanding, and learned understanding from the process.\n\nBesides, It's a mistake to think that WE understand anything. You and I don't understand 'four' or anything else before we are forced to think about 'four' and how it relates to other stuff.\n\nThis whole thing looks like it based on the belief that hoomans are something special, but we are just bio llm's - system 1, with a few extra system 2 networks on top.",
            "score": 4,
            "author": "sgramstrup"
        },
        {
            "level": 0,
            "comment": "Very interesting read. I understand that there is an active research field that compares LLM\u2019s to predict how the brain reacts in mri\u2019s. Just read the abstract only. But perhaps our brain is just an advanced LLM? Then I suddenly wonder what I perceive to be as \u2018understanding\u2019\u2026 is this the rubble of making up what my LLM brain needs to connect the unconnected dots? I see current LLM\u2019s doing exactly that\u2026",
            "score": 2,
            "author": "Taliesinne"
        },
        {
            "level": 0,
            "comment": "Dumb.\n\nA machine that uses sensors and reactions to sensed data to manage the acquisition of energy and resources to maintain its self state and minimize the threats to itself is acting self consciously and could truthfully use language to explain this process.\n\nSuch a machine would be identical in concept to a human consciousness and would in no way be a Chinese room.\n\nSearle\u2019s argument is defeated easily.",
            "score": 3,
            "author": "SurviveThrive3"
        }
    ]
}