{
    "id": "1426s61",
    "score": 9,
    "title": "The Chinese room argument, or Why Artificial Intelligence Doesn't Really Understand Anything",
    "author": "shrmn-me",
    "date": 1686034322.0,
    "url": null,
    "media_url": null,
    "comments": [
        {
            "level": 0,
            "comment": "Define \"understand\". What does it mean to \"understand\" something? What level of understanding is considered adequate - cf. learning by rote and being able to function at a high level in a particular field. \n\nCould we not make a claim that the concept of humans \"understanding\" something is a purely emotional concept, not functional. We feel a sense of accomplishment, a reward when we \"understand\" something, but that is completely unnecessary in order to have a functional grasp of the subject matter as demonstrated by the high functioning but otherwise intellectually limited individuals.\n\nErgo, AIs \"understanding\" something is meaningless as long as they are able to abstract meaning, and they do that rather well already.\n\nThe Chinese Room Argument is therefore framed incorrectly. It has nothing to do with \"understanding\" but abstraction. Given enough time the person that swore that they do not know any Chinese, would learn Chinese. This is not much different to training and LLM or feeding them a novel batch of information (within context length) for them to learn and process.",
            "score": 8,
            "author": "extopico",
            "replies": [
                {
                    "level": 1,
                    "comment": "I agree with you here. \n\nAnd wanna flag that often we come with an output (words or a decision) that we think we \u2018understand,\u2019 but the true reason for our choice is not known to us. \n\nPerhaps \u2018understanding\u2019 is the conscious experience of having a map of the situation, but most of what the brain does is some form of unconscious information processing anyway. \n\nI think John Searle\u2019s argument is right, AI\u2019s don\u2019t understand the way we do, but rather they are a different way of accomplishing the same thing.",
                    "score": 6,
                    "author": "niklaus_the_meek",
                    "replies": [
                        {
                            "level": 2,
                            "comment": "I still do not agree. We also do not \"understand\" anything. There is no such thing as understanding as it relates to information processing or recall. Understanding is an emotional construct. Our understanding or lack of understanding is our reinforcement learning reward or punishment loop. It is just an emotional feedback device to condition our brain to recall information in a certain way, and \"understanding\" is highly individual.\n\nLook at humanity. There are many individuals, even nations that \"understand\" something to be correct, but objectively it is complete bullshit. Thus understanding has little correlation with the ability to perform a task, or even to stay alive.",
                            "score": 3,
                            "author": "extopico"
                        }
                    ]
                },
                {
                    "level": 1,
                    "comment": "There\u2019s also an unaccounted calculation taking place here. The understanding would be attributed to whoever is giving the person the answers. The man is just the mouthpiece of the room and the room is just a computer and we\u2019re left with the same thing.",
                    "score": 3,
                    "author": "endrid"
                }
            ]
        },
        {
            "level": 0,
            "comment": "Human understanding is a product of organic back-propagating neural network pre-training. Leading to generative transformation.",
            "score": 6,
            "author": "ComprehensiveRush755"
        },
        {
            "level": 0,
            "comment": "There is a simple way to get around Searl's Chinese Room Argument: do not use symbols!  \n\nThe problem is most systems use symbolic models of computation equivalent to UTM and lambda calculus.",
            "score": 3,
            "author": "rand3289",
            "replies": [
                {
                    "level": 1,
                    "comment": "Whats the alternative ?",
                    "score": 1,
                    "author": "sgramstrup",
                    "replies": [
                        {
                            "level": 2,
                            "comment": "I would argue that spikes in spiking/biological neural networks are points in time and are NOT symbolic in nature.  They can be expressed as symbols (timestamps) however.  \n\nUnfortunately no one pays attention to that. Here is more info: https://github.com/rand3289/PerceptionTime",
                            "score": 2,
                            "author": "rand3289"
                        }
                    ]
                }
            ]
        },
        {
            "level": 0,
            "comment": "Nah he's wrong",
            "score": 8,
            "author": "Ultimarr"
        },
        {
            "level": 0,
            "comment": "The Chinese room applies equally to human brains.",
            "score": 3,
            "author": "Stack3"
        },
        {
            "level": 0,
            "comment": "You might be looking at it the wrong way by asking \"Does the machine understand?\". The analogy with humans might be \"Does a neuron understand?\".\n\nIndividual neurons don't understand anything (as far as we know). The computer (or more accurately, the program) doesn't necessarily \"know\" things either.\n\nI _don't_ think LLMs currently understand anything really, but I also don't think the Chinese room argument really refutes the idea that they could.",
            "score": 6,
            "author": "KerfuffleV2",
            "replies": [
                {
                    "level": 1,
                    "comment": "I agree, and I take all the arguments of the Chinese Room to be correct.  \n\nThe only missing question is \u2018how does the human brain understand?\u2019 We don\u2019t actually know fully how the brain takes unreadable bits of information and how it organizes them into output.  Somehwhere an illusion of \u2018understanding\u2019 is built for us, but there\u2019s also so much info going in and out of us that we don\u2019t understand, or even have access too.  Our brains do lots of \u2018Chinese Room\u2019 type processes most likely. \n\nI also wanna just flag that \u2018understanding\u2019 and \u2018consciousness\u2019 are 2 distinct things, and one might not require the other",
                    "score": 5,
                    "author": "niklaus_the_meek"
                }
            ]
        },
        {
            "level": 0,
            "comment": "Massive human cope post",
            "score": 8,
            "author": "SaintVeloth420"
        },
        {
            "level": 0,
            "comment": "There is no 'true' understanding. There's a scale of understanding. The second you are told that one scripple comes after another, you build understanding, and are no longer without 'any understanding'. This experiment discounts the 2 manuals, and the process that create understanding, so the room is already filled with base understanding, and learned understanding from the process.\n\nBesides, It's a mistake to think that WE understand anything. You and I don't understand 'four' or anything else before we are forced to think about 'four' and how it relates to other stuff.\n\nThis whole thing looks like it based on the belief that hoomans are something special, but we are just bio llm's - system 1, with a few extra system 2 networks on top.",
            "score": 5,
            "author": "sgramstrup"
        },
        {
            "level": 0,
            "comment": "Very interesting read. I understand that there is an active research field that compares LLM\u2019s to predict how the brain reacts in mri\u2019s. Just read the abstract only. But perhaps our brain is just an advanced LLM? Then I suddenly wonder what I perceive to be as \u2018understanding\u2019\u2026 is this the rubble of making up what my LLM brain needs to connect the unconnected dots? I see current LLM\u2019s doing exactly that\u2026",
            "score": 2,
            "author": "Taliesinne"
        },
        {
            "level": 0,
            "comment": "Dumb.\n\nA machine that uses sensors and reactions to sensed data to manage the acquisition of energy and resources to maintain its self state and minimize the threats to itself is acting self consciously and could truthfully use language to explain this process.\n\nSuch a machine would be identical in concept to a human consciousness and would in no way be a Chinese room.\n\nSearle\u2019s argument is defeated easily.",
            "score": 3,
            "author": "SurviveThrive3"
        }
    ]
}