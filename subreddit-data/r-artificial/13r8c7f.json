{
    "id": "13r8c7f",
    "score": 26,
    "title": "If we gave AI all the data available at the time, could it have derived Einstein's relativity?",
    "author": "Whalesftw123",
    "date": 1684990582.0,
    "url": null,
    "media_url": null,
    "comments": [
        {
            "level": 0,
            "comment": "Theories like Relativity require a lot of mathematical prowess, which LLMs simply don't have *at this time*.\n\nWhat LLMs are, are vast collections of conceptual dots, and when we talk to them, they connect those dots.  Most of the connections between those dots are well worn.  We know what they are, we can recognize them, and when the LLMs produce text based on the connections between those dots, we think of it as \"regurgitating\" what it's been trained on.  \"Yup\", we say, \"it's providing correct output.\"\n\nBut simple regurgitation is not *exactly* true: it's regurgitating connections *that it's seen*.  And it's seen a LOT, and not all of them in the same places, so just because *it's* seen them doesn't mean anyone else has, and it may have picked up some dots from far flung locations and the connections are ONLY implicit in its vast training material.\n\nMaybe for some things, as yet undiscovered, to us the dots are scattered and disconnected, but for the LLMs there are logical groupings we just haven't noticed yet.  They can connect dots that we can't, because they have such an enormous amount of data to draw upon to interpret those dots and how they connect.\n\nIt is very possible that LLMs can produce material that is only implicit in its training data, that they can produce logical inferences that we may not have previously made.  \"New\" material, so to speak, even though it derives from known material.\n\nRelativity, though, is more than just making a few connections between dots we hadn't made before.  So I wouldn't expect that.  But I *do* actually expect some of the simpler missed connections to finally get made.",
            "score": 7,
            "replies": [
                {
                    "level": 1,
                    "comment": "A good, thorough answer on LLMs. However, there are many other types of AI already out there, in development, and under active research. Some for specific scientific problems, some that are planned to do what LLMs do, but on maths/physics in general.\n\nHere are two examples of what such AIs were already capable of in 2022:\n\n [https://www.sciencealert.com/ai-has-discovered-alternate-physics-on-its-own](https://www.sciencealert.com/ai-has-discovered-alternate-physics-on-its-own)\n\n[https://www.quantamagazine.org/machine-scientists-distill-the-laws-of-physics-from-raw-data-20220510/](https://www.quantamagazine.org/machine-scientists-distill-the-laws-of-physics-from-raw-data-20220510/) \n\nIn the second case, the AI found a relation/pattern, that the researchers missed. The resulting equation the AI provided describes the phenomenon under study ten times more accurately than the formula the researchers came up with.",
                    "score": 7,
                    "replies": [
                        {
                            "level": 2,
                            "comment": "Yes, and I expect to see a lot more of this going forwards.  People are smart, and there's been lots of people looking at lots of things, but I doubt we've covered 100% of the scientific space even within our existing tech level, and our subject fields are pretty wide.\n\nWe're still every now and then running into situations where someone notices that the data coming from some scientific experiment seems to match odd equations that some obscure mathematician was working on in 1870, but that the connection only got made because one of the students had a weird hobby which is the only reason he even knew about the mathematician.  LLMs will likely start to see those connections without requiring the weird hobby (because they already know about *all* the hobbies).",
                            "score": 5
                        }
                    ]
                }
            ]
        },
        {
            "level": 0,
            "comment": "If by AI you mean chatGPT, then no. Full stop.",
            "score": 24
        },
        {
            "level": 0,
            "comment": "https://www.pnas.org/doi/10.1073/pnas.2200924119\n\nhttps://www.nature.com/articles/s43588-022-00281-6\n\nhttps://www.science.org/doi/10.1126/sciadv.aay2631",
            "score": 14,
            "replies": [
                {
                    "level": 1,
                    "comment": "Good job - most people on Reddit don\u2019t read. But essentially the answer is yes Fel the articles you posted",
                    "score": 2
                }
            ]
        },
        {
            "level": 0,
            "comment": "No, but keep in mind that intelligence doesn't start at Einstein and creativity doesn't start at Beethoven.\n\nIt is revolutionary technology long before it can do that.",
            "score": 4
        },
        {
            "level": 0,
            "comment": "The answer is no.  It can\u2019t come up with new things in that way at the moment unless you already have the idea and suggest it to it.",
            "score": 4
        },
        {
            "level": 0,
            "comment": "Current AI is capable of learning new things on it's own: [https://futurism.com/the-byte/google-ai-bengali](https://futurism.com/the-byte/google-ai-bengali)",
            "score": 2
        },
        {
            "level": 0,
            "comment": "If this is the case now, what makes AGI and ASI so special? They won't catapult us into the future because they cannot think outside the data.",
            "score": 1,
            "replies": [
                {
                    "level": 1,
                    "comment": "they can combine all humans knowledge kind of",
                    "score": 3,
                    "replies": [
                        {
                            "level": 2,
                            "comment": "They're LLMs. They parse input text and spit out text that it assumes the person who wrote the input expects.\n\nThat is to say, if you ask it questions, you'll just have your confirmation bias reinforced.",
                            "score": 0
                        }
                    ]
                },
                {
                    "level": 1,
                    "comment": "That is what would make it ASI. Or at least one of the things; the ability to formulate new ideas",
                    "score": 2,
                    "replies": [
                        {
                            "level": 2,
                            "comment": "Then our current technology is not yet there at all. I just cannot see the trend, we will need a new technological leap, a new revolution outside LLM's and training on big data. Hope it happens soon but one never knows.",
                            "score": 0,
                            "replies": [
                                {
                                    "level": 3,
                                    "comment": "I agree. We definitely don\u2019t have ASI. It\u2019s impossible to say how far away it is though. Current models are already exhibiting emergent behaviours. A GPT-5 or 6 with internet access, long-term memory and connected to other APIs to deal with logic, maths etc. may well be an AGI. None of those things are beyond the realms of current tech and most are being worked on as we speak.",
                                    "score": 2
                                }
                            ]
                        }
                    ]
                },
                {
                    "level": 1,
                    "comment": "AGI and ASI will be able to \"think outside the data\", in the sense that a human mind can anyway, but we are nowhere near that today",
                    "score": 1
                }
            ]
        },
        {
            "level": 0,
            "comment": "No, this is well known since the 90s\n\nwhat's missing is a faithful handing of analogies. ML can't do that.\n\nhttps://deepai.org/publication/towards-analogy-based-explanations-in-machine-learning",
            "score": -1,
            "replies": [
                {
                    "level": 1,
                    "comment": "GPT4 has early indications it can and that future tech will definitely be able to.",
                    "score": 4
                }
            ]
        },
        {
            "level": 0,
            "comment": "No, AI doesn\u2019t come up with new things per se. it\u2019s just excellent at finding patterns",
            "score": -1,
            "replies": [
                {
                    "level": 1,
                    "comment": "Finding patterns is literally what leads to finding out about general relativity, just like all laws of nature.",
                    "score": 4,
                    "replies": [
                        {
                            "level": 2,
                            "comment": "It\u2019s not at all that simple tho. AI can create connections and group in the matter of finding patterns, but not innovate",
                            "score": -1,
                            "replies": [
                                {
                                    "level": 3,
                                    "comment": "I wouldn't call finding the laws of nature 'innovating', but even then, innovation is trying new things and there is nothing preventing AI from doing exactly that. Ever heard of stable diffusion?",
                                    "score": 2,
                                    "replies": [
                                        {
                                            "level": 4,
                                            "comment": "You can\u2019t just put the laws of nature into numbers. All the stuff you see AI doing today such as computer vision, natural language processing, audio processing, translation, etc. are all quantified. You can\u2019t exactly quantify the laws or nature",
                                            "score": 0,
                                            "replies": [
                                                {
                                                    "level": 5,
                                                    "comment": "The way 'understanding' is encoded is hard to describe. It's about how neurons are wired together and how they fire together. We can model the human brain with artificial neurons, which show very similar collective behaviour. There is no reason why understanding of universal patterns cannot be encoded in a neural network.",
                                                    "score": 3
                                                },
                                                {
                                                    "level": 5,
                                                    "comment": "You absolutely can... thats the whole point of science.",
                                                    "score": 1
                                                }
                                            ]
                                        }
                                    ]
                                }
                            ]
                        }
                    ]
                }
            ]
        },
        {
            "level": 0,
            "comment": "ChatGPT 4 **can** invent things such as novel tools.\n\nI'm not so sure about novel theories 'tho.",
            "score": 1
        },
        {
            "level": 0,
            "comment": "So, are some of you saying that ai can't be programmed to invent and then attempt to support its own novel ideas? Will it ever? Soon? Or will we always have to spell out the precise pursuit for it?",
            "score": 1
        },
        {
            "level": 0,
            "comment": "Not really. LLMs don't come up with new information. I mean, if you asked it about relativity it would just agree with you.",
            "score": 1
        },
        {
            "level": 0,
            "comment": "Jesus, some people really have wild expectations from chatgpt",
            "score": 1
        },
        {
            "level": 0,
            "comment": "Current chatbots are not creative or original. At best, they could have poorly and inaccurately summarized existing research. They don't understand physics, they statistically manipulate words",
            "score": 1
        },
        {
            "level": 0,
            "comment": "These models are all based on an \"if-then\" type of processing. It's basic to their architecture, as in, \"If these are the inputs, then these are the most likely outputs.\" You can see this in a diagram of how a neural network works. A given input (or node) is given a certain weight going to the next node, given the pattern of inputs. Any of you familiar with Pearl's ladder of causality know that \"if-then\" analysis is the second step in a ladder that has three steps. This is not to say that you can't get far with \"if-then\" reasoning. It allows you to do interventions. It probably underlies the way many animals navigate the world. But there is another step, one that Pearl calls \"counterfactuals,\" and which are more simply called \"what if\" thinking. The type of question this involves is like, \"What would have happened if I had done something different?\" or \"What might have happened in this new circumstance?\" Neural networks can't do that because of their basic architecture. There has to be a stated first condition, the ingoing \"if.\" Now Microsoft has been making noises that they have \"emergent\" behavior, which apparently would mean that if you put enough \"if-then\" rules together, you get \"what if\" thinking. At the moment, I think the technical answer to that is \"yeah, right.\"",
            "score": 1
        },
        {
            "level": 0,
            "comment": "I had the pleasure of working for a couple months with some excellent physicists over at the Geoforschungs Zentrum (German area search Center for Geosciences) back in 2020, and physicists like Yuri Shprits and the others working there had saw the potential for AI to teach us physics and had at the time I was there already begun to do so. They had a physics based model that would predict space weather (things like geomagnetic storms) and a data driven model, which was learned by AI. These being essentially distinct models, they would then compare the different predictions with what would actually happen and try to use the results to teach us new physics. This has absolutely nothing do with LLMs or other large models that you can find on hugging face, but was rather a model created in house. \n\nThis sparked my interest, and since then I have been doing my best to keep up with the research in the area (referred to in papers often as physics informed neural networks (PINNs)) as well as other related endeavors. What I have found is a lot of success in this field, and since personally my background is in relativity, I have been particularly interested in how this would be applied there, and for the past several years I have been going to physics conferences eagerly attending any sessions that mentioned relativity and machine learning (AI) together, and while this most recent year I\u2019ve been rather busy, one of the coolest papers I\u2019ve found on the subject is the following:\n\nhttps://journals.aps.org/prresearch/pdf/10.1103/PhysRevResearch.3.043101\n\nSince it\u2019s probably somewhat opaque, the summary is as follows: using a \u2018universal\u2019 set of PDEs and gravitational wave data, they use machine learning to see if it can learn how to calculate the orbits of a merging binary black hole system. it is surprisingly successful, and captures high order effects like kickback, radiation reaction, and other fun sounding effects that you typically have to go to great effort to capture, all just with data*\n\nYes, you may argue, this isn\u2019t \u2018deriving relativity\u2019 in the sense that maybe you were thinking, but it\u2019s effectually equivalent, and here\u2019s why: finding these orbits is hard, and I mean *really* hard. There is no explicit expression or \u2018equation\u2019 as you may traditionally consider that just tells you based off of parameters, but rather must be solved numerically, which is in and of itself a challenging and complex field. I\u2019ll spare you the details, but for reference, solving for a few milliseconds of these trajectories on supercomputers can take *weeks* of runtime, when done to the highest accuracy.\n\nThis model takes those weeks, scoffs at it, and after learning can spit out comparable results in seconds. I think this is absolutely incredible, and a real testament to the kinds of thing machine learning and AI can do.\n\nIt is worth mentioning this kind of model, like others I\u2019ve mentioned, is not an LLM like chatGPT but is one built by these researchers specifically for this problem.\n\nThe reason I vomit these words is because I feel like I have been a lone observer for the longest time watching these incredible computational physicists push the boundaries of what it means to do research before the AI boom, and I haven\u2019t had a way to tell anyone about this until now. I think their contributions are amazing but under appreciated for their time, and now that the world sees AI for what it can be, my sincerest hope is that these incredible scientists get the recognition the deserve for their avant-garde research in AI.\n\nSo, *tl;dr*, yes, it looks like it can in the ways that are really useful and that are hard, and scientist have been doing so for years before the AI boom.\n\nEdit: typos",
            "score": 1
        },
        {
            "level": 0,
            "comment": "No. \n\nEspecially because of how we store the information. It doesn't know how to access the data creatively. Imagine it, currently, to be a filing clerk who retrieves stuff you ask it to. Stupid. That's why he's a filing clerk.",
            "score": -2,
            "replies": [
                {
                    "level": 1,
                    "comment": "Wow, you really hate filing clerks.",
                    "score": 3
                },
                {
                    "level": 1,
                    "comment": "The mere fact that it can compose novel text (poems, short stories, etc) means it's doing more than \"stupidly\" retrieving information.",
                    "score": 2,
                    "replies": [
                        {
                            "level": 2,
                            "comment": "Analyze the mathematical/symbolic/metaphorical patterns of {text}. Pay special attention to the {structure}. Describe if there is any topic/connecting word, concept or idea that is found throughout the text. {text} \n\nTry that and tell me how smart it is.",
                            "score": -1,
                            "replies": [
                                {
                                    "level": 3,
                                    "comment": "I have no idea what you're trying to say.\n\nBut to expand on what I already said, in case that helps, a \"stupid\" filing clerk won't--and likely can't--compose a sestina solely from the contents of the files in their cabinets.\n\nThe bottom line is that your analogy is unapt.",
                                    "score": 2,
                                    "replies": [
                                        {
                                            "level": 4,
                                            "comment": "Even chatGPT thinks you're dumb:\n\nHere's why:\r  \n\r  \n1. \\*\\*Lack of General Understanding:\\*\\* AI lacks a genuine understanding of the world as we humans perceive it. It can parse data, find patterns, and even generate novel content based on that, but that's not equivalent to a holistic understanding of the world.\r  \n\r  \n2. \\*\\*Limited to Trained Domains:\\*\\* AI operates within the boundaries of the training it receives. It does not have the ability to transfer knowledge from one domain to another, which is a critical part of creativity and the development of innovative theories like relativity.\r  \n\r  \n3. \\*\\*Repetition, Not Creativity:\\*\\* AI's generation of poems and short stories, as mentioned by Cephalopong, demonstrates its ability to replicate patterns, but not necessarily to create original content. AI doesn't \"imagine\" or \"create\" in the way humans do\u2014it only applies patterns and structures it has learned from data.\r  \n\r  \n4. \\*\\*Dependence on External Input:\\*\\* In the case of the AI being asked to analyze a given text for patterns and structure, it relies heavily on external input and instruction. The fact that it needs such specific instructions to perform complex tasks shows a lack of independent thought or ingenuity.\r  \n\r  \n5. \\*\\*Lack of Abstract Reasoning:\\*\\* AI currently struggles with symbolic and abstract reasoning, the kind of cognitive capabilities that were crucial in deriving theories like Einstein's relativity. Thus, even if all the data were available, it would not necessarily be able to connect the dots to develop such theories.\r  \n\r  \n6. \\*\\*The Filing Clerk Analogy Is Valid:\\*\\* The filing clerk analogy is a good representation of how AI functions. Even when AI generates something like a sestina, it's effectively accessing stored \"files\" of language patterns, structures, and word associations. While it's true that this is a bit more complex than a typical filing clerk's job, the core principle is similar. Both are retrieving and organizing data based on a set of rules, without truly understanding the deeper significance of the information.\r  \n\r  \nIn summary, while AI has impressive abilities and potential, it doesn't yet possess the level of creativity, understanding, and abstract reasoning needed to independently derive complex theories like Einstein's relativity.",
                                            "score": -1,
                                            "replies": [
                                                {
                                                    "level": 5,
                                                    "comment": "Awesome, go post that in reply to someone who claimed AI can derive relativity.  \n\nWhat I said was that it's doing more than \"stupid\" retrieval.\n\n&gt;While it's true that this is a bit more complex than a typical filing clerk's job, the core principle is similar\n\nFile clerks don't synthesize new products, deliverables, whatever from the documents they file.  They're responsible for putting the documents into the system.  LLMs do something altogether different, in that they *do* synthesize new text from what's in their \"filing cabinets\".",
                                                    "score": 2,
                                                    "replies": [
                                                        {
                                                            "level": 6,
                                                            "comment": "Go away!\n\nYou didn't even know that I gave you a prompt to try\n\n`I have no idea what you're trying to say.`\n\n&amp;#x200B;\nNo point in talking to you, yet you have too much pride to just slink away. \nIf by synthesize you mean combine different data and recombine them in a new way, perhaps. But, if you mean synthesize as in combine data that is not topically similar but semantically similar or conceptually similar, then no.",
                                                            "score": 0,
                                                            "replies": [
                                                                {
                                                                    "level": 7,
                                                                    "comment": "It really sounds like you just wanted to shout your opinion without having anyone disagree with you.  I've presented cogent counter-arguments to your claim, and your rebuttal is \"go away\", after which you imply there's some reason for me to \"slink away\" like I've done something shameful.\n\nThis is the internet.  People will disagree with you.  If you can't handle that, don't post on a public forum.",
                                                                    "score": 2,
                                                                    "replies": [
                                                                        {
                                                                            "level": 8,
                                                                            "comment": "If you took the time to understand what you're saying, then yes. I could accept a different opinion as being a personal choice. But, you don't even know what you're talking about. It shows, you know.\n\nEven your Creative Genius thinks you're wrong, yet you still say you're right?\n\nCogent? In that, we have different opinion about what is cogent.\n\nJust consider what you wrote:\n\n`The mere fact that it can compose novel text (poems, short stories, etc) means it's doing more than \"stupidly\" retrieving information.`\n\nCompose... This is a key word that is contradicted even by chatgpt. It didn't compose but rather took a string (through the prompt) and then analyzed what was in its filing cabinet and sorted by best match iteratively throughout the piece until it reached the end. \n\nHere, you're using compose in the sense that it creatively imagined, made from nothing, used a spark of genius to create. It did no such thing.",
                                                                            "score": 1,
                                                                            "replies": [
                                                                                {
                                                                                    "level": 9,
                                                                                    "comment": "You still seem to think I'm making claims that LLMs are \"creative geniuses\" and can derive relativity.  I've repeatedly corrected you about what I've claimed. \n\nYou're not debating in good faith.  I don't think you even know what that means, much less how to do it.  \n\nHave a swell day.",
                                                                                    "score": 1,
                                                                                    "replies": []
                                                                                }
                                                                            ]
                                                                        }
                                                                    ]
                                                                }
                                                            ]
                                                        }
                                                    ]
                                                }
                                            ]
                                        }
                                    ]
                                }
                            ]
                        }
                    ]
                }
            ]
        },
        {
            "level": 0,
            "comment": "No, because it required a radically different approach and predictions that were not based on any current data.  AI is assuming that the information is correct and builds upon that, but radically shifting without data is still a very human process.",
            "score": 0
        }
    ]
}