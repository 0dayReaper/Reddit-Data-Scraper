{
    "id": "13fnatx",
    "score": 1,
    "title": "GPU for CS classes?",
    "author": "thr278",
    "date": 1683903005.0,
    "url": "https://www.reddit.com/r/Cornell/comments/13fnatx",
    "media_urls": [],
    "other_urls": [],
    "postText": "Would having a decent GPU, maybe a 30 or 40 Series Nvidia, make a difference in projects for  ML / AI / etc. courses? I imagine faster training times would help finish projects sooner or give you more opportunities to improve, but maybe it doesn't matter much based on the projects.",
    "comments": [
        {
            "level": 0,
            "comment": "I haven\u2019t taken any of the courses, but I\u2019d imagine there are two likely scenarios, neither of which would require a gpu. \n1. If you are writing your own implementations of algorithms (which Id expect you\u2019d be doing) you are not likely going to go as far as adding CUDA functionality to it, as Id think that\u2019s beyond the scope of the courses. \n2. If you need more power, the course would probably allow access to the CS dept servers. \n\nThere is no way they would expect you to buy a $1000+ graphics card. If you want one and have the means to, you could get one, but I wouldn\u2019t lie to yourself and say it will help you in classes",
            "score": 13,
            "author": "pw11111"
        },
        {
            "level": 0,
            "comment": "Ask ur parents for a gpu and just say that u need it for school",
            "score": 5,
            "author": "TheBlackDrago"
        },
        {
            "level": 0,
            "comment": "I say you just download some ram off the internet and you\u2019ll be fine",
            "score": 5,
            "author": "Sea-Ad4009"
        },
        {
            "level": 0,
            "comment": "You can also use AWS for more demanding computational tasks",
            "score": 4,
            "author": "nibornil1999"
        },
        {
            "level": 0,
            "comment": "It's really not needed",
            "score": 1,
            "author": "lellomn"
        },
        {
            "level": 0,
            "comment": "From my experience, yes but the difference is incredibly marginal. Many undergrad CS classes are designed in mind that people don\u2019t have access to high powered GPUs. For example, most of the models you train in ML take at most 1 min to train. For RL, it takes at most 10 mins. Only class where a GPU might be handy is for 4701, where you will likely have to train your own model. Also grad CS classes sometimes have a final project that might require you to train your own model. \nAs the people mentioned here, if you need more power, you can use the cloud (which is what I did for 4701).",
            "score": 1,
            "author": "Raladin123"
        },
        {
            "level": 0,
            "comment": "[https://coral.ai/products/](https://coral.ai/products/)  \n\n\nI imagine that the time you save with an nvidia gpu will be offset by your gaming habit. Buying an nvidia gpu doesn't make sense because like &gt;50% of the silicon is for ray tracing and rasterization... it just also happens to do AI.",
            "score": 0,
            "author": "PoopyDootyBooty",
            "replies": [
                {
                    "level": 1,
                    "comment": "There is a reason that NVIDIA (data center) GPUs are basically the benchmark for high performance compute for things like AI \u2026. Definitely not cost effective but they are incredibly large dies on modern process nodes. Also what you linked seems to be intended for pre trained models based on a quick look, while the vast majority of the compute effort comes in training a model",
                    "score": 1,
                    "author": "pw11111"
                }
            ]
        }
    ]
}